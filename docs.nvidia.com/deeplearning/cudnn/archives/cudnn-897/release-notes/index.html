<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-us" xml:lang="en-us">
   
<!-- Mirrored from docs.nvidia.com/deeplearning/cudnn/archives/cudnn-897/release-notes/index.html by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 06 Aug 2024 07:08:52 GMT -->
<head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta>
      <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>
      <meta name="copyright" content="(C) Copyright 2005"></meta>
      <meta name="DC.rights.owner" content="(C) Copyright 2005"></meta>
      <meta name="DC.Type" content="concept"></meta>
      <meta name="DC.Title" content="cuDNN Release Notes"></meta>
      <meta name="abstract" content="NVIDIA CUDA Deep Neural Network (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. It provides highly tuned implementations of routines arising frequently in DNN applications. These release notes describe the key features, software enhancements and improvements, and known issues for the NVIDIA cuDNN 8.9.7 and earlier releases."></meta>
      <meta name="description" content="NVIDIA CUDA Deep Neural Network (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. It provides highly tuned implementations of routines arising frequently in DNN applications. These release notes describe the key features, software enhancements and improvements, and known issues for the NVIDIA cuDNN 8.9.7 and earlier releases."></meta>
      <meta name="DC.Coverage" content="Getting Started"></meta>
      <meta name="DC.subject" content="Release Notes, cuDNN"></meta>
      <meta name="keywords" content="Release Notes, cuDNN"></meta>
      <meta name="DC.Format" content="XHTML"></meta>
      <meta name="DC.Identifier" content="abstract"></meta>
      <link rel="stylesheet" type="text/css" href="../common/formatting/commonltr.css"></link>
      <link rel="stylesheet" type="text/css" href="../common/formatting/site.css"></link>
      <title>Release Notes :: NVIDIA cuDNN Documentation</title>
      <!--[if lt IE 9]>
      <script src="../common/formatting/html5shiv-printshiv.min.js"></script>
      <![endif]-->
      <script src="../../../../../../assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js"></script>
      <script type="text/javascript" src="../../../../../../cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-svg.min.js"></script>
      <script type="text/javascript" charset="utf-8" src="../common/formatting/jquery.min.js"></script>
      <script type="text/javascript" charset="utf-8" src="../common/formatting/jquery.ba-hashchange.min.js"></script>
      <script type="text/javascript" charset="utf-8" src="../common/formatting/jquery.scrollintoview.min.js"></script>
      <script type="text/javascript" src="../search/htmlFileList.js"></script>
      <script type="text/javascript" src="../search/htmlFileInfoList.js"></script>
      <script type="text/javascript" src="../search/nwSearchFnt.min.js"></script>
      <script type="text/javascript" src="../search/stemmers/en_stemmer.min.js"></script>
      <script type="text/javascript" src="../search/index-1.js"></script>
      <script type="text/javascript" src="../search/index-2.js"></script>
      <script type="text/javascript" src="../search/index-3.js"></script>
      <link rel="canonical" href="https://docs.nvidia.com/deeplearning/cudnn/release-notes/index.html"></link>
      <link rel="stylesheet" type="text/css" href="../common/formatting/qwcode.highlight.css"></link>
   </head>
   <body>
      
      <header id="header"><span id="company">NVIDIA</span><span id="site-title">NVIDIA cuDNN Documentation</span><form id="search" method="get" action="https://docs.nvidia.com/deeplearning/cudnn/archives/cudnn-897/release-notes/search">
            <input type="text" name="search-text"></input><fieldset id="search-location">
               <legend>Search In:</legend>
               <label><input type="radio" name="search-type" value="site"></input>Entire Site</label>
               <label><input type="radio" name="search-type" value="document"></input>Just This Document</label></fieldset>
            <button type="reset">clear search</button>
            <button id="submit" type="submit">search</button></form>
      </header>
      <div id="site-content">
         <nav id="site-nav">
            <div class="category closed"><a href="../index.html" title="The root of the site.">Getting Started</a></div>
            <div class="category"><a href="index.html" title="Release Notes">Release Notes</a></div>
            <ul>
               <li>
                  <div class="section-link"><a href="#rel_8">1.&nbsp;cuDNN Release 8.x.x</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#rel-897">1.1.&nbsp;cuDNN Release 8.9.7</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-896">1.2.&nbsp;cuDNN Release 8.9.6</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-895">1.3.&nbsp;cuDNN Release 8.9.5</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-894">1.4.&nbsp;cuDNN Release 8.9.4</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-893">1.5.&nbsp;cuDNN Release 8.9.3</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-892">1.6.&nbsp;cuDNN Release 8.9.2</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-891">1.7.&nbsp;cuDNN Release 8.9.1</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-890">1.8.&nbsp;cuDNN Release 8.9.0</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-881">1.9.&nbsp;cuDNN Release 8.8.1</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-880">1.10.&nbsp;cuDNN Release 8.8.0</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-870">1.11.&nbsp;cuDNN Release 8.7.0</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-860">1.12.&nbsp;cuDNN Release 8.6.0</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-850">1.13.&nbsp;cuDNN Release 8.5.0</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-841">1.14.&nbsp;cuDNN Release 8.4.1</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-840">1.15.&nbsp;cuDNN Release 8.4.0</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-833">1.16.&nbsp;cuDNN Release 8.3.3</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-832">1.17.&nbsp;cuDNN Release 8.3.2</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-831">1.18.&nbsp;cuDNN Release 8.3.1</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-830">1.19.&nbsp;cuDNN Release 8.3.0</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-824">1.20.&nbsp;cuDNN Release 8.2.4</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-822">1.21.&nbsp;cuDNN Release 8.2.2</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-821">1.22.&nbsp;cuDNN Release 8.2.1</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-820">1.23.&nbsp;cuDNN Release 8.2.0</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-811">1.24.&nbsp;cuDNN Release 8.1.1</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-810">1.25.&nbsp;cuDNN Release 8.1.0</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-805">1.26.&nbsp;cuDNN Release 8.0.5</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-804">1.27.&nbsp;cuDNN Release 8.0.4</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-803">1.28.&nbsp;cuDNN Release 8.0.3</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-802">1.29.&nbsp;cuDNN Release 8.0.2</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-801-Preview">1.30.&nbsp;cuDNN Release 8.0.1 Preview</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel-800-Preview">1.31.&nbsp;cuDNN Release 8.0.0 Preview</a></div>
                     </li>
                  </ul>
               </li>
               <li>
                  <div class="section-link"><a href="#rel_7xx">2.&nbsp;cuDNN Release 7.x.x</a></div>
                  <ul>
                     <li>
                        <div class="section-link"><a href="#rel_765">2.1.&nbsp;cuDNN Release 7.6.5</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_764">2.2.&nbsp;cuDNN Release 7.6.4</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_763">2.3.&nbsp;cuDNN Release 7.6.3</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_762">2.4.&nbsp;cuDNN Release 7.6.2</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_761">2.5.&nbsp;cuDNN Release 7.6.1</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_760">2.6.&nbsp;cuDNN Release 7.6.0</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_751">2.7.&nbsp;cuDNN Release 7.5.1</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_750">2.8.&nbsp;cuDNN Release 7.5.0</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_742">2.9.&nbsp;cuDNN Release 7.4.2</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_741">2.10.&nbsp;cuDNN Release 7.4.1</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_731">2.11.&nbsp;cuDNN Release 7.3.1</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_730">2.12.&nbsp;cuDNN Release 7.3.0</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_721">2.13.&nbsp;cuDNN Release 7.2.1</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_714">2.14.&nbsp;cuDNN Release 7.1.4</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_713">2.15.&nbsp;cuDNN Release 7.1.3</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_712">2.16.&nbsp;cuDNN Release 7.1.2</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_711">2.17.&nbsp;cuDNN Release 7.1.1</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_705">2.18.&nbsp;cuDNN Release 7.0.5</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_704">2.19.&nbsp;cuDNN Release 7.0.4</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_703">2.20.&nbsp;cuDNN Release 7.0.3</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#rel_7">2.21.&nbsp;cuDNN Release 7.0.2</a></div>
                     </li>
                     <li>
                        <div class="section-link"><a href="#unique_536635605">2.22.&nbsp;cuDNN Release 7.0.1</a></div>
                     </li>
                  </ul>
               </li>
            </ul>
         </nav>
         <div id="resize-nav"></div>
         <nav id="search-results">
            <h2>Search Results</h2>
            <ol></ol>
         </nav>
         
         <div id="contents-container">
            <div id="breadcrumbs-container">
               <div id="release-info">Release Notes (<a href="../pdf/cuDNN-Release-Notes.pdf">PDF</a>)
                  -
                  
                  
                  
                  -
                  
                  Last updated April 20, 2024
               </div>
            </div>
            <article id="contents">
               <div class="topic nested0" id="abstract"><a name="abstract" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#abstract" name="abstract" shape="rect">cuDNN Release Notes</a></h2>
                  <div class="body conbody">
                     <p class="shortdesc">NVIDIA CUDA Deep Neural Network (cuDNN) is a GPU-accelerated library of primitives
                        for deep neural networks. It provides highly tuned implementations of routines arising
                        frequently in DNN applications. These release notes describe the key features, software
                        enhancements and improvements, and known issues for the NVIDIA cuDNN 8.9.7 and earlier
                        releases.
                     </p>
                  </div>
               </div>
               <div class="topic reference nested0" id="rel_8"><a name="rel_8" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#rel_8" name="rel_8" shape="rect">1.&nbsp;cuDNN Release 8.x.x</a></h2>
                  <div class="body refbody">
                     <p class="shortdesc"></p>
                     <div class="section"></div>
                  </div>
                  <div class="topic reference nested1" id="rel-897"><a name="rel-897" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-897" name="rel-897" shape="rect">1.1.&nbsp;cuDNN Release 8.9.7</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">These are the NVIDIA cuDNN 8.9.7 Release Notes. These Release Notes include fixes
                           		from the previous cuDNN releases as well as the following additional changes.
                        </p>
                        <div class="section" id="rel-897__section_l2m_s4c_2jb"><a name="rel-897__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">These Release Notes are applicable to both cuDNN and NVIDIA JetPack™
                              				users of cuDNN unless appended specifically with <em class="ph i">(not applicable for Jetson
                                 					platforms)</em>.
                           </p>
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-897__section_a11_3bx_4zb"><a name="rel-897__section_a11_3bx_4zb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-897__ul_lyr_3bx_4zb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-897__ul_lyr_3bx_4zb">
                                 <li class="li liexpand">Expanded support for FP16 and BF16 fused flash attention by adding Grouped Query Attention
                                    						(GQA) for NVIDIA Hopper and NVIDIA Ampere GPUs. Added an additional
                                    						reduction kernel for gradient calculation for key and value tensor with less
                                    						heads than query tensor. For more information refer to the  <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html" target="_blank" shape="rect">NVIDIA cuDNN Developer Guide</a>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-897__section_yv4_gbx_4zb"><a name="rel-897__section_yv4_gbx_4zb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:<a name="rel-897__ul_avx_gbx_4zb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-897__ul_avx_gbx_4zb">
                                 <li class="li liexpand">When using the Runtime Fusion Engine to compile for convolution backward
                                    						data related fusions, if the convolution operation has both <samp class="ph codeph">strides
                                       							!= 1</samp> and <samp class="ph codeph">dilation !=1</samp>, incorrect results may
                                    						be produced. We have added supported-check conditions to return
                                    							<samp class="ph codeph">NOT_SUPPORTED</samp> instead.
                                 </li>
                                 <li class="li liexpand">Running convolution backward data with <samp class="ph codeph">batch size &gt; 65535 &amp;&amp;
                                       							stride &gt; 1</samp> could fail with
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> during the execution. This
                                    						issue has been fixed.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-897__section_t2m_s5p_nnb"><a name="rel-897__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel-897__ul_mbs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-897__ul_mbs_qgh_fsb">
                                 <li class="li liexpand">For the <samp class="ph codeph">ConvBiasAct</samp> operation,
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 39</samp> may return
                                    							<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> when running on a system
                                    						with CUDA Toolkit version 11.0.3 through 11.1. Upgrading the CUDA Toolkit
                                    						version to 11.1 Update 1 or later should resolve this issue.
                                 </li>
                                 <li class="li liexpand">ZLIB version 1.2.13 is statically linked into the cuDNN Windows dynamic
                                    						libraries. Changing to the static linkage of ZLIB for other platforms will
                                    						be addressed in future cuDNN releases.
                                 </li>
                                 <li class="li liexpand">A race condition in memory write accesses was flagged by the
                                    						"compute-sanitizer" tool in some cuBLAS kernels invoked by the cuDNN
                                    						multihead attention API <samp class="ph codeph">cudnnMultiHeadAttnForward()</samp> on H100
                                    						GPUs. Extensive testing on H100, with different clock speeds and
                                    						computational loads, did not reveal any impact on functional results that
                                    						were always identical and correct. This issue is currently being
                                    						investigated. 
                                 </li>
                                 <li class="li liexpand">cuDNN's usage of cuBLAS from CUDA Toolkit 12.1 may result in race-check
                                    						failures when the library is tested under compute sanitizer. These failures
                                    						are believed to be a cuBLAS issue and are being investigated. A workaround
                                    						for this issue is to use cuBLAS from CUDA Toolkit 12.0.
                                 </li>
                                 <li class="li liexpand">A compiler bug in NVRTC in CUDA version 11.7 and earlier, was causing
                                    						incorrect outputs when computing logical operations on boolean input tensors
                                    						in the runtime fusion engine. A workaround had been integrated to avoid the
                                    						most common issues. However, it is highly recommended to update to at least
                                    						CUDA version 11.7u1 for a fix. Specifically, known failure cases are when
                                    						pointwise operations of mode <samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_NOT</samp>,
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_AND</samp> or
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_OR</samp> operates on boolean tensors.
                                    						This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 57</samp> for convolution forward
                                    						and <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 62</samp> for convolution
                                    						backward data may have performance regressions for non-zero beta problems.
                                    						However, they are not typically recommended by cuDNN heuristics, so the
                                    						observable impact should be minimal.
                                 </li>
                                 <li class="li liexpand">Use of <samp class="ph codeph">cudnnFusedOpsExecute()</samp> on NVIDIA Volta compatible
                                    						architectures hosted on AArch64 systems may generate incorrect results when
                                    						used with <samp class="ph codeph">cudnnFusedOps_t</samp> set to
                                    							<samp class="ph codeph">CUDNN_FUSED_SCALE_BIAS_ACTIVATION_WGRAD</samp>.
                                 </li>
                                 <li class="li liexpand">With CUDA 11.7, the NVRTC library may cause a small memory leak when using
                                    						the runtime fusion engine. The issue has been addressed in CUDA Toolkit 11.8
                                    						or later. 
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 0</samp> for
                                    							<samp class="ph codeph">DgradDreluBnBwdWeights</samp> may see a performance regression
                                    						when moving from cuDNN 8.8 to cuDNN 8.9.
                                 </li>
                                 <li class="li liexpand">If cuDNN 8.4.1 or earlier statically links with
                                    							<samp class="ph codeph">libcudart.so</samp> from the CUDA Toolkit 11.7 or later, when
                                    						the LFL feature is activated, the results from
                                    							<samp class="ph codeph">cudnnFind*Algo</samp> will not be accurate.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX
                                    						3090 compared to 2080 Ti. This includes EfficientNet with up to 6x
                                    						performance difference, UNet up to 1.6x performance difference and Tacotron
                                    						up to 1.6x performance difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    							<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior,
                                    						accumulator data types, and so on, have not been documented as of yet.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with filter size
                                    						1x1. The severity would be different depending on which version of the CUDA
                                    						Toolkit the user is using.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with high group
                                    						count. The issue is more severe on V100.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-897__section_dph_zkv_jrb"><a name="rel-897__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and
                              				the NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-897__section_nbs_qgh_fsb"><a name="rel-897__section_nbs_qgh_fsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel-897__ul_obs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-897__ul_obs_qgh_fsb">
                                 <li class="li liexpand">Disabling CUDA context preemption on Windows can sometimes lead to
                                    							<samp class="ph codeph">CUDNN_INTERNAL_ERRORS</samp> being returned from convolution
                                    						runs. When using cuDNN, do not disable CUDA context preemption.
                                 </li>
                                 <li class="li liexpand">When using the cuDNN static library, you will need to use the same
                                    						major.minor version of the CUDA Toolkit by which cuDNN was built to build
                                    						your application. Refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a> for
                                    						the exact supported CUDA versions.
                                 </li>
                                 <li class="li liexpand">In cuDNN 8.9.0, runtime fusion engines (with
                                    							<samp class="ph codeph">CUDNN_BEHAVIOR_NOTE_RUNTIME_COMPILATION</samp>) will only work
                                    						with NVRTC from CUDA Toolkit 11.8, 12.0 and 12.1. They are not guaranteed to
                                    						be forward compatible with future CUDA 12.x Toolkits.
                                 </li>
                                 <li class="li liexpand">Within the cuDNN version 8 backend API, the following engines are known not
                                    						to be thread-safe when executed simultaneously with multiple threads sharing
                                    						the same execution plan:
                                    
                                    
                                    <div class="tablenoborder"><a name="rel-897__table_t2h_lsp_r5b" shape="rect">
                                          <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="rel-897__table_t2h_lsp_r5b" class="table" frame="border" border="1" rules="all">
                                          <caption><span class="tablecap">Table 1. Engines That Are Not Thread-Safe</span></caption>
                                          <thead class="thead" align="left">
                                             <tr class="row">
                                                <th class="entry" valign="top" width="20%" id="d54e376" rowspan="1" colspan="1"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e380" rowspan="1" colspan="1"><samp class="ph codeph">convolution forward</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e384" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward data</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e388" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward filter</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e392" rowspan="1" colspan="1"><samp class="ph codeph">cudnnConvolutionBiasActivationForward</samp></th>
                                             </tr>
                                          </thead>
                                          <tbody class="tbody">
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e376" rowspan="1" colspan="1">A100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e380" rowspan="1" colspan="1"><a name="rel-897__ul_c53_qsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-897__ul_c53_qsp_r5b">
                                                      <li class="li">15</li>
                                                      <li class="li">20</li>
                                                      <li class="li">21</li>
                                                      <li class="li">22</li>
                                                      <li class="li">23</li>
                                                      <li class="li">25</li>
                                                      <li class="li">27</li>
                                                      <li class="li">34</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">39</li>
                                                      <li class="li">40</li>
                                                      <li class="li">41</li>
                                                      <li class="li">43</li>
                                                      <li class="li">45</li>
                                                      <li class="li">46</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                      <li class="li">57</li>
                                                      <li class="li">59</li>
                                                      <li class="li">60</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e384" rowspan="1" colspan="1"><a name="rel-897__ul_rh2_ssp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-897__ul_rh2_ssp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">19</li>
                                                      <li class="li">20</li>
                                                      <li class="li">22</li>
                                                      <li class="li">23</li>
                                                      <li class="li">24</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">27</li>
                                                      <li class="li">28</li>
                                                      <li class="li">32</li>
                                                      <li class="li">33</li>
                                                      <li class="li">34</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">40</li>
                                                      <li class="li">43</li>
                                                      <li class="li">46</li>
                                                      <li class="li">49</li>
                                                      <li class="li">51</li>
                                                      <li class="li">52</li>
                                                      <li class="li">53</li>
                                                      <li class="li">54</li>
                                                      <li class="li">55</li>
                                                      <li class="li">56</li>
                                                      <li class="li">57</li>
                                                      <li class="li">58</li>
                                                      <li class="li">59</li>
                                                      <li class="li">60</li>
                                                      <li class="li">62</li>
                                                      <li class="li">64</li>
                                                      <li class="li">65</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e388" rowspan="1" colspan="1"><a name="rel-897__ul_mjw_vsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-897__ul_mjw_vsp_r5b">
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">13</li>
                                                      <li class="li">15</li>
                                                      <li class="li">21</li>
                                                      <li class="li">22</li>
                                                      <li class="li">23</li>
                                                      <li class="li">24</li>
                                                      <li class="li">25</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">33</li>
                                                      <li class="li">34</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">39</li>
                                                      <li class="li">40</li>
                                                      <li class="li">45</li>
                                                      <li class="li">46</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                      <li class="li">49</li>
                                                      <li class="li">50</li>
                                                      <li class="li">55</li>
                                                      <li class="li">56</li>
                                                      <li class="li">61</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e392" rowspan="1" colspan="1"><a name="rel-897__ul_mfh_1d3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-897__ul_mfh_1d3_kvb">
                                                      <li class="li">4005</li>
                                                      <li class="li">4006</li>
                                                      <li class="li">4007</li>
                                                      <li class="li">4010</li>
                                                      <li class="li">4017</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4023</li>
                                                      <li class="li">4024</li>
                                                      <li class="li">4025</li>
                                                      <li class="li">4026</li>
                                                      <li class="li">4027</li>
                                                      <li class="li">4028</li>
                                                      <li class="li">4029</li>
                                                      <li class="li">4032</li>
                                                      <li class="li">4033</li>
                                                      <li class="li">4037</li>
                                                      <li class="li">4038</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e376" rowspan="1" colspan="1">V100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e380" rowspan="1" colspan="1"><a name="rel-897__ul_qrn_btp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-897__ul_qrn_btp_r5b">
                                                      <li class="li">7</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">11</li>
                                                      <li class="li">12</li>
                                                      <li class="li">16</li>
                                                      <li class="li">17</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">32</li>
                                                      <li class="li">33</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">43</li>
                                                      <li class="li">49</li>
                                                      <li class="li">57</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e384" rowspan="1" colspan="1"><a name="rel-897__ul_ffy_dtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-897__ul_ffy_dtp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">11</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">14</li>
                                                      <li class="li">15</li>
                                                      <li class="li">16</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">41</li>
                                                      <li class="li">44</li>
                                                      <li class="li">47</li>
                                                      <li class="li">62</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e388" rowspan="1" colspan="1"><a name="rel-897__ul_lg1_htp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-897__ul_lg1_htp_r5b">
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">6</li>
                                                      <li class="li">7</li>
                                                      <li class="li">11</li>
                                                      <li class="li">14</li>
                                                      <li class="li">21</li>
                                                      <li class="li">32</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">42</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">52</li>
                                                      <li class="li">61</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e392" rowspan="1" colspan="1"><a name="rel-897__ul_w1l_cd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-897__ul_w1l_cd3_kvb">
                                                      <li class="li">4001</li>
                                                      <li class="li">4002</li>
                                                      <li class="li">4008</li>
                                                      <li class="li">4015</li>
                                                      <li class="li">4016</li>
                                                      <li class="li">4017</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4021</li>
                                                      <li class="li">4030</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e376" rowspan="1" colspan="1">T4</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e380" rowspan="1" colspan="1"><a name="rel-897__ul_q3j_jtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-897__ul_q3j_jtp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">14</li>
                                                      <li class="li">15</li>
                                                      <li class="li">16</li>
                                                      <li class="li">18</li>
                                                      <li class="li">19</li>
                                                      <li class="li">24</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">43</li>
                                                      <li class="li">50</li>
                                                      <li class="li">57</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e384" rowspan="1" colspan="1"><a name="rel-897__ul_ax4_ntp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-897__ul_ax4_ntp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">15</li>
                                                      <li class="li">18</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">37</li>
                                                      <li class="li">39</li>
                                                      <li class="li">42</li>
                                                      <li class="li">44</li>
                                                      <li class="li">45</li>
                                                      <li class="li">48</li>
                                                      <li class="li">62</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e388" rowspan="1" colspan="1"><a name="rel-897__ul_j3t_rtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-897__ul_j3t_rtp_r5b">
                                                      <li class="li">5</li>
                                                      <li class="li">12</li>
                                                      <li class="li">21</li>
                                                      <li class="li">26</li>
                                                      <li class="li">27</li>
                                                      <li class="li">28</li>
                                                      <li class="li">32</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">53</li>
                                                      <li class="li">54</li>
                                                      <li class="li">61</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e392" rowspan="1" colspan="1"><a name="rel-897__ul_kc5_jd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-897__ul_kc5_jd3_kvb">
                                                      <li class="li">4003</li>
                                                      <li class="li">4004</li>
                                                      <li class="li">4007</li>
                                                      <li class="li">4009</li>
                                                      <li class="li">4015</li>
                                                      <li class="li">4017</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4022</li>
                                                      <li class="li">4031</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                          </tbody>
                                       </table>
                                    </div>
                                 </li>
                                 <li class="li liexpand">The status returned by <samp class="ph codeph">cudnnBackendFinalize()</samp> or
                                    							<samp class="ph codeph">cudnnBackendExecute()</samp> on a
                                    							<samp class="ph codeph">CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR</samp> may change
                                    						depending on the version of the dynamic dependencies of cuDNN. As of this
                                    						writing, only cuBLAS is known to affect the return status of these function
                                    						calls.
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not
                                    						required to consider padding. Users of cuDNN can witness an unexpected lack
                                    						of problem support when forward convolution spatial dimensions are less than
                                    						the filter size and padding is nonzero but is sufficient to extend spatial
                                    						dimensions to or beyond filter dimensions. This is commonly observed with,
                                    						but not limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand">When performing batch normalization in cuDNN, the operation is allowed to
                                    						proceed if the output tensor strides are overlapping, however, there is no
                                    						guarantee of deterministic results.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 25</samp> for convolution
                                    						backwards data (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp>) does not support
                                    						tensors in which the product N*C*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX =1</samp> for convolution backwards
                                    						data (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp>) does not support
                                    						tensors in which the product N*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. This issue has been present in all previous releases of
                                    						cuDNN and exercising the use case for the engine would show incorrect
                                    						results.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later. It also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples must be installed in a writable location. If not installed in a
                                    						writable location, the samples can crash.
                                 </li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit nondeterministic
                                       							behavior when the cuDNN library is built with CUDA Toolkit 10.2 or
                                       							higher. This is the result of a new buffer management and heuristics in
                                       							the cuBLAS library. As described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This happens
                                       							when two buffer sizes (16 KB and 4 MB) are used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting
                                       							for a buffer of that size to be released, a smaller buffer may be used
                                       							with a different GPU kernel. The kernel selection may affect numerical
                                       							results. The user can eliminate the nondeterministic behavior of cuDNN
                                       							RNN and multihead attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16
                                       							KB each in GPU memory while the second setting creates two buffers of 4
                                       							MB each. The default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors
                                    						in order to run efficiently. As always, cuDNN recommends users to align
                                    						tensors to 16 byte boundaries that will be sufficiently aligned for any
                                    						computational option in cuDNN. Doing otherwise may cause performance
                                    						regressions in cuDNN 8.0.x compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and
                                    						the output is in FP16 (half float), there are cases where the numerical
                                    						accuracy between the different algorithms might differ. cuDNN 8.0.x users
                                    						can target the backend API to query the numerical notes of the algorithms to
                                    						get the information programmatically. There are cases where algo0 and algo1
                                    						will have a reduced precision accumulation when users target the legacy API.
                                    						In all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and
                                    						backward filter, grouped convolution with groups larger than 1 and with odd
                                    						product of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D
                                    						convolution), <samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on
                                    						devices older than NVIDIA Volta. To prevent a potential illegal memory
                                    						access by an instruction that only has a 16-bit version in NVIDIA Volta and
                                    						later, pad at least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use
                                    						texture-based load structure for performance improvements particularly in
                                    						older hardware architectures. Users can opt out of using texture using the
                                    						environmental variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this
                                    						variable is removed. Texture loading is turned off by default. Users who
                                    						want to continue to use texture-based load, can adapt the new backend API,
                                    						and toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check
                                    						API (for example, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command
                                    						explicitly to resolve the undefined symbols from cuDNN static
                                    						libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA
                                    						Pascal and later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-897__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-897__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in
                                    						compatibility mode are tested with compute-sanitizer,
                                    							<samp class="ph codeph">cuGetProcAddress</samp> failures with error code 500 will
                                    						arise due to missing functions. This error can be ignored, or suppressed
                                    						with the <samp class="ph codeph">--report-api-errors no</samp> option, as this is due to
                                    						CUDA backward compatibility checking if a function is usable with the CUDA
                                    						toolkit combination. The functions are introduced in a later version of CUDA
                                    						but are not available on the current platform. The absence of these
                                    						functions is harmless and will not give rise to any functional issues.
                                 </li>
                                 <li class="li liexpand">Users of <samp class="ph codeph">cudnn_cnn_infer_static.a</samp> may need to update their
                                    						application linkage so that symbols absent in that library are subsequently
                                    						made available with <samp class="ph codeph">cudnn_ops_infer_static.a</samp>. On Linux,
                                    						this is specifying the ops library after <samp class="ph codeph">cnn</samp> on the linker
                                    						line. The same applies to <samp class="ph codeph">cudnn_cnn_train_static.a</samp> and
                                    							<samp class="ph codeph">cudnn_ops_train_static.a</samp>.
                                 </li>
                                 <li class="li liexpand">The fused attention and flash attention runtime engines have been disabled
                                    						for NVRTC 11.8 due to compiler limitations.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-896"><a name="rel-896" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-896" name="rel-896" shape="rect">1.2.&nbsp;cuDNN Release 8.9.6</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">These are the NVIDIA cuDNN 8.9.6 Release Notes. These Release Notes include fixes
                           		from the previous cuDNN releases as well as the following additional changes.
                        </p>
                        <div class="section" id="rel-896__section_l2m_s4c_2jb"><a name="rel-896__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">These Release Notes are applicable to both cuDNN and NVIDIA JetPack™
                              				users of cuDNN unless appended specifically with <em class="ph i">(not applicable for Jetson
                                 					platforms)</em>.
                           </p>
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-896__section_wl4_4l4_vyb"><a name="rel-896__section_wl4_4l4_vyb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-896__ul_u1h_pl4_vyb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-896__ul_u1h_pl4_vyb">
                                 <li class="li liexpand">RMS normalization is now supported by cuDNN for the forward training and
                                    						backwards pass with <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 3</samp> and
                                    						with <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 4</samp> for the forwards
                                    						inference pass.
                                 </li>
                                 <li class="li liexpand">Expanded support of FP16 and BF16 flash attention by adding the gradient for
                                    						relative positional encoding on NVIDIA Hopper GPUs.
                                 </li>
                                 <li class="li liexpand">FP16 and BF16 fused flash attention engine <samp class="ph codeph">fprop</samp>
                                    						performance has been improved for NVIDIA Hopper and Ampere GPUs giving a
                                    						speed-up of up to 20% over cuDNN 8.9.5.
                                 </li>
                                 <li class="li liexpand">
                                    <p class="p">H100 only: For input fusion of matrix multiplications, tensor A now supports arbitrary
                                       							input type with scalar, row, and column broadcast or full tensor
                                       							operation, and the graph of mainloop fusion can be branchy. Inputs will
                                       							be automatically casted to the compute type and results will be
                                       							converted to the data type specified by the user. 
                                    </p>
                                    <div class="p">We recommend users to set intermediate data as float to avoid unnecessary type conversion.
                                       							Tensor A and B no longer need to be in the same data type.
                                       <div class="note note"><span class="notetitle">Note:</span> A/B
                                          								inputs for matrix multiplication operation should have matching data
                                          								types. Otherwise, an explicit
                                          									<samp class="ph codeph">CUDNN_POINTWISE_IDENTITY</samp> operation is needed to
                                          								perform data type conversion after mainloop fusion.
                                       </div>
                                    </div>
                                 </li>
                                 <li class="li liexpand">Updated runtime fusion engine heuristics for FP16 and BF16 for Matmul
                                    						operation and INT8 for both Convolution and Matmul operations on the NVIDIA
                                    						Hopper architecture to support new batched GEMM kernel, with improved fusion
                                    						performance, and compilation time.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-896__section_w52_kwk_bzb"><a name="rel-896__section_w52_kwk_bzb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:<a name="rel-896__ul_vsk_lwk_bzb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-896__ul_vsk_lwk_bzb">
                                 <li class="li liexpand">Fixed a race condition where a kernel hang was observed in the <samp class="ph codeph">ConvBNfprop</samp>
                                    						pattern matching engine on the NVIDIA Hopper architecture when multiple
                                    						threads were executing concurrently and the system was under heavy
                                    						load.
                                 </li>
                                 <li class="li liexpand">Fixed the behavior note for the <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 3</samp>
                                    						instance normalization engine.
                                 </li>
                                 <li class="li liexpand">On NVIDIA Hopper architectures, incorrect results were possible in the
                                    						backpropagation of FP16 and BF16 flash attention with dropout enabled. This
                                    						issue has been fixed.
                                 </li>
                                 <li class="li liexpand">Possibility of a race condition with multiple threads executing the same
                                    						execution plan in BF16 and FP16 flash attention has been fixed.
                                 </li>
                                 <li class="li liexpand">On Hopper architectures, FP16 and BF16 fused flash attention training needed
                                    						the layout of the gradient and the input layout to be the same. This issue
                                    						has been fixed.
                                 </li>
                                 <li class="li liexpand">Fixed typos in the sample code of <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#use-cases" target="_blank" shape="rect">Use Cases</a> in the cuDNN API
                                    						Reference documentation.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-896__section_t2m_s5p_nnb"><a name="rel-896__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel-896__ul_mbs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-896__ul_mbs_qgh_fsb">
                                 <li class="li liexpand">For the <samp class="ph codeph">ConvBiasAct</samp> operation,
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 39</samp> may return
                                    							<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> when running on a system
                                    						with CUDA Toolkit version 11.0.3 through 11.1. Upgrading the CUDA Toolkit
                                    						version to 11.1 Update 1 or later should resolve this issue.
                                 </li>
                                 <li class="li liexpand">ZLIB version 1.2.13 is statically linked into the cuDNN Windows dynamic
                                    						libraries. Changing to the static linkage of ZLIB for other platforms will
                                    						be addressed in future cuDNN releases.
                                 </li>
                                 <li class="li liexpand">A race condition in memory write accesses was flagged by the
                                    						"compute-sanitizer" tool in some cuBLAS kernels invoked by the cuDNN
                                    						multihead attention API <samp class="ph codeph">cudnnMultiHeadAttnForward()</samp> on H100
                                    						GPUs. Extensive testing on H100, with different clock speeds and
                                    						computational loads, did not reveal any impact on functional results that
                                    						were always identical and correct. This issue is currently being
                                    						investigated. 
                                 </li>
                                 <li class="li liexpand">cuDNN's usage of cuBLAS from CUDA Toolkit 12.1 may result in race-check
                                    						failures when the library is tested under compute sanitizer. These failures
                                    						are believed to be a cuBLAS issue and are being investigated. A workaround
                                    						for this issue is to use cuBLAS from CUDA Toolkit 12.0.
                                 </li>
                                 <li class="li liexpand">A compiler bug in NVRTC in CUDA version 11.7 and earlier, was causing
                                    						incorrect outputs when computing logical operations on boolean input tensors
                                    						in the runtime fusion engine. A workaround had been integrated to avoid the
                                    						most common issues. However, it is highly recommended to update to at least
                                    						CUDA version 11.7u1 for a fix. Specifically, known failure cases are when
                                    						pointwise operations of mode <samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_NOT</samp>,
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_AND</samp> or
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_OR</samp> operates on boolean tensors.
                                    						This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 57</samp> for convolution forward
                                    						and <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 62</samp> for convolution
                                    						backward data may have performance regressions for non-zero beta problems.
                                    						However, they are not typically recommended by cuDNN heuristics, so the
                                    						observable impact should be minimal.
                                 </li>
                                 <li class="li liexpand">Use of <samp class="ph codeph">cudnnFusedOpsExecute()</samp> on NVIDIA Volta compatible
                                    						architectures hosted on AArch64 systems may generate incorrect results when
                                    						used with <samp class="ph codeph">cudnnFusedOps_t</samp> set to
                                    							<samp class="ph codeph">CUDNN_FUSED_SCALE_BIAS_ACTIVATION_WGRAD</samp>.
                                 </li>
                                 <li class="li liexpand">With CUDA 11.7, the NVRTC library may cause a small memory leak when using
                                    						the runtime fusion engine. The issue has been addressed in CUDA Toolkit 11.8
                                    						or later. 
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 0</samp> for
                                    							<samp class="ph codeph">DgradDreluBnBwdWeights</samp> may see a performance regression
                                    						when moving from cuDNN 8.8 to cuDNN 8.9.
                                 </li>
                                 <li class="li liexpand">If cuDNN 8.4.1 or earlier statically links with
                                    							<samp class="ph codeph">libcudart.so</samp> from the CUDA Toolkit 11.7 or later, when
                                    						the LFL feature is activated, the results from
                                    							<samp class="ph codeph">cudnnFind*Algo</samp> will not be accurate.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX
                                    						3090 compared to 2080 Ti. This includes EfficientNet with up to 6x
                                    						performance difference, UNet up to 1.6x performance difference and Tacotron
                                    						up to 1.6x performance difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    							<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior,
                                    						accumulator data types, and so on, have not been documented as of yet.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with filter size
                                    						1x1. The severity would be different depending on which version of the CUDA
                                    						Toolkit the user is using.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with high group
                                    						count. The issue is more severe on V100.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-896__section_dph_zkv_jrb"><a name="rel-896__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and
                              				the NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-896__section_nbs_qgh_fsb"><a name="rel-896__section_nbs_qgh_fsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel-896__ul_obs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-896__ul_obs_qgh_fsb">
                                 <li class="li liexpand">Disabling CUDA context preemption on Windows can sometimes lead to
                                    							<samp class="ph codeph">CUDNN_INTERNAL_ERRORS</samp> being returned from convolution
                                    						runs. When using cuDNN, do not disable CUDA context preemption.
                                 </li>
                                 <li class="li liexpand">When using the cuDNN static library, you will need to use the same
                                    						major.minor version of the CUDA Toolkit by which cuDNN was built to build
                                    						your application. Refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a> for
                                    						the exact supported CUDA versions.
                                 </li>
                                 <li class="li liexpand">In cuDNN 8.9.0, runtime fusion engines (with
                                    							<samp class="ph codeph">CUDNN_BEHAVIOR_NOTE_RUNTIME_COMPILATION</samp>) will only work
                                    						with NVRTC from CUDA Toolkit 11.8, 12.0 and 12.1. They are not guaranteed to
                                    						be forward compatible with future CUDA 12.x Toolkits.
                                 </li>
                                 <li class="li liexpand">Within the cuDNN version 8 backend API, the following engines are known not
                                    						to be thread-safe when executed simultaneously with multiple threads sharing
                                    						the same execution plan:
                                    
                                    
                                    <div class="tablenoborder"><a name="rel-896__table_t2h_lsp_r5b" shape="rect">
                                          <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="rel-896__table_t2h_lsp_r5b" class="table" frame="border" border="1" rules="all">
                                          <caption><span class="tablecap">Table 2. Engines That Are Not Thread-Safe</span></caption>
                                          <thead class="thead" align="left">
                                             <tr class="row">
                                                <th class="entry" valign="top" width="20%" id="d54e1844" rowspan="1" colspan="1"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e1848" rowspan="1" colspan="1"><samp class="ph codeph">convolution forward</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e1852" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward data</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e1856" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward filter</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e1860" rowspan="1" colspan="1"><samp class="ph codeph">cudnnConvolutionBiasActivationForward</samp></th>
                                             </tr>
                                          </thead>
                                          <tbody class="tbody">
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e1844" rowspan="1" colspan="1">A100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e1848" rowspan="1" colspan="1"><a name="rel-896__ul_c53_qsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-896__ul_c53_qsp_r5b">
                                                      <li class="li">15</li>
                                                      <li class="li">20</li>
                                                      <li class="li">21</li>
                                                      <li class="li">22</li>
                                                      <li class="li">23</li>
                                                      <li class="li">25</li>
                                                      <li class="li">27</li>
                                                      <li class="li">34</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">39</li>
                                                      <li class="li">40</li>
                                                      <li class="li">41</li>
                                                      <li class="li">43</li>
                                                      <li class="li">45</li>
                                                      <li class="li">46</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                      <li class="li">57</li>
                                                      <li class="li">59</li>
                                                      <li class="li">60</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e1852" rowspan="1" colspan="1"><a name="rel-896__ul_rh2_ssp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-896__ul_rh2_ssp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">19</li>
                                                      <li class="li">20</li>
                                                      <li class="li">22</li>
                                                      <li class="li">23</li>
                                                      <li class="li">24</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">27</li>
                                                      <li class="li">28</li>
                                                      <li class="li">32</li>
                                                      <li class="li">33</li>
                                                      <li class="li">34</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">40</li>
                                                      <li class="li">43</li>
                                                      <li class="li">46</li>
                                                      <li class="li">49</li>
                                                      <li class="li">51</li>
                                                      <li class="li">52</li>
                                                      <li class="li">53</li>
                                                      <li class="li">54</li>
                                                      <li class="li">55</li>
                                                      <li class="li">56</li>
                                                      <li class="li">57</li>
                                                      <li class="li">58</li>
                                                      <li class="li">59</li>
                                                      <li class="li">60</li>
                                                      <li class="li">62</li>
                                                      <li class="li">64</li>
                                                      <li class="li">65</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e1856" rowspan="1" colspan="1"><a name="rel-896__ul_mjw_vsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-896__ul_mjw_vsp_r5b">
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">13</li>
                                                      <li class="li">15</li>
                                                      <li class="li">21</li>
                                                      <li class="li">22</li>
                                                      <li class="li">23</li>
                                                      <li class="li">24</li>
                                                      <li class="li">25</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">33</li>
                                                      <li class="li">34</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">39</li>
                                                      <li class="li">40</li>
                                                      <li class="li">45</li>
                                                      <li class="li">46</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                      <li class="li">49</li>
                                                      <li class="li">50</li>
                                                      <li class="li">55</li>
                                                      <li class="li">56</li>
                                                      <li class="li">61</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e1860" rowspan="1" colspan="1"><a name="rel-896__ul_mfh_1d3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-896__ul_mfh_1d3_kvb">
                                                      <li class="li">4005</li>
                                                      <li class="li">4006</li>
                                                      <li class="li">4007</li>
                                                      <li class="li">4010</li>
                                                      <li class="li">4017</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4023</li>
                                                      <li class="li">4024</li>
                                                      <li class="li">4025</li>
                                                      <li class="li">4026</li>
                                                      <li class="li">4027</li>
                                                      <li class="li">4028</li>
                                                      <li class="li">4029</li>
                                                      <li class="li">4032</li>
                                                      <li class="li">4033</li>
                                                      <li class="li">4037</li>
                                                      <li class="li">4038</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e1844" rowspan="1" colspan="1">V100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e1848" rowspan="1" colspan="1"><a name="rel-896__ul_qrn_btp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-896__ul_qrn_btp_r5b">
                                                      <li class="li">7</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">11</li>
                                                      <li class="li">12</li>
                                                      <li class="li">16</li>
                                                      <li class="li">17</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">32</li>
                                                      <li class="li">33</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">43</li>
                                                      <li class="li">49</li>
                                                      <li class="li">57</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e1852" rowspan="1" colspan="1"><a name="rel-896__ul_ffy_dtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-896__ul_ffy_dtp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">11</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">14</li>
                                                      <li class="li">15</li>
                                                      <li class="li">16</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">41</li>
                                                      <li class="li">44</li>
                                                      <li class="li">47</li>
                                                      <li class="li">62</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e1856" rowspan="1" colspan="1"><a name="rel-896__ul_lg1_htp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-896__ul_lg1_htp_r5b">
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">6</li>
                                                      <li class="li">7</li>
                                                      <li class="li">11</li>
                                                      <li class="li">14</li>
                                                      <li class="li">21</li>
                                                      <li class="li">32</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">42</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">52</li>
                                                      <li class="li">61</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e1860" rowspan="1" colspan="1"><a name="rel-896__ul_w1l_cd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-896__ul_w1l_cd3_kvb">
                                                      <li class="li">4001</li>
                                                      <li class="li">4002</li>
                                                      <li class="li">4008</li>
                                                      <li class="li">4015</li>
                                                      <li class="li">4016</li>
                                                      <li class="li">4017</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4021</li>
                                                      <li class="li">4030</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e1844" rowspan="1" colspan="1">T4</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e1848" rowspan="1" colspan="1"><a name="rel-896__ul_q3j_jtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-896__ul_q3j_jtp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">14</li>
                                                      <li class="li">15</li>
                                                      <li class="li">16</li>
                                                      <li class="li">18</li>
                                                      <li class="li">19</li>
                                                      <li class="li">24</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">43</li>
                                                      <li class="li">50</li>
                                                      <li class="li">57</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e1852" rowspan="1" colspan="1"><a name="rel-896__ul_ax4_ntp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-896__ul_ax4_ntp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">15</li>
                                                      <li class="li">18</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">37</li>
                                                      <li class="li">39</li>
                                                      <li class="li">42</li>
                                                      <li class="li">44</li>
                                                      <li class="li">45</li>
                                                      <li class="li">48</li>
                                                      <li class="li">62</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e1856" rowspan="1" colspan="1"><a name="rel-896__ul_j3t_rtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-896__ul_j3t_rtp_r5b">
                                                      <li class="li">5</li>
                                                      <li class="li">12</li>
                                                      <li class="li">21</li>
                                                      <li class="li">26</li>
                                                      <li class="li">27</li>
                                                      <li class="li">28</li>
                                                      <li class="li">32</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">53</li>
                                                      <li class="li">54</li>
                                                      <li class="li">61</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e1860" rowspan="1" colspan="1"><a name="rel-896__ul_kc5_jd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-896__ul_kc5_jd3_kvb">
                                                      <li class="li">4003</li>
                                                      <li class="li">4004</li>
                                                      <li class="li">4007</li>
                                                      <li class="li">4009</li>
                                                      <li class="li">4015</li>
                                                      <li class="li">4017</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4022</li>
                                                      <li class="li">4031</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                          </tbody>
                                       </table>
                                    </div>
                                 </li>
                                 <li class="li liexpand">The status returned by <samp class="ph codeph">cudnnBackendFinalize()</samp> or
                                    							<samp class="ph codeph">cudnnBackendExecute()</samp> on a
                                    							<samp class="ph codeph">CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR</samp> may change
                                    						depending on the version of the dynamic dependencies of cuDNN. As of this
                                    						writing, only cuBLAS is known to affect the return status of these function
                                    						calls.
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not
                                    						required to consider padding. Users of cuDNN can witness an unexpected lack
                                    						of problem support when forward convolution spatial dimensions are less than
                                    						the filter size and padding is nonzero but is sufficient to extend spatial
                                    						dimensions to or beyond filter dimensions. This is commonly observed with,
                                    						but not limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand">When performing batch normalization in cuDNN, the operation is allowed to
                                    						proceed if the output tensor strides are overlapping, however, there is no
                                    						guarantee of deterministic results.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 25</samp> for convolution
                                    						backwards data (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp>) does not support
                                    						tensors in which the product N*C*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX =1</samp> for convolution backwards data (which is
                                    						part of legacy <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp>) does not
                                    						support tensors in which the product N*H*W of the output gradient tensor
                                    						equals to or exceeds 2^31. This issue has been present in all previous
                                    						releases of cuDNN and exercising the use case for the engine would show
                                    						incorrect results.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later. It also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples must be installed in a writable location. If not installed in a
                                    						writable location, the samples can crash.
                                 </li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit nondeterministic
                                       							behavior when the cuDNN library is built with CUDA Toolkit 10.2 or
                                       							higher. This is the result of a new buffer management and heuristics in
                                       							the cuBLAS library. As described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This happens
                                       							when two buffer sizes (16 KB and 4 MB) are used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting
                                       							for a buffer of that size to be released, a smaller buffer may be used
                                       							with a different GPU kernel. The kernel selection may affect numerical
                                       							results. The user can eliminate the nondeterministic behavior of cuDNN
                                       							RNN and multihead attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16
                                       							KB each in GPU memory while the second setting creates two buffers of 4
                                       							MB each. The default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors
                                    						in order to run efficiently. As always, cuDNN recommends users to align
                                    						tensors to 16 byte boundaries that will be sufficiently aligned for any
                                    						computational option in cuDNN. Doing otherwise may cause performance
                                    						regressions in cuDNN 8.0.x compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and
                                    						the output is in FP16 (half float), there are cases where the numerical
                                    						accuracy between the different algorithms might differ. cuDNN 8.0.x users
                                    						can target the backend API to query the numerical notes of the algorithms to
                                    						get the information programmatically. There are cases where algo0 and algo1
                                    						will have a reduced precision accumulation when users target the legacy API.
                                    						In all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and
                                    						backward filter, grouped convolution with groups larger than 1 and with odd
                                    						product of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D
                                    						convolution), <samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on
                                    						devices older than NVIDIA Volta. To prevent a potential illegal memory
                                    						access by an instruction that only has a 16-bit version in NVIDIA Volta and
                                    						later, pad at least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use
                                    						texture-based load structure for performance improvements particularly in
                                    						older hardware architectures. Users can opt out of using texture using the
                                    						environmental variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this
                                    						variable is removed. Texture loading is turned off by default. Users who
                                    						want to continue to use texture-based load, can adapt the new backend API,
                                    						and toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check
                                    						API (for example, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command
                                    						explicitly to resolve the undefined symbols from cuDNN static
                                    						libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA
                                    						Pascal and later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-896__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-896__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in
                                    						compatibility mode are tested with compute-sanitizer,
                                    							<samp class="ph codeph">cuGetProcAddress</samp> failures with error code 500 will
                                    						arise due to missing functions. This error can be ignored, or suppressed
                                    						with the <samp class="ph codeph">--report-api-errors no</samp> option, as this is due to
                                    						CUDA backward compatibility checking if a function is usable with the CUDA
                                    						toolkit combination. The functions are introduced in a later version of CUDA
                                    						but are not available on the current platform. The absence of these
                                    						functions is harmless and will not give rise to any functional issues.
                                 </li>
                                 <li class="li liexpand">Users of <samp class="ph codeph">cudnn_cnn_infer_static.a</samp> may need to update their
                                    						application linkage so that symbols absent in that library are subsequently
                                    						made available with <samp class="ph codeph">cudnn_ops_infer_static.a</samp>. On Linux,
                                    						this is specifying the ops library after <samp class="ph codeph">cnn</samp> on the linker
                                    						line. The same applies to <samp class="ph codeph">cudnn_cnn_train_static.a</samp> and
                                    							<samp class="ph codeph">cudnn_ops_train_static.a</samp>.
                                 </li>
                                 <li class="li liexpand">The fused attention and flash attention runtime engines have been disabled
                                    						for NVRTC 11.8 due to compiler limitations.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-895"><a name="rel-895" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-895" name="rel-895" shape="rect">1.3.&nbsp;cuDNN Release 8.9.5</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">These are the NVIDIA cuDNN 8.9.5 Release Notes. These Release Notes include fixes
                           		from the previous cuDNN releases as well as the following additional changes.
                        </p>
                        <div class="section" id="rel-895__section_l2m_s4c_2jb"><a name="rel-895__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">These Release Notes are applicable to both cuDNN and NVIDIA JetPack™
                              				users of cuDNN unless appended specifically with <em class="ph i">(not applicable for Jetson
                                 					platforms)</em>.
                           </p>
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-895__section_kl4_5hq_ryb"><a name="rel-895__section_kl4_5hq_ryb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Announcements</h3>
                           <div class="p"><a name="rel-895__ul_b3d_vhq_ryb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-895__ul_b3d_vhq_ryb">
                                 <li class="li">cuDNN for CUDA 12.x no longer supports Ubuntu 18.04.</li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-895__section_n3n_k4r_pyb"><a name="rel-895__section_n3n_k4r_pyb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-895__ul_xwd_l4r_pyb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-895__ul_xwd_l4r_pyb">
                                 <li class="li liexpand">Layer normalization forward pass and backward pass performance has been
                                    						improved on NVIDIA Ampere and NVIDIA Hopper GPUs for popular large language
                                    						models (LLM), providing a speedup of nearly 2x-40x over cuDNN 8.9.4.
                                 </li>
                                 <li class="li liexpand">SM carveout is supported on NVIDIA Hopper; whereby you can set the number of SMs to target
                                    						for parallel execution. This feature allows expert users to reserve SMs for
                                    						concurrent execution on a separate CUDA stream.
                                 </li>
                                 <li class="li liexpand">FP16 and BF16 fused flash attention engine <samp class="ph codeph">bprop</samp>
                                    						performance has been improved for NVIDIA Hopper GPUs giving a speedup of
                                    						nearly 2x-3x over cuDNN 8.9.4.
                                 </li>
                                 <li class="li liexpand">For small batch sizes, FP8 fused flash attention is 25% - 50% faster
                                    						compared to cuDNN 8.9.4.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-895__section_yw4_15g_nyb"><a name="rel-895__section_yw4_15g_nyb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:<a name="rel-895__ul_fqs_b5g_nyb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-895__ul_fqs_b5g_nyb">
                                 <li class="li liexpand">For NVIDIA Hopper architectures, we fixed a memory leak in the
                                    							<samp class="ph codeph">DgradDreluBNBwdWeight</samp>, <samp class="ph codeph">ConvBNwgrad</samp>,
                                    						and <samp class="ph codeph">ConvBNfprop</samp> pattern matching engines.
                                 </li>
                                 <li class="li liexpand">A race condition that could cause numerically inaccurate results when computing batch
                                    						normalization in a multi-GPU setting has been corrected. This issue affected
                                    						all previous 8.9 releases for batch norm engines with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 0</samp>.
                                 </li>
                                 <li class="li liexpand">On NVIDIA Ampere and Hopper architectures, incorrect results were possible in
                                    						the backpropagation of FP16 and BF16 flash attention with dropout enabled.
                                    						This issue has been fixed.
                                 </li>
                                 <li class="li liexpand">On NVIDIA Ampere and Hopper architectures, incorrect results were possible in
                                    						variable sequence lengths and when the sequence length was not a multiple of
                                    						128. This issue has been fixed.
                                 </li>
                                 <li class="li liexpand">For NVIDIA Hopper architectures, for <samp class="ph codeph">DgradDreluBNBwdWeight</samp>,
                                    							<samp class="ph codeph">ConvBNwgrad</samp>, and <samp class="ph codeph">ConvBNfprop</samp> matched
                                    						engines, static CGA was deprecated and dynamic CGA is supported. You can
                                    						only use <samp class="ph codeph">CUDNN_KNOB_TYPE_TILE_CGA_M</samp> and
                                    							<samp class="ph codeph">CUDNN_KNOB_TYPE_TILE_CGA_N</samp> knobs to set CGA size. The
                                    							<samp class="ph codeph">CUDNN_KNOB_TYPE_TILE_CGA</samp> knobs are deprecated for
                                    							<samp class="ph codeph">DgradDreluBNBwdWeight</samp>, <samp class="ph codeph">ConvBNwgrad</samp>,
                                    						and <samp class="ph codeph">ConvBNfprop</samp> engines.
                                 </li>
                                 <li class="li liexpand">The documentation of <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnDestroy" target="_blank" shape="rect">cudnnDestroy</a> has been updated to
                                    						reflect the proper ordering of this call with respect to destroying the CUDA
                                    						context.
                                 </li>
                                 <li class="li liexpand">FP8 fused flash attention was slower than FP16 fused flash attention for
                                    						small batch sizes. This issue has been fixed.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-895__section_t2m_s5p_nnb"><a name="rel-895__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel-895__ul_mbs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-895__ul_mbs_qgh_fsb">
                                 <li class="li liexpand">On Hopper architectures, FP16 and BF16 fused flash attention training needs
                                    						the layout of the gradient and the input layout to be the same.
                                 </li>
                                 <li class="li liexpand">ZLIB version 1.2.13 is statically linked into the cuDNN Windows dynamic
                                    						libraries. Changing to the static linkage of ZLIB for other platforms will
                                    						be addressed in future cuDNN releases.
                                 </li>
                                 <li class="li liexpand">A race condition in memory write accesses was flagged by the
                                    						"compute-sanitizer" tool in some cuBLAS kernels invoked by the cuDNN
                                    						multihead attention API <samp class="ph codeph">cudnnMultiHeadAttnForward()</samp> on H100
                                    						GPUs. Extensive testing on H100, with different clock speeds and
                                    						computational loads, did not reveal any impact on functional results that
                                    						were always identical and correct. This issue is currently being
                                    						investigated. 
                                 </li>
                                 <li class="li liexpand">cuDNN's usage of cuBLAS from CUDA Toolkit 12.1 may result in race-check
                                    						failures when the library is tested under compute sanitizer. These failures
                                    						are believed to be a cuBLAS issue and are being investigated. A workaround
                                    						for this issue is to use cuBLAS from CUDA Toolkit 12.0.
                                 </li>
                                 <li class="li liexpand">A compiler bug in NVRTC in CUDA version 11.7 and earlier, was causing
                                    						incorrect outputs when computing logical operations on boolean input tensors
                                    						in the runtime fusion engine. A workaround had been integrated to avoid the
                                    						most common issues. However, it is highly recommended to update to at least
                                    						CUDA version 11.7u1 for a fix. Specifically, known failure cases are when
                                    						pointwise operations of mode <samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_NOT</samp>,
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_AND</samp> or
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_OR</samp> operates on boolean tensors.
                                    						This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 57</samp> for convolution forward
                                    						and <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 62</samp> for convolution
                                    						backward data may have performance regressions for non-zero beta problems.
                                    						However, they are not typically recommended by cuDNN heuristics, so the
                                    						observable impact should be minimal.
                                 </li>
                                 <li class="li liexpand">Use of <samp class="ph codeph">cudnnFusedOpsExecute()</samp> on NVIDIA Volta compatible
                                    						architectures hosted on AArch64 systems may generate incorrect results when
                                    						used with <samp class="ph codeph">cudnnFusedOps_t</samp> set to
                                    							<samp class="ph codeph">CUDNN_FUSED_SCALE_BIAS_ACTIVATION_WGRAD</samp>.
                                 </li>
                                 <li class="li liexpand">With CUDA 11.7, the NVRTC library may cause a small memory leak when using
                                    						the runtime fusion engine. The issue has been addressed in CUDA Toolkit 11.8
                                    						or later. 
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 0</samp> for
                                    							<samp class="ph codeph">DgradDreluBnBwdWeights</samp> may see a performance regression
                                    						when moving from cuDNN 8.8 to cuDNN 8.9.
                                 </li>
                                 <li class="li liexpand">If cuDNN 8.4.1 or earlier statically links with
                                    							<samp class="ph codeph">libcudart.so</samp> from the CUDA Toolkit 11.7 or later, when
                                    						the LFL feature is activated, the results from
                                    							<samp class="ph codeph">cudnnFind*Algo</samp> will not be accurate.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX
                                    						3090 compared to 2080 Ti. This includes EfficientNet with up to 6x
                                    						performance difference, UNet up to 1.6x performance difference and Tacotron
                                    						up to 1.6x performance difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    							<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior,
                                    						accumulator data types, and so on, have not been documented as of yet.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with filter size
                                    						1x1. The severity would be different depending on which version of the CUDA
                                    						Toolkit the user is using.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with high group
                                    						count. The issue is more severe on V100.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-895__section_dph_zkv_jrb"><a name="rel-895__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and
                              				the NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-895__section_nbs_qgh_fsb"><a name="rel-895__section_nbs_qgh_fsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel-895__ul_obs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-895__ul_obs_qgh_fsb">
                                 <li class="li liexpand">Disabling CUDA context preemption on Windows can sometimes lead to
                                    							<samp class="ph codeph">CUDNN_INTERNAL_ERRORS</samp> being returned from convolution
                                    						runs. When using cuDNN, do not disable CUDA context preemption.
                                 </li>
                                 <li class="li liexpand">When using the cuDNN static library, you will need to use the same
                                    						major.minor version of the CUDA Toolkit by which cuDNN was built to build
                                    						your application. Refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a> for
                                    						the exact supported CUDA versions.
                                 </li>
                                 <li class="li liexpand">In cuDNN 8.9.0, runtime fusion engines (with
                                    							<samp class="ph codeph">CUDNN_BEHAVIOR_NOTE_RUNTIME_COMPILATION</samp>) will only work
                                    						with NVRTC from CUDA Toolkit 11.8, 12.0 and 12.1. They are not guaranteed to
                                    						be forward compatible with future CUDA 12.x Toolkits.
                                 </li>
                                 <li class="li liexpand">Within the cuDNN version 8 backend API, the following engines are known not
                                    						to be thread-safe when executed simultaneously with multiple threads sharing
                                    						the same execution plan:
                                    
                                    
                                    <div class="tablenoborder"><a name="rel-895__table_t2h_lsp_r5b" shape="rect">
                                          <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="rel-895__table_t2h_lsp_r5b" class="table" frame="border" border="1" rules="all">
                                          <caption><span class="tablecap">Table 3. Engines That Are Not Thread-Safe</span></caption>
                                          <thead class="thead" align="left">
                                             <tr class="row">
                                                <th class="entry" valign="top" width="20%" id="d54e3334" rowspan="1" colspan="1"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e3338" rowspan="1" colspan="1"><samp class="ph codeph">convolution forward</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e3342" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward data</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e3346" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward filter</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e3350" rowspan="1" colspan="1"><samp class="ph codeph">cudnnConvolutionBiasActivationForward</samp></th>
                                             </tr>
                                          </thead>
                                          <tbody class="tbody">
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e3334" rowspan="1" colspan="1">A100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e3338" rowspan="1" colspan="1"><a name="rel-895__ul_c53_qsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-895__ul_c53_qsp_r5b">
                                                      <li class="li">15</li>
                                                      <li class="li">20</li>
                                                      <li class="li">21</li>
                                                      <li class="li">22</li>
                                                      <li class="li">23</li>
                                                      <li class="li">25</li>
                                                      <li class="li">27</li>
                                                      <li class="li">34</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">39</li>
                                                      <li class="li">40</li>
                                                      <li class="li">41</li>
                                                      <li class="li">43</li>
                                                      <li class="li">45</li>
                                                      <li class="li">46</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                      <li class="li">57</li>
                                                      <li class="li">59</li>
                                                      <li class="li">60</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e3342" rowspan="1" colspan="1"><a name="rel-895__ul_rh2_ssp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-895__ul_rh2_ssp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">19</li>
                                                      <li class="li">20</li>
                                                      <li class="li">22</li>
                                                      <li class="li">23</li>
                                                      <li class="li">24</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">27</li>
                                                      <li class="li">28</li>
                                                      <li class="li">32</li>
                                                      <li class="li">33</li>
                                                      <li class="li">34</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">40</li>
                                                      <li class="li">43</li>
                                                      <li class="li">46</li>
                                                      <li class="li">49</li>
                                                      <li class="li">51</li>
                                                      <li class="li">52</li>
                                                      <li class="li">53</li>
                                                      <li class="li">54</li>
                                                      <li class="li">55</li>
                                                      <li class="li">56</li>
                                                      <li class="li">57</li>
                                                      <li class="li">58</li>
                                                      <li class="li">59</li>
                                                      <li class="li">60</li>
                                                      <li class="li">62</li>
                                                      <li class="li">64</li>
                                                      <li class="li">65</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e3346" rowspan="1" colspan="1"><a name="rel-895__ul_mjw_vsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-895__ul_mjw_vsp_r5b">
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">13</li>
                                                      <li class="li">15</li>
                                                      <li class="li">21</li>
                                                      <li class="li">22</li>
                                                      <li class="li">23</li>
                                                      <li class="li">24</li>
                                                      <li class="li">25</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">33</li>
                                                      <li class="li">34</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">39</li>
                                                      <li class="li">40</li>
                                                      <li class="li">45</li>
                                                      <li class="li">46</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                      <li class="li">49</li>
                                                      <li class="li">50</li>
                                                      <li class="li">55</li>
                                                      <li class="li">56</li>
                                                      <li class="li">61</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e3350" rowspan="1" colspan="1"><a name="rel-895__ul_mfh_1d3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-895__ul_mfh_1d3_kvb">
                                                      <li class="li">4005</li>
                                                      <li class="li">4006</li>
                                                      <li class="li">4007</li>
                                                      <li class="li">4010</li>
                                                      <li class="li">4017</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4023</li>
                                                      <li class="li">4024</li>
                                                      <li class="li">4025</li>
                                                      <li class="li">4026</li>
                                                      <li class="li">4027</li>
                                                      <li class="li">4028</li>
                                                      <li class="li">4029</li>
                                                      <li class="li">4032</li>
                                                      <li class="li">4033</li>
                                                      <li class="li">4037</li>
                                                      <li class="li">4038</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e3334" rowspan="1" colspan="1">V100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e3338" rowspan="1" colspan="1"><a name="rel-895__ul_qrn_btp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-895__ul_qrn_btp_r5b">
                                                      <li class="li">7</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">11</li>
                                                      <li class="li">12</li>
                                                      <li class="li">16</li>
                                                      <li class="li">17</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">32</li>
                                                      <li class="li">33</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">43</li>
                                                      <li class="li">49</li>
                                                      <li class="li">57</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e3342" rowspan="1" colspan="1"><a name="rel-895__ul_ffy_dtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-895__ul_ffy_dtp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">11</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">14</li>
                                                      <li class="li">15</li>
                                                      <li class="li">16</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">41</li>
                                                      <li class="li">44</li>
                                                      <li class="li">47</li>
                                                      <li class="li">62</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e3346" rowspan="1" colspan="1"><a name="rel-895__ul_lg1_htp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-895__ul_lg1_htp_r5b">
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">6</li>
                                                      <li class="li">7</li>
                                                      <li class="li">11</li>
                                                      <li class="li">14</li>
                                                      <li class="li">21</li>
                                                      <li class="li">32</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">42</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">52</li>
                                                      <li class="li">61</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e3350" rowspan="1" colspan="1"><a name="rel-895__ul_w1l_cd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-895__ul_w1l_cd3_kvb">
                                                      <li class="li">4001</li>
                                                      <li class="li">4002</li>
                                                      <li class="li">4008</li>
                                                      <li class="li">4015</li>
                                                      <li class="li">4016</li>
                                                      <li class="li">4017</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4021</li>
                                                      <li class="li">4030</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e3334" rowspan="1" colspan="1">T4</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e3338" rowspan="1" colspan="1"><a name="rel-895__ul_q3j_jtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-895__ul_q3j_jtp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">14</li>
                                                      <li class="li">15</li>
                                                      <li class="li">16</li>
                                                      <li class="li">18</li>
                                                      <li class="li">19</li>
                                                      <li class="li">24</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">43</li>
                                                      <li class="li">50</li>
                                                      <li class="li">57</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e3342" rowspan="1" colspan="1"><a name="rel-895__ul_ax4_ntp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-895__ul_ax4_ntp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">15</li>
                                                      <li class="li">18</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">37</li>
                                                      <li class="li">39</li>
                                                      <li class="li">42</li>
                                                      <li class="li">44</li>
                                                      <li class="li">45</li>
                                                      <li class="li">48</li>
                                                      <li class="li">62</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e3346" rowspan="1" colspan="1"><a name="rel-895__ul_j3t_rtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-895__ul_j3t_rtp_r5b">
                                                      <li class="li">5</li>
                                                      <li class="li">12</li>
                                                      <li class="li">21</li>
                                                      <li class="li">26</li>
                                                      <li class="li">27</li>
                                                      <li class="li">28</li>
                                                      <li class="li">32</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">53</li>
                                                      <li class="li">54</li>
                                                      <li class="li">61</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e3350" rowspan="1" colspan="1"><a name="rel-895__ul_kc5_jd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-895__ul_kc5_jd3_kvb">
                                                      <li class="li">4003</li>
                                                      <li class="li">4004</li>
                                                      <li class="li">4007</li>
                                                      <li class="li">4009</li>
                                                      <li class="li">4015</li>
                                                      <li class="li">4017</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4022</li>
                                                      <li class="li">4031</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                          </tbody>
                                       </table>
                                    </div>
                                 </li>
                                 <li class="li liexpand">The status returned by <samp class="ph codeph">cudnnBackendFinalize()</samp> or
                                    							<samp class="ph codeph">cudnnBackendExecute()</samp> on a
                                    							<samp class="ph codeph">CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR</samp> may change
                                    						depending on the version of the dynamic dependencies of cuDNN. As of this
                                    						writing, only cuBLAS is known to affect the return status of these function
                                    						calls.
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not
                                    						required to consider padding. Users of cuDNN can witness an unexpected lack
                                    						of problem support when forward convolution spatial dimensions are less than
                                    						the filter size and padding is nonzero but is sufficient to extend spatial
                                    						dimensions to or beyond filter dimensions. This is commonly observed with,
                                    						but not limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand">When performing batch normalization in cuDNN, the operation is allowed to
                                    						proceed if the output tensor strides are overlapping, however, there is no
                                    						guarantee of deterministic results.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 25</samp> for convolution
                                    						backwards data (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp>) does not support
                                    						tensors in which the product N*C*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX =</samp><samp class="ph codeph">1 for convolution
                                       							backwards data</samp> (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp>) does not support
                                    						tensors in which the product N*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. This issue has been present in all previous releases of
                                    						cuDNN and exercising the use case for the engine would show incorrect
                                    						results.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later. It also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples must be installed in a writable location. If not installed in a
                                    						writable location, the samples can crash.
                                 </li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit nondeterministic
                                       							behavior when the cuDNN library is built with CUDA Toolkit 10.2 or
                                       							higher. This is the result of a new buffer management and heuristics in
                                       							the cuBLAS library. As described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This happens
                                       							when two buffer sizes (16 KB and 4 MB) are used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting
                                       							for a buffer of that size to be released, a smaller buffer may be used
                                       							with a different GPU kernel. The kernel selection may affect numerical
                                       							results. The user can eliminate the nondeterministic behavior of cuDNN
                                       							RNN and multihead attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16
                                       							KB each in GPU memory while the second setting creates two buffers of 4
                                       							MB each. The default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors
                                    						in order to run efficiently. As always, cuDNN recommends users to align
                                    						tensors to 16 byte boundaries that will be sufficiently aligned for any
                                    						computational option in cuDNN. Doing otherwise may cause performance
                                    						regressions in cuDNN 8.0.x compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and
                                    						the output is in FP16 (half float), there are cases where the numerical
                                    						accuracy between the different algorithms might differ. cuDNN 8.0.x users
                                    						can target the backend API to query the numerical notes of the algorithms to
                                    						get the information programmatically. There are cases where algo0 and algo1
                                    						will have a reduced precision accumulation when users target the legacy API.
                                    						In all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and
                                    						backward filter, grouped convolution with groups larger than 1 and with odd
                                    						product of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D
                                    						convolution), <samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on
                                    						devices older than NVIDIA Volta. To prevent a potential illegal memory
                                    						access by an instruction that only has a 16-bit version in NVIDIA Volta and
                                    						later, pad at least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use
                                    						texture-based load structure for performance improvements particularly in
                                    						older hardware architectures. Users can opt out of using texture using the
                                    						environmental variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this
                                    						variable is removed. Texture loading is turned off by default. Users who
                                    						want to continue to use texture-based load, can adapt the new backend API,
                                    						and toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check
                                    						API (for example, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command
                                    						explicitly to resolve the undefined symbols from cuDNN static
                                    						libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA
                                    						Pascal and later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-895__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-895__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in
                                    						compatibility mode are tested with compute-sanitizer,
                                    							<samp class="ph codeph">cuGetProcAddress</samp> failures with error code 500 will
                                    						arise due to missing functions. This error can be ignored, or suppressed
                                    						with the <samp class="ph codeph">--report-api-errors no</samp> option, as this is due to
                                    						CUDA backward compatibility checking if a function is usable with the CUDA
                                    						toolkit combination. The functions are introduced in a later version of CUDA
                                    						but are not available on the current platform. The absence of these
                                    						functions is harmless and will not give rise to any functional issues.
                                 </li>
                                 <li class="li liexpand">Users of <samp class="ph codeph">cudnn_cnn_infer_static.a</samp> may need to update their
                                    						application linkage so that symbols absent in that library are subsequently
                                    						made available with <samp class="ph codeph">cudnn_ops_infer_static.a</samp>. On Linux,
                                    						this is specifying the ops library after <samp class="ph codeph">cnn</samp> on the linker
                                    						line. The same applies to <samp class="ph codeph">cudnn_cnn_train_static.a</samp> and
                                    							<samp class="ph codeph">cudnn_ops_train_static.a</samp>.
                                 </li>
                                 <li class="li liexpand">The fused attention and flash attention runtime engines have been disabled
                                    						for NVRTC 11.8 due to compiler limitations.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-894"><a name="rel-894" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-894" name="rel-894" shape="rect">1.4.&nbsp;cuDNN Release 8.9.4</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">These are the NVIDIA cuDNN 8.9.4 Release Notes. These Release Notes include fixes
                           		from the previous cuDNN releases as well as the following additional changes.
                        </p>
                        <div class="section" id="rel-894__section_l2m_s4c_2jb"><a name="rel-894__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">These Release Notes are applicable to both cuDNN and NVIDIA JetPack™
                              				users of cuDNN unless appended specifically with <em class="ph i">(not applicable for Jetson
                                 					platforms)</em>.
                           </p>
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-894__section_dsy_4hd_3yb"><a name="rel-894__section_dsy_4hd_3yb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-894__ul_n2g_phd_3yb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-894__ul_n2g_phd_3yb">
                                 <li class="li liexpand">FP16 and BF16 fused flash attention engine <samp class="ph codeph">fprop</samp>
                                    						performance has been improved for NVIDIA Hopper GPUs giving a speedup of
                                    						nearly 2x-3x over cuDNN 8.9.3.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-894__section_mg1_b3f_2yb"><a name="rel-894__section_mg1_b3f_2yb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:<a name="rel-894__ul_o3n_b3f_2yb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-894__ul_o3n_b3f_2yb">
                                 <li class="li liexpand">Fixed IMA in the <samp class="ph codeph">grouped_direct</samp> kernel for tensor sizes
                                    						greater than 2^32 / [size of input element data type].
                                 </li>
                                 <li class="li liexpand">Fixed large overhead of enabling dropout in FP16 and BF16 fused flash
                                    						attention engine for training.
                                 </li>
                                 <li class="li liexpand">Fixed the output for the <samp class="ph codeph">dx</samp> tensor in FP8 backwards data
                                    						grouped convolutions. Previously, the values for only the first group were
                                    						written, with zeros written elsewhere.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-894__section_t2m_s5p_nnb"><a name="rel-894__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel-894__ul_mbs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-894__ul_mbs_qgh_fsb">
                                 <li class="li liexpand">On NVIDIA Ampere and Hopper architectures, incorrect results are possible in
                                    						variable sequence lengths and when the sequence length is not a multiple of
                                    						128.
                                 </li>
                                 <li class="li liexpand">ZLIB version 1.2.13 is statically linked into the cuDNN Windows dynamic
                                    						libraries. Changing to the static linkage of ZLIB for other platforms will
                                    						be addressed in future cuDNN releases.
                                 </li>
                                 <li class="li liexpand">For NVIDIA Hopper architectures, for DgradDreluBNBwdWeight, ConvBNwgrad, and ConvBNfprop
                                    						matched engines, static CGA is deprecated and dynamic CGA is supported. You
                                    						can only use <samp class="ph codeph">CUDNN_KNOB_TYPE_TILE_CGA_M</samp> and
                                    							<samp class="ph codeph">CUDNN_KNOB_TYPE_TILE_CGA_N</samp> knobs to set CGA size. The
                                    							<samp class="ph codeph">CUDNN_KNOB_TYPE_TILE_CGA</samp> knobs are deprecated for
                                    						DgradDreluBNBwdWeight, ConvBNwgrad, and ConvBNfprop engines.
                                 </li>
                                 <li class="li liexpand">A race condition in memory write accesses was flagged by the
                                    						"compute-sanitizer" tool in some cuBLAS kernels invoked by the cuDNN
                                    						multihead attention API <samp class="ph codeph">cudnnMultiHeadAttnForward()</samp> on H100
                                    						GPUs. Extensive testing on H100, with different clock speeds and
                                    						computational loads, did not reveal any impact on functional results that
                                    						were always identical and correct. This issue is currently being
                                    						investigated. 
                                 </li>
                                 <li class="li liexpand">The FP8 fused flash attention is known to be slower than FP16 fused flash
                                    						attention for small batch sizes.
                                 </li>
                                 <li class="li liexpand">cuDNN's usage of cuBLAS from CUDA Toolkit 12.1 may result in race-check
                                    						failures when the library is tested under compute sanitizer. These failures
                                    						are believed to be a cuBLAS issue and are being investigated. A workaround
                                    						for this issue is to use cuBLAS from CUDA Toolkit 12.0.
                                 </li>
                                 <li class="li liexpand">A compiler bug in NVRTC in CUDA version 11.7 and earlier, was causing
                                    						incorrect outputs when computing logical operations on boolean input tensors
                                    						in the runtime fusion engine. A workaround had been integrated to avoid the
                                    						most common issues. However, it is highly recommended to update to at least
                                    						CUDA version 11.7u1 for a fix. Specifically, known failure cases are when
                                    						pointwise operations of mode <samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_NOT</samp>,
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_AND</samp> or
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_OR</samp> operates on boolean tensors.
                                    						This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 57</samp> for convolution forward
                                    						and <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 62</samp> for convolution
                                    						backward data may have performance regressions for non-zero beta problems.
                                    						However, they are not typically recommended by cuDNN heuristics, so the
                                    						observable impact should be minimal.
                                 </li>
                                 <li class="li liexpand">Use of <samp class="ph codeph">cudnnFusedOpsExecute()</samp> on NVIDIA Volta compatible
                                    						architectures hosted on AArch64 systems may generate incorrect results when
                                    						used with <samp class="ph codeph">cudnnFusedOps_t</samp> set to
                                    							<samp class="ph codeph">CUDNN_FUSED_SCALE_BIAS_ACTIVATION_WGRAD</samp>.
                                 </li>
                                 <li class="li liexpand">With CUDA 11.7, the NVRTC library may cause a small memory leak when using
                                    						the runtime fusion engine. The issue has been addressed in CUDA Toolkit 11.8
                                    						or later. 
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 0</samp> for
                                    							<samp class="ph codeph">DgradDreluBnBwdWeights</samp> may see a performance regression
                                    						when moving from cuDNN 8.8 to cuDNN 8.9.
                                 </li>
                                 <li class="li liexpand">If cuDNN 8.4.1 or earlier statically links with
                                    							<samp class="ph codeph">libcudart.so</samp> from the CUDA Toolkit 11.7 or later, when
                                    						the LFL feature is activated, the results from
                                    							<samp class="ph codeph">cudnnFind*Algo</samp> will not be accurate.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX
                                    						3090 compared to 2080 Ti. This includes EfficientNet with up to 6x
                                    						performance difference, UNet up to 1.6x performance difference and Tacotron
                                    						up to 1.6x performance difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    							<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior,
                                    						accumulator data types, and so on, have not been documented as of yet.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with filter size
                                    						1x1. The severity would be different depending on which version of the CUDA
                                    						Toolkit the user is using.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with high group
                                    						count. The issue is more severe on V100.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-894__section_dph_zkv_jrb"><a name="rel-894__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and
                              				the NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-894__section_nbs_qgh_fsb"><a name="rel-894__section_nbs_qgh_fsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel-894__ul_obs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-894__ul_obs_qgh_fsb">
                                 <li class="li liexpand">Disabling CUDA context preemption on Windows can sometimes lead to
                                    							<samp class="ph codeph">CUDNN_INTERNAL_ERRORS</samp> being returned from convolution
                                    						runs. When using cuDNN, do not disable CUDA context preemption.
                                 </li>
                                 <li class="li liexpand">When using the cuDNN static library, you will need to use the same
                                    						major.minor version of the CUDA Toolkit by which cuDNN was built to build
                                    						your application. Refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a> for
                                    						the exact supported CUDA versions.
                                 </li>
                                 <li class="li liexpand">In cuDNN 8.9.0, runtime fusion engines (with
                                    							<samp class="ph codeph">CUDNN_BEHAVIOR_NOTE_RUNTIME_COMPILATION</samp>) will only work
                                    						with NVRTC from CUDA Toolkit 11.8, 12.0 and 12.1. They are not guaranteed to
                                    						be forward compatible with future CUDA 12.x Toolkits.
                                 </li>
                                 <li class="li liexpand">Within the cuDNN version 8 backend API, the following engines are known not
                                    						to be thread-safe when executed simultaneously with multiple threads sharing
                                    						the same execution plan:
                                    
                                    
                                    <div class="tablenoborder"><a name="rel-894__table_t2h_lsp_r5b" shape="rect">
                                          <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="rel-894__table_t2h_lsp_r5b" class="table" frame="border" border="1" rules="all">
                                          <caption><span class="tablecap">Table 4. Engines That Are Not Thread-Safe</span></caption>
                                          <thead class="thead" align="left">
                                             <tr class="row">
                                                <th class="entry" valign="top" width="20%" id="d54e4767" rowspan="1" colspan="1"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e4771" rowspan="1" colspan="1"><samp class="ph codeph">convolution forward</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e4775" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward data</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e4779" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward filter</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e4783" rowspan="1" colspan="1"><samp class="ph codeph">cudnnConvolutionBiasActivationForward</samp></th>
                                             </tr>
                                          </thead>
                                          <tbody class="tbody">
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e4767" rowspan="1" colspan="1">A100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e4771" rowspan="1" colspan="1"><a name="rel-894__ul_c53_qsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-894__ul_c53_qsp_r5b">
                                                      <li class="li">15</li>
                                                      <li class="li">20</li>
                                                      <li class="li">21</li>
                                                      <li class="li">22</li>
                                                      <li class="li">23</li>
                                                      <li class="li">25</li>
                                                      <li class="li">27</li>
                                                      <li class="li">34</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">39</li>
                                                      <li class="li">40</li>
                                                      <li class="li">41</li>
                                                      <li class="li">43</li>
                                                      <li class="li">45</li>
                                                      <li class="li">46</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                      <li class="li">57</li>
                                                      <li class="li">59</li>
                                                      <li class="li">60</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e4775" rowspan="1" colspan="1"><a name="rel-894__ul_rh2_ssp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-894__ul_rh2_ssp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">19</li>
                                                      <li class="li">20</li>
                                                      <li class="li">22</li>
                                                      <li class="li">23</li>
                                                      <li class="li">24</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">27</li>
                                                      <li class="li">28</li>
                                                      <li class="li">32</li>
                                                      <li class="li">33</li>
                                                      <li class="li">34</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">40</li>
                                                      <li class="li">43</li>
                                                      <li class="li">46</li>
                                                      <li class="li">49</li>
                                                      <li class="li">51</li>
                                                      <li class="li">52</li>
                                                      <li class="li">53</li>
                                                      <li class="li">54</li>
                                                      <li class="li">55</li>
                                                      <li class="li">56</li>
                                                      <li class="li">57</li>
                                                      <li class="li">58</li>
                                                      <li class="li">59</li>
                                                      <li class="li">60</li>
                                                      <li class="li">62</li>
                                                      <li class="li">64</li>
                                                      <li class="li">65</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e4779" rowspan="1" colspan="1"><a name="rel-894__ul_mjw_vsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-894__ul_mjw_vsp_r5b">
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">13</li>
                                                      <li class="li">15</li>
                                                      <li class="li">21</li>
                                                      <li class="li">22</li>
                                                      <li class="li">23</li>
                                                      <li class="li">24</li>
                                                      <li class="li">25</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">33</li>
                                                      <li class="li">34</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">39</li>
                                                      <li class="li">40</li>
                                                      <li class="li">45</li>
                                                      <li class="li">46</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                      <li class="li">49</li>
                                                      <li class="li">50</li>
                                                      <li class="li">55</li>
                                                      <li class="li">56</li>
                                                      <li class="li">61</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e4783" rowspan="1" colspan="1"><a name="rel-894__ul_mfh_1d3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-894__ul_mfh_1d3_kvb">
                                                      <li class="li">4005</li>
                                                      <li class="li">4006</li>
                                                      <li class="li">4007</li>
                                                      <li class="li">4010</li>
                                                      <li class="li">4017</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4023</li>
                                                      <li class="li">4024</li>
                                                      <li class="li">4025</li>
                                                      <li class="li">4026</li>
                                                      <li class="li">4027</li>
                                                      <li class="li">4028</li>
                                                      <li class="li">4029</li>
                                                      <li class="li">4032</li>
                                                      <li class="li">4033</li>
                                                      <li class="li">4037</li>
                                                      <li class="li">4038</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e4767" rowspan="1" colspan="1">V100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e4771" rowspan="1" colspan="1"><a name="rel-894__ul_qrn_btp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-894__ul_qrn_btp_r5b">
                                                      <li class="li">7</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">11</li>
                                                      <li class="li">12</li>
                                                      <li class="li">16</li>
                                                      <li class="li">17</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">32</li>
                                                      <li class="li">33</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">43</li>
                                                      <li class="li">49</li>
                                                      <li class="li">57</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e4775" rowspan="1" colspan="1"><a name="rel-894__ul_ffy_dtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-894__ul_ffy_dtp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">11</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">14</li>
                                                      <li class="li">15</li>
                                                      <li class="li">16</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">41</li>
                                                      <li class="li">44</li>
                                                      <li class="li">47</li>
                                                      <li class="li">62</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e4779" rowspan="1" colspan="1"><a name="rel-894__ul_lg1_htp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-894__ul_lg1_htp_r5b">
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">6</li>
                                                      <li class="li">7</li>
                                                      <li class="li">11</li>
                                                      <li class="li">14</li>
                                                      <li class="li">21</li>
                                                      <li class="li">32</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">42</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">52</li>
                                                      <li class="li">61</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e4783" rowspan="1" colspan="1"><a name="rel-894__ul_w1l_cd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-894__ul_w1l_cd3_kvb">
                                                      <li class="li">4001</li>
                                                      <li class="li">4002</li>
                                                      <li class="li">4008</li>
                                                      <li class="li">4015</li>
                                                      <li class="li">4016</li>
                                                      <li class="li">4017</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4021</li>
                                                      <li class="li">4030</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e4767" rowspan="1" colspan="1">T4</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e4771" rowspan="1" colspan="1"><a name="rel-894__ul_q3j_jtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-894__ul_q3j_jtp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">14</li>
                                                      <li class="li">15</li>
                                                      <li class="li">16</li>
                                                      <li class="li">18</li>
                                                      <li class="li">19</li>
                                                      <li class="li">24</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">43</li>
                                                      <li class="li">50</li>
                                                      <li class="li">57</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e4775" rowspan="1" colspan="1"><a name="rel-894__ul_ax4_ntp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-894__ul_ax4_ntp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">15</li>
                                                      <li class="li">18</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">37</li>
                                                      <li class="li">39</li>
                                                      <li class="li">42</li>
                                                      <li class="li">44</li>
                                                      <li class="li">45</li>
                                                      <li class="li">48</li>
                                                      <li class="li">62</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e4779" rowspan="1" colspan="1"><a name="rel-894__ul_j3t_rtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-894__ul_j3t_rtp_r5b">
                                                      <li class="li">5</li>
                                                      <li class="li">12</li>
                                                      <li class="li">21</li>
                                                      <li class="li">26</li>
                                                      <li class="li">27</li>
                                                      <li class="li">28</li>
                                                      <li class="li">32</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">53</li>
                                                      <li class="li">54</li>
                                                      <li class="li">61</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e4783" rowspan="1" colspan="1"><a name="rel-894__ul_kc5_jd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-894__ul_kc5_jd3_kvb">
                                                      <li class="li">4003</li>
                                                      <li class="li">4004</li>
                                                      <li class="li">4007</li>
                                                      <li class="li">4009</li>
                                                      <li class="li">4015</li>
                                                      <li class="li">4017</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4022</li>
                                                      <li class="li">4031</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                          </tbody>
                                       </table>
                                    </div>
                                 </li>
                                 <li class="li liexpand">The status returned by <samp class="ph codeph">cudnnBackendFinalize()</samp> or
                                    							<samp class="ph codeph">cudnnBackendExecute()</samp> on a
                                    							<samp class="ph codeph">CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR</samp> may change
                                    						depending on the version of the dynamic dependencies of cuDNN. As of this
                                    						writing, only cuBLAS is known to affect the return status of these function
                                    						calls.
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not
                                    						required to consider padding. Users of cuDNN can witness an unexpected lack
                                    						of problem support when forward convolution spatial dimensions are less than
                                    						the filter size and padding is nonzero but is sufficient to extend spatial
                                    						dimensions to or beyond filter dimensions. This is commonly observed with,
                                    						but not limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand">When performing batch normalization in cuDNN, the operation is allowed to
                                    						proceed if the output tensor strides are overlapping, however, there is no
                                    						guarantee of deterministic results.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 25</samp> for convolution
                                    						backwards data (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp>) does not support
                                    						tensors in which the product N*C*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX =</samp><samp class="ph codeph">1 for convolution
                                       							backwards data</samp> (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp>) does not support
                                    						tensors in which the product N*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. This issue has been present in all previous releases of
                                    						cuDNN and exercising the use case for the engine would show incorrect
                                    						results.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later. It also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples must be installed in a writable location. If not installed in a
                                    						writable location, the samples can crash.
                                 </li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit nondeterministic
                                       							behavior when the cuDNN library is built with CUDA Toolkit 10.2 or
                                       							higher. This is the result of a new buffer management and heuristics in
                                       							the cuBLAS library. As described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This happens
                                       							when two buffer sizes (16 KB and 4 MB) are used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting
                                       							for a buffer of that size to be released, a smaller buffer may be used
                                       							with a different GPU kernel. The kernel selection may affect numerical
                                       							results. The user can eliminate the nondeterministic behavior of cuDNN
                                       							RNN and multihead attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16
                                       							KB each in GPU memory while the second setting creates two buffers of 4
                                       							MB each. The default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors
                                    						in order to run efficiently. As always, cuDNN recommends users to align
                                    						tensors to 16 byte boundaries that will be sufficiently aligned for any
                                    						computational option in cuDNN. Doing otherwise may cause performance
                                    						regressions in cuDNN 8.0.x compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and
                                    						the output is in FP16 (half float), there are cases where the numerical
                                    						accuracy between the different algorithms might differ. cuDNN 8.0.x users
                                    						can target the backend API to query the numerical notes of the algorithms to
                                    						get the information programmatically. There are cases where algo0 and algo1
                                    						will have a reduced precision accumulation when users target the legacy API.
                                    						In all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and
                                    						backward filter, grouped convolution with groups larger than 1 and with odd
                                    						product of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D
                                    						convolution), <samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on
                                    						devices older than NVIDIA Volta. To prevent a potential illegal memory
                                    						access by an instruction that only has a 16-bit version in NVIDIA Volta and
                                    						later, pad at least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use
                                    						texture-based load structure for performance improvements particularly in
                                    						older hardware architectures. Users can opt out of using texture using the
                                    						environmental variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this
                                    						variable is removed. Texture loading is turned off by default. Users who
                                    						want to continue to use texture-based load, can adapt the new backend API,
                                    						and toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check
                                    						API (for example, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command
                                    						explicitly to resolve the undefined symbols from cuDNN static
                                    						libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA
                                    						Pascal and later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-894__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-894__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in
                                    						compatibility mode are tested with compute-sanitizer,
                                    							<samp class="ph codeph">cuGetProcAddress</samp> failures with error code 500 will
                                    						arise due to missing functions. This error can be ignored, or suppressed
                                    						with the <samp class="ph codeph">--report-api-errors no</samp> option, as this is due to
                                    						CUDA backward compatibility checking if a function is usable with the CUDA
                                    						toolkit combination. The functions are introduced in a later version of CUDA
                                    						but are not available on the current platform. The absence of these
                                    						functions is harmless and will not give rise to any functional issues.
                                 </li>
                                 <li class="li liexpand">Users of <samp class="ph codeph">cudnn_cnn_infer_static.a</samp> may need to update their
                                    						application linkage so that symbols absent in that library are subsequently
                                    						made available with <samp class="ph codeph">cudnn_ops_infer_static.a</samp>. On Linux,
                                    						this is specifying the ops library after <samp class="ph codeph">cnn</samp> on the linker
                                    						line. The same applies to <samp class="ph codeph">cudnn_cnn_train_static.a</samp> and
                                    							<samp class="ph codeph">cudnn_ops_train_static.a</samp>.
                                 </li>
                                 <li class="li liexpand">The fused attention and flash attention runtime engines have been disabled
                                    						for NVRTC 11.8 due to compiler limitations.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-893"><a name="rel-893" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-893" name="rel-893" shape="rect">1.5.&nbsp;cuDNN Release 8.9.3</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">These are the NVIDIA cuDNN 8.9.3 Release Notes. These Release Notes include fixes
                           		from the previous cuDNN releases as well as the following additional changes.
                        </p>
                        <div class="section" id="rel-893__section_l2m_s4c_2jb"><a name="rel-893__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">These Release Notes are applicable to both cuDNN and NVIDIA JetPack™
                              				users of cuDNN unless appended specifically with <em class="ph i">(not applicable for Jetson
                                 					platforms)</em>.
                           </p>
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-893__section_x1v_2z4_1yb"><a name="rel-893__section_x1v_2z4_1yb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-893__ul_qsf_gz4_1yb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-893__ul_qsf_gz4_1yb">
                                 <li class="li liexpand">Major improvements in the FP16/BF16 fused flash attention engine:<a name="rel-893__ul_upz_4z4_1yb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-893__ul_upz_4z4_1yb">
                                       <li class="li liexpand">The supported layout of Q, K, V and O tensors and their gradients
                                          								have been generalized. Now, the only requirement is that the d
                                          								dimension must have stride 1. All other dimensions can have
                                          								arbitrary strides.
                                       </li>
                                       <li class="li liexpand">The engine now supports both self attention and cross
                                          								attention.
                                       </li>
                                       <li class="li liexpand">The engine now allows online generation of padding mask and causal
                                          								mask; each one can be turned on/off independently.
                                       </li>
                                       <li class="li liexpand">The engine now supports an extra additive bias passed in as a tensor
                                          								after the first batch MatMul and before the Softmax, which can be
                                          								used to pass in customized masks (for example, Alibi).
                                       </li>
                                       <li class="li liexpand">The engine now supports multi-query attention where the headcount
                                          								dimension of K and V tensors are 1
                                       </li>
                                       <li class="li liexpand">The engine now supports (padded) variable sequence length
                                          								computation. 
                                       </li>
                                       <li class="li liexpand">The engine now supports head dimensions to be 64 or 128.</li>
                                       <li class="li liexpand">The engine now supports arbitrary sequence length, batch size, and
                                          								headcount.
                                       </li>
                                       <li class="li liexpand">The engine now supports both dropouts turned on and off.</li>
                                       <li class="li liexpand">The engine now supports the dropout mask to be accepted as a tensor
                                          								rather than being generated inside the kernel.
                                       </li>
                                       <li class="li liexpand">The engine now supports the mask before Softmax to be an input
                                          								tensor created by the user, rather than being online generated.
                                       </li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand">Improved <samp class="ph codeph">CUDNN_HEUR_MODE_A</samp> heuristic recommendations for
                                    						depthwise convolution on NVIDIA Hopper GPUs.
                                 </li>
                                 <li class="li liexpand">Improved error reporting during cuDNN handle creation. For more information
                                    						on how to enable error reporting, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#unique_988125865" target="_blank" shape="rect">NVIDIA cuDNN Developer Guide</a>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-893__section_r1g_pjd_wxb"><a name="rel-893__section_r1g_pjd_wxb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:<a name="rel-893__ul_hp5_pjd_wxb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-893__ul_hp5_pjd_wxb">
                                 <li class="li liexpand">Use of <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 17</samp> for fused
                                    						convolutions with bias and ReLU could generate incorrect results on the
                                    						Maxwell GPU architectures. This issue is now fixed in 8.9.3.
                                 </li>
                                 <li class="li liexpand">Running Scale-Bias-Add-Relu-Conv-GenStats (refer to <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#convbnfprop" target="_blank" shape="rect">ConvBNfprop</a>) with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 0</samp> could generate
                                    						incorrect results on NVIDIA Ampere GPUs. This applies to whether dual
                                    						tensors are set or not in the cited pattern. This pattern is no longer
                                    						supported on NVIDIA Ampere GPUs.
                                 </li>
                                 <li class="li liexpand">A bug in FP16/BF16 fused flash attention for large problem sizes, that is,
                                    						when b * h * s * d &gt; 512 * 64 * number of SMs, which was resulting in
                                    						incorrect values being computed, has been fixed.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-893__section_t2m_s5p_nnb"><a name="rel-893__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel-893__ul_mbs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-893__ul_mbs_qgh_fsb">
                                 <li class="li liexpand">A race condition in memory write accesses was flagged by the
                                    						"compute-sanitizer" tool in some cuBLAS kernels invoked by the cuDNN
                                    						multihead attention API <samp class="ph codeph">cudnnMultiHeadAttnForward()</samp> on H100
                                    						GPUs. Extensive testing on H100, with different clock speeds and
                                    						computational loads, did not reveal any impact on functional results that
                                    						were always identical and correct. This issue is currently being
                                    						investigated. 
                                 </li>
                                 <li class="li liexpand">The FP8 fused flash attention is known to be slower than FP16 fused flash
                                    						attention for small batch sizes.
                                 </li>
                                 <li class="li liexpand">cuDNN's usage of cuBLAS from CUDA Toolkit 12.1 may result in race-check
                                    						failures when the library is tested under compute sanitizer. These failures
                                    						are believed to be a cuBLAS issue and are being investigated. A workaround
                                    						for this issue is to use cuBLAS from CUDA Toolkit 12.0.
                                 </li>
                                 <li class="li liexpand">A compiler bug in NVRTC in CUDA version 11.7 and earlier, was causing
                                    						incorrect outputs when computing logical operations on boolean input tensors
                                    						in the runtime fusion engine. A workaround had been integrated to avoid the
                                    						most common issues. However, it is highly recommended to update to at least
                                    						CUDA version 11.7u1 for a fix. Specifically, known failure cases are when
                                    						pointwise operations of mode <samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_NOT</samp>,
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_AND</samp> or
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_OR</samp> operates on boolean tensors.
                                    						This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 57</samp> for convolution forward
                                    						and <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 62</samp> for convolution
                                    						backward data may have performance regressions for non-zero beta problems.
                                    						However, they are not typically recommended by cuDNN heuristics, so the
                                    						observable impact should be minimal.
                                 </li>
                                 <li class="li liexpand">Use of <samp class="ph codeph">cudnnFusedOpsExecute()</samp> on NVIDIA Volta compatible
                                    						architectures hosted on AArch64 systems may generate incorrect results when
                                    						used with <samp class="ph codeph">cudnnFusedOps_t</samp> set to
                                    							<samp class="ph codeph">CUDNN_FUSED_SCALE_BIAS_ACTIVATION_WGRAD</samp>.
                                 </li>
                                 <li class="li liexpand">With CUDA 11.7, the NVRTC library may cause a small memory leak when using
                                    						the runtime fusion engine. The issue has been addressed in CUDA Toolkit 11.8
                                    						or later. 
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 0</samp> for
                                    							<samp class="ph codeph">DgradDreluBnBwdWeights</samp> may see a performance regression
                                    						when moving from cuDNN 8.8 to cuDNN 8.9.
                                 </li>
                                 <li class="li liexpand">If cuDNN 8.4.1 or earlier statically links with
                                    							<samp class="ph codeph">libcudart.so</samp> from the CUDA Toolkit 11.7 or later, when
                                    						the LFL feature is activated, the results from
                                    							<samp class="ph codeph">cudnnFind*Algo</samp> will not be accurate.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX
                                    						3090 compared to 2080 Ti. This includes EfficientNet with up to 6x
                                    						performance difference, UNet up to 1.6x performance difference and Tacotron
                                    						up to 1.6x performance difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    							<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior,
                                    						accumulator data types, and so on, have not been documented as of yet.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with filter size
                                    						1x1. The severity would be different depending on which version of the CUDA
                                    						Toolkit the user is using.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with high group
                                    						count. The issue is more severe on V100.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-893__section_dph_zkv_jrb"><a name="rel-893__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and
                              				the NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-893__section_nbs_qgh_fsb"><a name="rel-893__section_nbs_qgh_fsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel-893__ul_obs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-893__ul_obs_qgh_fsb">
                                 <li class="li liexpand">Disabling CUDA context preemption on Windows can sometimes lead to
                                    							<samp class="ph codeph">CUDNN_INTERNAL_ERRORS</samp> being returned from convolution
                                    						runs. When using cuDNN, do not disable CUDA context preemption.
                                 </li>
                                 <li class="li liexpand">When using the cuDNN static library, you will need to use the same
                                    						major.minor version of the CUDA Toolkit by which cuDNN was built to build
                                    						your application. Refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a> for
                                    						the exact supported CUDA versions.
                                 </li>
                                 <li class="li liexpand">In cuDNN 8.9.0, runtime fusion engines (with
                                    							<samp class="ph codeph">CUDNN_BEHAVIOR_NOTE_RUNTIME_COMPILATION</samp>) will only work
                                    						with NVRTC from CUDA Toolkit 11.8, 12.0 and 12.1. They are not guaranteed to
                                    						be forward compatible with future CUDA 12.x Toolkits.
                                 </li>
                                 <li class="li liexpand">Within the cuDNN version 8 backend API, the following engines are known not
                                    						to be thread-safe when executed simultaneously with multiple threads sharing
                                    						the same execution plan:
                                    
                                    
                                    <div class="tablenoborder"><a name="rel-893__table_t2h_lsp_r5b" shape="rect">
                                          <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="rel-893__table_t2h_lsp_r5b" class="table" frame="border" border="1" rules="all">
                                          <caption><span class="tablecap">Table 5. Engines That Are Not Thread-Safe</span></caption>
                                          <thead class="thead" align="left">
                                             <tr class="row">
                                                <th class="entry" valign="top" width="20%" id="d54e6230" rowspan="1" colspan="1"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e6234" rowspan="1" colspan="1"><samp class="ph codeph">convolution forward</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e6238" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward data</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e6242" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward filter</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e6246" rowspan="1" colspan="1"><samp class="ph codeph">cudnnConvolutionBiasActivationForward</samp></th>
                                             </tr>
                                          </thead>
                                          <tbody class="tbody">
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e6230" rowspan="1" colspan="1">A100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e6234" rowspan="1" colspan="1"><a name="rel-893__ul_c53_qsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-893__ul_c53_qsp_r5b">
                                                      <li class="li">36</li>
                                                      <li class="li">38</li>
                                                      <li class="li">45</li>
                                                      <li class="li">46</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e6238" rowspan="1" colspan="1"><a name="rel-893__ul_rh2_ssp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-893__ul_rh2_ssp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">19</li>
                                                      <li class="li">22</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">28</li>
                                                      <li class="li">40</li>
                                                      <li class="li">46</li>
                                                      <li class="li">51</li>
                                                      <li class="li">56</li>
                                                      <li class="li">57</li>
                                                      <li class="li">58</li>
                                                      <li class="li">59</li>
                                                      <li class="li">60</li>
                                                      <li class="li">65</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e6242" rowspan="1" colspan="1"><a name="rel-893__ul_mjw_vsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-893__ul_mjw_vsp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">21</li>
                                                      <li class="li">23</li>
                                                      <li class="li">33</li>
                                                      <li class="li">37</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                      <li class="li">49</li>
                                                      <li class="li">50</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e6246" rowspan="1" colspan="1"><a name="rel-893__ul_mfh_1d3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-893__ul_mfh_1d3_kvb">
                                                      <li class="li">4024</li>
                                                      <li class="li">4026</li>
                                                      <li class="li">4032</li>
                                                      <li class="li">4033</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e6230" rowspan="1" colspan="1">V100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e6234" rowspan="1" colspan="1"><a name="rel-893__ul_qrn_btp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-893__ul_qrn_btp_r5b">
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">12</li>
                                                      <li class="li">16</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">49</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e6238" rowspan="1" colspan="1"><a name="rel-893__ul_ffy_dtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-893__ul_ffy_dtp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">44</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e6242" rowspan="1" colspan="1"><a name="rel-893__ul_lg1_htp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-893__ul_lg1_htp_r5b">
                                                      <li class="li">6</li>
                                                      <li class="li">7</li>
                                                      <li class="li">21</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">52</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e6246" rowspan="1" colspan="1"><a name="rel-893__ul_w1l_cd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-893__ul_w1l_cd3_kvb">
                                                      <li class="li">4002</li>
                                                      <li class="li">4015</li>
                                                      <li class="li">4008</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4030</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e6230" rowspan="1" colspan="1">T4</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e6234" rowspan="1" colspan="1"><a name="rel-893__ul_q3j_jtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-893__ul_q3j_jtp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">14</li>
                                                      <li class="li">16</li>
                                                      <li class="li">18</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">50</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e6238" rowspan="1" colspan="1"><a name="rel-893__ul_ax4_ntp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-893__ul_ax4_ntp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">15</li>
                                                      <li class="li">18</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">37</li>
                                                      <li class="li">39</li>
                                                      <li class="li">44</li>
                                                      <li class="li">45</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e6242" rowspan="1" colspan="1"><a name="rel-893__ul_j3t_rtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-893__ul_j3t_rtp_r5b">
                                                      <li class="li">12</li>
                                                      <li class="li">21</li>
                                                      <li class="li">27</li>
                                                      <li class="li">28</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">53</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e6246" rowspan="1" colspan="1"><a name="rel-893__ul_kc5_jd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-893__ul_kc5_jd3_kvb">
                                                      <li class="li">4015</li>
                                                      <li class="li">4003</li>
                                                      <li class="li">4004</li>
                                                      <li class="li">4009</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4031</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                          </tbody>
                                       </table>
                                    </div>
                                 </li>
                                 <li class="li liexpand">The status returned by <samp class="ph codeph">cudnnBackendFinalize()</samp> or
                                    							<samp class="ph codeph">cudnnBackendExecute()</samp> on a
                                    							<samp class="ph codeph">CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR</samp> may change
                                    						depending on the version of the dynamic dependencies of cuDNN. As of this
                                    						writing, only cuBLAS is known to affect the return status of these function
                                    						calls.
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not
                                    						required to consider padding. Users of cuDNN can witness an unexpected lack
                                    						of problem support when forward convolution spatial dimensions are less than
                                    						the filter size and padding is nonzero but is sufficient to extend spatial
                                    						dimensions to or beyond filter dimensions. This is commonly observed with,
                                    						but not limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand">When performing batch normalization in cuDNN, the operation is allowed to
                                    						proceed if the output tensor strides are overlapping, however, there is no
                                    						guarantee of deterministic results.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 25</samp> for convolution
                                    						backwards data (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp>) does not support
                                    						tensors in which the product N*C*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX =</samp><samp class="ph codeph">1 for convolution
                                       							backwards data</samp> (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp>) does not support
                                    						tensors in which the product N*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. This issue has been present in all previous releases of
                                    						cuDNN and exercising the use case for the engine would show incorrect
                                    						results.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later. It also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples must be installed in a writable location. If not installed in a
                                    						writable location, the samples can crash.
                                 </li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit nondeterministic
                                       							behavior when the cuDNN library is built with CUDA Toolkit 10.2 or
                                       							higher. This is the result of a new buffer management and heuristics in
                                       							the cuBLAS library. As described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This happens
                                       							when two buffer sizes (16 KB and 4 MB) are used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting
                                       							for a buffer of that size to be released, a smaller buffer may be used
                                       							with a different GPU kernel. The kernel selection may affect numerical
                                       							results. The user can eliminate the nondeterministic behavior of cuDNN
                                       							RNN and multihead attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16
                                       							KB each in GPU memory while the second setting creates two buffers of 4
                                       							MB each. The default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors
                                    						in order to run efficiently. As always, cuDNN recommends users to align
                                    						tensors to 16 byte boundaries that will be sufficiently aligned for any
                                    						computational option in cuDNN. Doing otherwise may cause performance
                                    						regressions in cuDNN 8.0.x compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and
                                    						the output is in FP16 (half float), there are cases where the numerical
                                    						accuracy between the different algorithms might differ. cuDNN 8.0.x users
                                    						can target the backend API to query the numerical notes of the algorithms to
                                    						get the information programmatically. There are cases where algo0 and algo1
                                    						will have a reduced precision accumulation when users target the legacy API.
                                    						In all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and
                                    						backward filter, grouped convolution with groups larger than 1 and with odd
                                    						product of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D
                                    						convolution), <samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on
                                    						devices older than NVIDIA Volta. To prevent a potential illegal memory
                                    						access by an instruction that only has a 16-bit version in NVIDIA Volta and
                                    						later, pad at least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use
                                    						texture-based load structure for performance improvements particularly in
                                    						older hardware architectures. Users can opt out of using texture using the
                                    						environmental variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this
                                    						variable is removed. Texture loading is turned off by default. Users who
                                    						want to continue to use texture-based load, can adapt the new backend API,
                                    						and toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check
                                    						API (for example, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command
                                    						explicitly to resolve the undefined symbols from cuDNN static
                                    						libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA
                                    						Pascal and later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-893__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-893__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in
                                    						compatibility mode are tested with compute-sanitizer,
                                    							<samp class="ph codeph">cuGetProcAddress</samp> failures with error code 500 will
                                    						arise due to missing functions. This error can be ignored, or suppressed
                                    						with the <samp class="ph codeph">--report-api-errors no</samp> option, as this is due to
                                    						CUDA backward compatibility checking if a function is usable with the CUDA
                                    						toolkit combination. The functions are introduced in a later version of CUDA
                                    						but are not available on the current platform. The absence of these
                                    						functions is harmless and will not give rise to any functional issues.
                                 </li>
                                 <li class="li liexpand">Users of <samp class="ph codeph">cudnn_cnn_infer_static.a</samp> may need to update their
                                    						application linkage so that symbols absent in that library are subsequently
                                    						made available with <samp class="ph codeph">cudnn_ops_infer_static.a</samp>. On Linux,
                                    						this is specifying the ops library after <samp class="ph codeph">cnn</samp> on the linker
                                    						line. The same applies to <samp class="ph codeph">cudnn_cnn_train_static.a</samp> and
                                    							<samp class="ph codeph">cudnn_ops_train_static.a</samp>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-892"><a name="rel-892" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-892" name="rel-892" shape="rect">1.6.&nbsp;cuDNN Release 8.9.2</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">These are the NVIDIA cuDNN 8.9.2 Release Notes. These Release Notes include fixes
                           		from the previous cuDNN releases as well as the following additional changes.
                        </p>
                        <div class="section" id="rel-892__section_l2m_s4c_2jb"><a name="rel-892__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">These Release Notes are applicable to both cuDNN and NVIDIA JetPack™
                              				users of cuDNN unless appended specifically with <em class="ph i">(not applicable for Jetson
                                 					platforms)</em>.
                           </p>
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-892__section_p5v_ghn_qxb"><a name="rel-892__section_p5v_ghn_qxb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-892__ul_j1n_hhn_qxb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-892__ul_j1n_hhn_qxb">
                                 <li class="li liexpand">Improvements in the fused flash attention runtime fusion engine:<a name="rel-892__ul_uw5_zj4_qxb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-892__ul_uw5_zj4_qxb">
                                       <li class="li">Added new performance knobs for the engine.</li>
                                       <li class="li">Updated heuristics for the engine for both <samp class="ph codeph">fprop</samp>
                                          								and <samp class="ph codeph">bprop</samp>.
                                       </li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand">Extended fused (non-flash) attention engine to accept dropout mask as an
                                    						input rather than being generated inside the kernel.
                                 </li>
                                 <li class="li liexpand">Improved the performance of the runtime fusion engine for the NVIDIA Hopper
                                    						architecture and improved the compilation time. 
                                 </li>
                                 <li class="li liexpand">Updated runtime fusion engine heuristics for FP16 and FP8 convolution
                                    							<samp class="ph codeph">fprop</samp>, <samp class="ph codeph">dgrad</samp>, and Matmul operations to
                                    						support new kernels on the NVIDIA Hopper architecture.
                                 </li>
                                 <li class="li liexpand">Relaxed the limitation that only one reduction operation is allowed in the
                                    						DAG of g<sub class="ph sub">2</sub> of <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#runtime-fusion-engine" target="_blank" shape="rect">Generic Runtime Fusion
                                       						Engines</a>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-892__section_hgw_fsc_pxb"><a name="rel-892__section_hgw_fsc_pxb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:<a name="rel-892__ul_m5l_gsc_pxb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-892__ul_m5l_gsc_pxb">
                                 <li class="li liexpand">Use of <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 15</samp> for fused convolutions with
                                    						bias and ReLU could generate incorrect results on the Pascal GPU
                                    						architectures for cuDNN 8.9 releases. This issue is now fixed in 8.9.2.
                                    						Users of <samp class="ph codeph">cudnnConvolutionBiasActivationForward</samp> would have
                                    						been similarly affected. 
                                 </li>
                                 <li class="li liexpand">Additional fixes were implemented in
                                    							<samp class="ph codeph">cudnnRNNBackwardWeights_v8()</samp> to harden the process of
                                    						transferring the variable sequence length array from the RNN data descriptor
                                    						to device memory. As in cuDNN 8.9.1, the <samp class="ph codeph">const int32_t
                                       							devSeqLengths[]</samp> argument is no longer used and can be set to
                                    							<samp class="ph codeph">NULL</samp> by the user.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-892__section_t2m_s5p_nnb"><a name="rel-892__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel-892__ul_mbs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-892__ul_mbs_qgh_fsb">
                                 <li class="li liexpand">The FP8 fused flash attention is known to be slower than FP16 fused flash
                                    						attention for small batch sizes.
                                 </li>
                                 <li class="li liexpand">cuDNN's usage of cuBLAS from CUDA Toolkit 12.1 may result in race-check
                                    						failures when the library is tested under compute sanitizer. These failures
                                    						are believed to be a cuBLAS issue and are being investigated. A workaround
                                    						for this issue is to use cuBLAS from CUDA Toolkit 12.0.
                                 </li>
                                 <li class="li liexpand">A compiler bug in NVRTC in CUDA version 11.7 and earlier, was causing
                                    						incorrect outputs when computing logical operations on boolean input tensors
                                    						in the runtime fusion engine. A workaround had been integrated to avoid the
                                    						most common issues. However, it is highly recommended to update to at least
                                    						CUDA version 11.7u1 for a fix. Specifically, known failure cases are when
                                    						pointwise operations of mode <samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_NOT</samp>,
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_AND</samp> or
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_OR</samp> operates on boolean tensors.
                                    						This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 57</samp> for convolution forward
                                    						and <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 62</samp> for convolution
                                    						backward data may have performance regressions for non-zero beta problems.
                                    						However, they are not typically recommended by cuDNN heuristics, so the
                                    						observable impact should be minimal.
                                 </li>
                                 <li class="li liexpand">Use of <samp class="ph codeph">cudnnFusedOpsExecute()</samp> on NVIDIA Volta compatible
                                    						architectures hosted on AArch64 systems may generate incorrect results when
                                    						used with <samp class="ph codeph">cudnnFusedOps_t</samp> set to
                                    							<samp class="ph codeph">CUDNN_FUSED_SCALE_BIAS_ACTIVATION_WGRAD</samp>.
                                 </li>
                                 <li class="li liexpand">With CUDA 11.7, the NVRTC library may cause a small memory leak when using
                                    						the runtime fusion engine. The issue has been addressed in CUDA Toolkit 11.8
                                    						or later. 
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 0</samp> for
                                    							<samp class="ph codeph">DgradDreluBnBwdWeights</samp> may see a performance regression
                                    						when moving from cuDNN 8.8 to cuDNN 8.9.
                                 </li>
                                 <li class="li liexpand">If cuDNN 8.4.1 or earlier statically links with
                                    							<samp class="ph codeph">libcudart.so</samp> from the CUDA Toolkit 11.7 or later, when
                                    						the LFL feature is activated, the results from
                                    							<samp class="ph codeph">cudnnFind*Algo</samp> will not be accurate.
                                 </li>
                                 <li class="li liexpand">The documentation for <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnReorderFilterAndBias" target="_blank" shape="rect"><samp class="ph codeph">cudnnReorderFilterAndBias()</samp></a> requires
                                    						corrections for clarity.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX
                                    						3090 compared to 2080 Ti. This includes EfficientNet with up to 6x
                                    						performance difference, UNet up to 1.6x performance difference and Tacotron
                                    						up to 1.6x performance difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    							<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior,
                                    						accumulator data types, and so on, have not been documented as of yet.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN 8.4.0 may observe a slowdown in the Single Shot Multibox
                                    						Detector (SSD) model. This will be fixed in a future release.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with filter size
                                    						1x1. The severity would be different depending on which version of the CUDA
                                    						Toolkit the user is using.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with high group
                                    						count. The issue is more severe on V100.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-892__section_dph_zkv_jrb"><a name="rel-892__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and
                              				the NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-892__section_nbs_qgh_fsb"><a name="rel-892__section_nbs_qgh_fsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel-892__ul_obs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-892__ul_obs_qgh_fsb">
                                 <li class="li liexpand">Disabling CUDA context preemption on Windows can sometimes lead to
                                    							<samp class="ph codeph">CUDNN_INTERNAL_ERRORS</samp> being returned from convolution
                                    						runs. When using cuDNN, do not disable CUDA context preemption.
                                 </li>
                                 <li class="li liexpand">When using the cuDNN static library, you will need to use the same
                                    						major.minor version of the CUDA Toolkit by which cuDNN was built to build
                                    						your application. Refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a> for
                                    						the exact supported CUDA versions.
                                 </li>
                                 <li class="li liexpand">In cuDNN 8.9.0, runtime fusion engines (with
                                    							<samp class="ph codeph">CUDNN_BEHAVIOR_NOTE_RUNTIME_COMPILATION</samp>) will only work
                                    						with NVRTC from CUDA Toolkit 11.8, 12.0 and 12.1. They are not guaranteed to
                                    						be forward compatible with future CUDA 12.x Toolkits.
                                 </li>
                                 <li class="li liexpand">Within the cuDNN version 8 backend API, the following engines are known not
                                    						to be thread-safe when executed simultaneously with multiple threads sharing
                                    						the same execution plan:
                                    
                                    
                                    <div class="tablenoborder"><a name="rel-892__table_t2h_lsp_r5b" shape="rect">
                                          <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="rel-892__table_t2h_lsp_r5b" class="table" frame="border" border="1" rules="all">
                                          <caption><span class="tablecap">Table 6. Engines That Are Not Thread-Safe</span></caption>
                                          <thead class="thead" align="left">
                                             <tr class="row">
                                                <th class="entry" valign="top" width="20%" id="d54e7342" rowspan="1" colspan="1"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e7346" rowspan="1" colspan="1"><samp class="ph codeph">convolution forward</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e7350" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward data</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e7354" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward filter</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e7358" rowspan="1" colspan="1"><samp class="ph codeph">cudnnConvolutionBiasActivationForward</samp></th>
                                             </tr>
                                          </thead>
                                          <tbody class="tbody">
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e7342" rowspan="1" colspan="1">A100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e7346" rowspan="1" colspan="1"><a name="rel-892__ul_c53_qsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-892__ul_c53_qsp_r5b">
                                                      <li class="li">36</li>
                                                      <li class="li">38</li>
                                                      <li class="li">45</li>
                                                      <li class="li">46</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e7350" rowspan="1" colspan="1"><a name="rel-892__ul_rh2_ssp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-892__ul_rh2_ssp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">19</li>
                                                      <li class="li">22</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">28</li>
                                                      <li class="li">40</li>
                                                      <li class="li">46</li>
                                                      <li class="li">51</li>
                                                      <li class="li">56</li>
                                                      <li class="li">57</li>
                                                      <li class="li">58</li>
                                                      <li class="li">59</li>
                                                      <li class="li">60</li>
                                                      <li class="li">65</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e7354" rowspan="1" colspan="1"><a name="rel-892__ul_mjw_vsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-892__ul_mjw_vsp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">21</li>
                                                      <li class="li">23</li>
                                                      <li class="li">33</li>
                                                      <li class="li">37</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                      <li class="li">49</li>
                                                      <li class="li">50</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e7358" rowspan="1" colspan="1"><a name="rel-892__ul_mfh_1d3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-892__ul_mfh_1d3_kvb">
                                                      <li class="li">4024</li>
                                                      <li class="li">4026</li>
                                                      <li class="li">4032</li>
                                                      <li class="li">4033</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e7342" rowspan="1" colspan="1">V100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e7346" rowspan="1" colspan="1"><a name="rel-892__ul_qrn_btp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-892__ul_qrn_btp_r5b">
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">12</li>
                                                      <li class="li">16</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">49</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e7350" rowspan="1" colspan="1"><a name="rel-892__ul_ffy_dtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-892__ul_ffy_dtp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">44</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e7354" rowspan="1" colspan="1"><a name="rel-892__ul_lg1_htp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-892__ul_lg1_htp_r5b">
                                                      <li class="li">6</li>
                                                      <li class="li">7</li>
                                                      <li class="li">21</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">52</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e7358" rowspan="1" colspan="1"><a name="rel-892__ul_w1l_cd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-892__ul_w1l_cd3_kvb">
                                                      <li class="li">4002</li>
                                                      <li class="li">4015</li>
                                                      <li class="li">4008</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4030</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e7342" rowspan="1" colspan="1">T4</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e7346" rowspan="1" colspan="1"><a name="rel-892__ul_q3j_jtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-892__ul_q3j_jtp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">14</li>
                                                      <li class="li">16</li>
                                                      <li class="li">18</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">50</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e7350" rowspan="1" colspan="1"><a name="rel-892__ul_ax4_ntp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-892__ul_ax4_ntp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">15</li>
                                                      <li class="li">18</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">37</li>
                                                      <li class="li">39</li>
                                                      <li class="li">44</li>
                                                      <li class="li">45</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e7354" rowspan="1" colspan="1"><a name="rel-892__ul_j3t_rtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-892__ul_j3t_rtp_r5b">
                                                      <li class="li">12</li>
                                                      <li class="li">21</li>
                                                      <li class="li">27</li>
                                                      <li class="li">28</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">53</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e7358" rowspan="1" colspan="1"><a name="rel-892__ul_kc5_jd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-892__ul_kc5_jd3_kvb">
                                                      <li class="li">4015</li>
                                                      <li class="li">4003</li>
                                                      <li class="li">4004</li>
                                                      <li class="li">4009</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4031</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                          </tbody>
                                       </table>
                                    </div>
                                 </li>
                                 <li class="li liexpand">The status returned by <samp class="ph codeph">cudnnBackendFinalize()</samp> or
                                    							<samp class="ph codeph">cudnnBackendExecute()</samp> on a
                                    							<samp class="ph codeph">CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR</samp> may change
                                    						depending on the version of the dynamic dependencies of cuDNN. As of this
                                    						writing, only cuBLAS is known to affect the return status of these function
                                    						calls.
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not
                                    						required to consider padding. Users of cuDNN can witness an unexpected lack
                                    						of problem support when forward convolution spatial dimensions are less than
                                    						the filter size and padding is nonzero but is sufficient to extend spatial
                                    						dimensions to or beyond filter dimensions. This is commonly observed with,
                                    						but not limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand">When performing batch normalization in cuDNN, the operation is allowed to
                                    						proceed if the output tensor strides are overlapping, however, there is no
                                    						guarantee of deterministic results.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 25</samp> for convolution
                                    						backwards data (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp>) does not support
                                    						tensors in which the product N*C*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX =</samp><samp class="ph codeph">1 for convolution
                                       							backwards data</samp> (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp>) does not support
                                    						tensors in which the product N*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. This issue has been present in all previous releases of
                                    						cuDNN and exercising the use case for the engine would show incorrect
                                    						results.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later. It also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples must be installed in a writable location. If not installed in a
                                    						writable location, the samples can crash.
                                 </li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit nondeterministic
                                       							behavior when the cuDNN library is built with CUDA Toolkit 10.2 or
                                       							higher. This is the result of a new buffer management and heuristics in
                                       							the cuBLAS library. As described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This happens
                                       							when two buffer sizes (16 KB and 4 MB) are used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting
                                       							for a buffer of that size to be released, a smaller buffer may be used
                                       							with a different GPU kernel. The kernel selection may affect numerical
                                       							results. The user can eliminate the nondeterministic behavior of cuDNN
                                       							RNN and multihead attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16
                                       							KB each in GPU memory while the second setting creates two buffers of 4
                                       							MB each. The default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors
                                    						in order to run efficiently. As always, cuDNN recommends users to align
                                    						tensors to 16 byte boundaries that will be sufficiently aligned for any
                                    						computational option in cuDNN. Doing otherwise may cause performance
                                    						regressions in cuDNN 8.0.x compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and
                                    						the output is in FP16 (half float), there are cases where the numerical
                                    						accuracy between the different algorithms might differ. cuDNN 8.0.x users
                                    						can target the backend API to query the numerical notes of the algorithms to
                                    						get the information programmatically. There are cases where algo0 and algo1
                                    						will have a reduced precision accumulation when users target the legacy API.
                                    						In all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and
                                    						backward filter, grouped convolution with groups larger than 1 and with odd
                                    						product of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D
                                    						convolution), <samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on
                                    						devices older than NVIDIA Volta. To prevent a potential illegal memory
                                    						access by an instruction that only has a 16-bit version in NVIDIA Volta and
                                    						later, pad at least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use
                                    						texture-based load structure for performance improvements particularly in
                                    						older hardware architectures. Users can opt out of using texture using the
                                    						environmental variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this
                                    						variable is removed. Texture loading is turned off by default. Users who
                                    						want to continue to use texture-based load, can adapt the new backend API,
                                    						and toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check
                                    						API (for example, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command
                                    						explicitly to resolve the undefined symbols from cuDNN static
                                    						libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA
                                    						Pascal and later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-892__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-892__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in
                                    						compatibility mode are tested with compute-sanitizer,
                                    							<samp class="ph codeph">cuGetProcAddress</samp> failures with error code 500 will
                                    						arise due to missing functions. This error can be ignored, or suppressed
                                    						with the <samp class="ph codeph">--report-api-errors no</samp> option, as this is due to
                                    						CUDA backward compatibility checking if a function is usable with the CUDA
                                    						toolkit combination. The functions are introduced in a later version of CUDA
                                    						but are not available on the current platform. The absence of these
                                    						functions is harmless and will not give rise to any functional issues.
                                 </li>
                                 <li class="li liexpand">Users of <samp class="ph codeph">cudnn_cnn_infer_static.a</samp> may need to update their
                                    						application linkage so that symbols absent in that library are subsequently
                                    						made available with <samp class="ph codeph">cudnn_ops_infer_static.a</samp>. On Linux,
                                    						this is specifying the ops library after <samp class="ph codeph">cnn</samp> on the linker
                                    						line. The same applies to <samp class="ph codeph">cudnn_cnn_train_static.a</samp> and
                                    							<samp class="ph codeph">cudnn_ops_train_static.a</samp>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-891"><a name="rel-891" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-891" name="rel-891" shape="rect">1.7.&nbsp;cuDNN Release 8.9.1</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">These are the NVIDIA cuDNN 8.9.1 Release Notes. These Release Notes include fixes
                           		from the previous cuDNN releases as well as the following additional changes.
                        </p>
                        <div class="section" id="rel-891__section_l2m_s4c_2jb"><a name="rel-891__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">These Release Notes are applicable to both cuDNN and NVIDIA JetPack™
                              				users of cuDNN unless appended specifically with <em class="ph i">(not applicable for Jetson
                                 					platforms)</em>.
                           </p>
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-891__section_obv_4r2_3xb"><a name="rel-891__section_obv_4r2_3xb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-891__ul_fwj_pr2_3xb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-891__ul_fwj_pr2_3xb">
                                 <li class="li liexpand">Improved library-wide error reporting coverage by providing the triggered error condition
                                    						for <samp class="ph codeph">CUDNN_STATUS_BAD_PARAM</samp> and
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> in the error and warning
                                    						level logging. The majority of error logs now include not just the error
                                    						code, but also the error "reason" (that is, the condition that failed).
                                    						Refer to <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#unique_1476739855" target="_blank" shape="rect">Error Reporting And API Logging</a>
                                    						for instructions to enable the error reporting feature.
                                 </li>
                                 <li class="li liexpand">Expanded support of fused flash attention inference and training to sequence
                                    						lengths of multiples of 64 and hidden dimension of 64 and 128.
                                 </li>
                                 <li class="li liexpand">Expanded support of fused attention to accept a general mask as an input for
                                    						training, added backward pass for relative positional encoding, and added
                                    						support for sequence lengths of multiples of 64 (not multiples of 128) up to
                                    						512.
                                 </li>
                                 <li class="li liexpand">Fine-tuned runtime fusion heuristics for NVIDIA Ada architecture for FP16
                                    						and INT8 convolution forward propagation.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">HEUR_MODE_A</samp> supports <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#bnaddrelu" target="_blank" shape="rect"><samp class="ph codeph">BnAddRelu</samp></a> and
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#dreluforkdbn" target="_blank" shape="rect"><samp class="ph codeph">DReluForkDBn</samp></a>
                                    						patterns. The support is currently limited to operation graphs with single
                                    						node multi-GPU batch norms without any pointwise node fusions.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-891__section_v5m_d32_gxb"><a name="rel-891__section_v5m_d32_gxb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:<a name="rel-891__ul_iq1_232_gxb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-891__ul_iq1_232_gxb">
                                 <li class="li liexpand">Corrected the engine config returned by heuristics for FP8 convolution.</li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnGetConvolutionForwardAlgorithm_v7()</samp>,
                                    							<samp class="ph codeph">cudnnGetConvolutionBackwardDataAlgorithm_v7()</samp>, and
                                    							<samp class="ph codeph">cudnnGetConvolutionBackwardFilterAlgorithm_v7()</samp> would
                                    						exhibit a memory leak on NVIDIA Hopper GPUs. This issue has been fixed in
                                    						this release.
                                 </li>
                                 <li class="li liexpand">For packed NCHW tensors using the FP16 datatype, cuDNN attempted to run an
                                    						optimized kernel if the values of N, C, H, and W were even. In cuDNN
                                    						versions before 8.4, it was possible that incorrect values were generated if
                                    						odd values for the strides of N or C were used. This issue has been fixed in
                                    						this release.
                                 </li>
                                 <li class="li liexpand">cuDNN 8.9.1 added tensor alignment checks to instance norm and layer norm engines to
                                    						prevent IMA issues.
                                 </li>
                                 <li class="li liexpand">Starting in cuDNN 8.9.1, the <samp class="ph codeph">const int32_t devSeqLengths[]</samp> argument in
                                    							<samp class="ph codeph">cudnnRNNForward()</samp>,
                                    							<samp class="ph codeph">cudnnRNNBackwardData_v8()</samp>, and
                                    							<samp class="ph codeph">cudnnRNNBackwardWeights_v8()</samp> APIs will be ignored. All
                                    						three functions will source variable sequence length arrays from RNN data
                                    						descriptors, configured through the <samp class="ph codeph">seqLengthArray</samp>
                                    						parameter of <samp class="ph codeph">cudnnSetRNNDataDescriptor()</samp>. The user does not
                                    						need to transfer this array to device memory; the operation will be
                                    						performed automatically by RNN APIs. This refinement simplifies the usage of
                                    						cuDNN RNN APIs. It is also a workaround for random crashes in multi-GPU RNN
                                    						training on TensorFlow. Replacing earlier versions of cuDNN 8.x shared
                                    						libraries with cuDNN 8.9.1 will eliminate those crashes without forcing the
                                    						user to switch the TensorFlow version. The cause of intermittent corruptions
                                    						of <samp class="ph codeph">devSeqLengths[]</samp>, fed to RNN APIs, is still being
                                    						investigated.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there were known performance regressions up to 2x on
                                    						select configurations for AlexNet-like models on NVIDIA Turing GPUs. This
                                    						issue has been fixed in this release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-891__section_t2m_s5p_nnb"><a name="rel-891__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel-891__ul_mbs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-891__ul_mbs_qgh_fsb">
                                 <li class="li liexpand">cuDNN's usage of cuBLAS from CUDA Toolkit 12.1 may result in race-check
                                    						failures when the library is tested under compute sanitizer. These failures
                                    						are believed to be a cuBLAS issue and are being investigated. A workaround
                                    						for this issue is to use cuBLAS from CUDA Toolkit 12.0.
                                 </li>
                                 <li class="li liexpand">A compiler bug in NVRTC in CUDA version 11.7 and earlier, was causing
                                    						incorrect outputs when computing logical operations on boolean input tensors
                                    						in the runtime fusion engine. A workaround had been integrated to avoid the
                                    						most common issues. However, it is highly recommended to update to at least
                                    						CUDA version 11.7u1 for a fix. Specifically, known failure cases are when
                                    						pointwise operations of mode <samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_NOT</samp>,
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_AND</samp> or
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_OR</samp> operates on boolean tensors.
                                    						This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 57</samp> for convolution forward
                                    						and <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 62</samp> for convolution
                                    						backward data may have performance regressions for non-zero beta problems.
                                    						However, they are not typically recommended by cuDNN heuristics, so the
                                    						observable impact should be minimal.
                                 </li>
                                 <li class="li liexpand">Use of <samp class="ph codeph">cudnnFusedOpsExecute()</samp> on NVIDIA Volta compatible
                                    						architectures hosted on AArch64 systems may generate incorrect results when
                                    						used with <samp class="ph codeph">cudnnFusedOps_t</samp> set to
                                    							<samp class="ph codeph">CUDNN_FUSED_SCALE_BIAS_ACTIVATION_WGRAD</samp>.
                                 </li>
                                 <li class="li liexpand">With CUDA 11.7, the NVRTC library may cause a small memory leak when using
                                    						the runtime fusion engine. The issue has been addressed in CUDA Toolkit 11.8
                                    						or later. 
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 0</samp> for
                                    							<samp class="ph codeph">DgradDreluBnBwdWeights</samp> may see a performance regression
                                    						when moving from cuDNN 8.8 to cuDNN 8.9.
                                 </li>
                                 <li class="li liexpand">If cuDNN 8.4.1 or earlier statically links with
                                    							<samp class="ph codeph">libcudart.so</samp> from the CUDA Toolkit 11.7 or later, when
                                    						the LFL feature is activated, the results from
                                    							<samp class="ph codeph">cudnnFind*Algo</samp> will not be accurate.
                                 </li>
                                 <li class="li liexpand">The documentation for <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnReorderFilterAndBias" target="_blank" shape="rect"><samp class="ph codeph">cudnnReorderFilterAndBias()</samp></a> requires
                                    						corrections for clarity.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX
                                    						3090 compared to 2080 Ti. This includes EfficientNet with up to 6x
                                    						performance difference, UNet up to 1.6x performance difference and Tacotron
                                    						up to 1.6x performance difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    							<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior,
                                    						accumulator data types, and so on, have not been documented as of yet.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN 8.4.0 may observe a slowdown in the Single Shot Multibox
                                    						Detector (SSD) model. This will be fixed in a future release.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with filter size
                                    						1x1. The severity would be different depending on which version of the CUDA
                                    						Toolkit the user is using.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with high group
                                    						count. The issue is more severe on V100.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-891__section_dph_zkv_jrb"><a name="rel-891__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and
                              				the NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-891__section_nbs_qgh_fsb"><a name="rel-891__section_nbs_qgh_fsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel-891__ul_obs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-891__ul_obs_qgh_fsb">
                                 <li class="li liexpand">Disabling CUDA context preemption on Windows can sometimes lead to
                                    							<samp class="ph codeph">CUDNN_INTERNAL_ERRORS</samp> being returned from convolution
                                    						runs. When using cuDNN, do not disable CUDA context preemption.
                                 </li>
                                 <li class="li liexpand">When using the cuDNN static library, you will need to use the same
                                    						major.minor version of the CUDA Toolkit by which cuDNN was built to build
                                    						your application. Refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a> for
                                    						the exact supported CUDA versions.
                                 </li>
                                 <li class="li liexpand">In cuDNN 8.9.0, runtime fusion engines (with
                                    							<samp class="ph codeph">CUDNN_BEHAVIOR_NOTE_RUNTIME_COMPILATION</samp>) will only work
                                    						with NVRTC from CUDA Toolkit 11.8, 12.0 and 12.1. They are not guaranteed to
                                    						be forward compatible with future CUDA 12.x Toolkits.
                                 </li>
                                 <li class="li liexpand">Within the cuDNN version 8 backend API, the following engines are known not
                                    						to be thread-safe when executed simultaneously with multiple threads sharing
                                    						the same execution plan:
                                    
                                    
                                    <div class="tablenoborder"><a name="rel-891__table_t2h_lsp_r5b" shape="rect">
                                          <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="rel-891__table_t2h_lsp_r5b" class="table" frame="border" border="1" rules="all">
                                          <caption><span class="tablecap">Table 7. Engines That Are Not Thread-Safe</span></caption>
                                          <thead class="thead" align="left">
                                             <tr class="row">
                                                <th class="entry" valign="top" width="20%" id="d54e8472" rowspan="1" colspan="1"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e8476" rowspan="1" colspan="1"><samp class="ph codeph">convolution forward</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e8480" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward data</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e8484" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward filter</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e8488" rowspan="1" colspan="1"><samp class="ph codeph">cudnnConvolutionBiasActivationForward</samp></th>
                                             </tr>
                                          </thead>
                                          <tbody class="tbody">
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e8472" rowspan="1" colspan="1">A100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e8476" rowspan="1" colspan="1"><a name="rel-891__ul_c53_qsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-891__ul_c53_qsp_r5b">
                                                      <li class="li">36</li>
                                                      <li class="li">38</li>
                                                      <li class="li">45</li>
                                                      <li class="li">46</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e8480" rowspan="1" colspan="1"><a name="rel-891__ul_rh2_ssp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-891__ul_rh2_ssp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">19</li>
                                                      <li class="li">22</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">28</li>
                                                      <li class="li">40</li>
                                                      <li class="li">46</li>
                                                      <li class="li">51</li>
                                                      <li class="li">56</li>
                                                      <li class="li">57</li>
                                                      <li class="li">58</li>
                                                      <li class="li">59</li>
                                                      <li class="li">60</li>
                                                      <li class="li">65</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e8484" rowspan="1" colspan="1"><a name="rel-891__ul_mjw_vsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-891__ul_mjw_vsp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">21</li>
                                                      <li class="li">23</li>
                                                      <li class="li">33</li>
                                                      <li class="li">37</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                      <li class="li">49</li>
                                                      <li class="li">50</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e8488" rowspan="1" colspan="1"><a name="rel-891__ul_mfh_1d3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-891__ul_mfh_1d3_kvb">
                                                      <li class="li">4024</li>
                                                      <li class="li">4026</li>
                                                      <li class="li">4032</li>
                                                      <li class="li">4033</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e8472" rowspan="1" colspan="1">V100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e8476" rowspan="1" colspan="1"><a name="rel-891__ul_qrn_btp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-891__ul_qrn_btp_r5b">
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">12</li>
                                                      <li class="li">16</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">49</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e8480" rowspan="1" colspan="1"><a name="rel-891__ul_ffy_dtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-891__ul_ffy_dtp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">44</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e8484" rowspan="1" colspan="1"><a name="rel-891__ul_lg1_htp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-891__ul_lg1_htp_r5b">
                                                      <li class="li">6</li>
                                                      <li class="li">7</li>
                                                      <li class="li">21</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">52</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e8488" rowspan="1" colspan="1"><a name="rel-891__ul_w1l_cd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-891__ul_w1l_cd3_kvb">
                                                      <li class="li">4002</li>
                                                      <li class="li">4015</li>
                                                      <li class="li">4008</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4030</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e8472" rowspan="1" colspan="1">T4</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e8476" rowspan="1" colspan="1"><a name="rel-891__ul_q3j_jtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-891__ul_q3j_jtp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">14</li>
                                                      <li class="li">16</li>
                                                      <li class="li">18</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">50</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e8480" rowspan="1" colspan="1"><a name="rel-891__ul_ax4_ntp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-891__ul_ax4_ntp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">15</li>
                                                      <li class="li">18</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">37</li>
                                                      <li class="li">39</li>
                                                      <li class="li">44</li>
                                                      <li class="li">45</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e8484" rowspan="1" colspan="1"><a name="rel-891__ul_j3t_rtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-891__ul_j3t_rtp_r5b">
                                                      <li class="li">12</li>
                                                      <li class="li">21</li>
                                                      <li class="li">27</li>
                                                      <li class="li">28</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">53</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e8488" rowspan="1" colspan="1"><a name="rel-891__ul_kc5_jd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-891__ul_kc5_jd3_kvb">
                                                      <li class="li">4015</li>
                                                      <li class="li">4003</li>
                                                      <li class="li">4004</li>
                                                      <li class="li">4009</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4031</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                          </tbody>
                                       </table>
                                    </div>
                                 </li>
                                 <li class="li liexpand">The status returned by <samp class="ph codeph">cudnnBackendFinalize()</samp> or
                                    							<samp class="ph codeph">cudnnBackendExecute()</samp> on a
                                    							<samp class="ph codeph">CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR</samp> may change
                                    						depending on the version of the dynamic dependencies of cuDNN. As of this
                                    						writing, only cuBLAS is known to affect the return status of these function
                                    						calls.
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not
                                    						required to consider padding. Users of cuDNN can witness an unexpected lack
                                    						of problem support when forward convolution spatial dimensions are less than
                                    						the filter size and padding is nonzero but is sufficient to extend spatial
                                    						dimensions to or beyond filter dimensions. This is commonly observed with,
                                    						but not limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand">When performing batch normalization in cuDNN, the operation is allowed to
                                    						proceed if the output tensor strides are overlapping, however, there is no
                                    						guarantee of deterministic results.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 25</samp> for convolution
                                    						backwards data (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp>) does not support
                                    						tensors in which the product N*C*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX =</samp><samp class="ph codeph">1 for convolution
                                       							backwards data</samp> (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp>) does not support
                                    						tensors in which the product N*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. This issue has been present in all previous releases of
                                    						cuDNN and exercising the use case for the engine would show incorrect
                                    						results.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later. It also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples must be installed in a writable location. If not installed in a
                                    						writable location, the samples can crash.
                                 </li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit nondeterministic
                                       							behavior when the cuDNN library is built with CUDA Toolkit 10.2 or
                                       							higher. This is the result of a new buffer management and heuristics in
                                       							the cuBLAS library. As described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This happens
                                       							when two buffer sizes (16 KB and 4 MB) are used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting
                                       							for a buffer of that size to be released, a smaller buffer may be used
                                       							with a different GPU kernel. The kernel selection may affect numerical
                                       							results. The user can eliminate the nondeterministic behavior of cuDNN
                                       							RNN and multihead attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16
                                       							KB each in GPU memory while the second setting creates two buffers of 4
                                       							MB each. The default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors
                                    						in order to run efficiently. As always, cuDNN recommends users to align
                                    						tensors to 16 byte boundaries that will be sufficiently aligned for any
                                    						computational option in cuDNN. Doing otherwise may cause performance
                                    						regressions in cuDNN 8.0.x compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and
                                    						the output is in FP16 (half float), there are cases where the numerical
                                    						accuracy between the different algorithms might differ. cuDNN 8.0.x users
                                    						can target the backend API to query the numerical notes of the algorithms to
                                    						get the information programmatically. There are cases where algo0 and algo1
                                    						will have a reduced precision accumulation when users target the legacy API.
                                    						In all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and
                                    						backward filter, grouped convolution with groups larger than 1 and with odd
                                    						product of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D
                                    						convolution), <samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on
                                    						devices older than NVIDIA Volta. To prevent a potential illegal memory
                                    						access by an instruction that only has a 16-bit version in NVIDIA Volta and
                                    						later, pad at least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use
                                    						texture-based load structure for performance improvements particularly in
                                    						older hardware architectures. Users can opt out of using texture using the
                                    						environmental variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this
                                    						variable is removed. Texture loading is turned off by default. Users who
                                    						want to continue to use texture-based load, can adapt the new backend API,
                                    						and toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check
                                    						API (for example, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command
                                    						explicitly to resolve the undefined symbols from cuDNN static
                                    						libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA
                                    						Pascal and later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-891__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-891__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in
                                    						compatibility mode are tested with compute-sanitizer,
                                    							<samp class="ph codeph">cuGetProcAddress</samp> failures with error code 500 will
                                    						arise due to missing functions. This error can be ignored, or suppressed
                                    						with the <samp class="ph codeph">--report-api-errors no</samp> option, as this is due to
                                    						CUDA backward compatibility checking if a function is usable with the CUDA
                                    						toolkit combination. The functions are introduced in a later version of CUDA
                                    						but are not available on the current platform. The absence of these
                                    						functions is harmless and will not give rise to any functional issues.
                                 </li>
                                 <li class="li liexpand">Users of <samp class="ph codeph">cudnn_cnn_infer_static.a</samp> may need to update their
                                    						application linkage so that symbols absent in that library are subsequently
                                    						made available with <samp class="ph codeph">cudnn_ops_infer_static.a</samp>. On Linux,
                                    						this is specifying the ops library after <samp class="ph codeph">cnn</samp> on the linker
                                    						line. The same applies to <samp class="ph codeph">cudnn_cnn_train_static.a</samp> and
                                    							<samp class="ph codeph">cudnn_ops_train_static.a</samp>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-890"><a name="rel-890" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-890" name="rel-890" shape="rect">1.8.&nbsp;cuDNN Release 8.9.0</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">These are the NVIDIA cuDNN 8.9.0 Release Notes. These Release Notes include fixes
                           		from the previous cuDNN releases as well as the following additional changes.
                        </p>
                        <div class="section" id="rel-890__section_l2m_s4c_2jb"><a name="rel-890__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">These Release Notes are applicable to both cuDNN and NVIDIA JetPack™
                              				users of cuDNN unless appended specifically with <em class="ph i">(not applicable for Jetson
                                 					platforms)</em>.
                           </p>
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-890__section_djp_lny_wwb"><a name="rel-890__section_djp_lny_wwb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-890__ul_ak3_mny_wwb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-890__ul_ak3_mny_wwb">
                                 <li class="li liexpand">Added support for transformer models training and inference using Flash Attention in cuDNN
                                    						runtime fusion engine. For more information, refer to the flash fused
                                    						multi-head attention <samp class="ph codeph">fprop</samp> and <samp class="ph codeph">bprop</samp>
                                    						patterns listed in the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#specialized-runtime-fusion-engines" target="_blank" shape="rect">NVIDIA cuDNN Developer Guide</a>.
                                 </li>
                                 <li class="li liexpand">Added support for FP8 fused-multi-head attention training and inference
                                    						support targeting BERT on NVIDIA Hopper GPUs.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine has been split into four engines (with engine id 0, 1, 2, and 3)
                                    						according to functional coverage and device architecture.<a name="rel-890__ul_nby_xfh_1xb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-890__ul_nby_xfh_1xb">
                                       <li class="li">Engine id 0 targets general runtime fusion for the NVIDIA Volta and
                                          								NVIDIA Turing architectures.
                                       </li>
                                       <li class="li">Engine id 1 targets general runtime fusion for the NVIDIA Ampere
                                          								architecture.
                                       </li>
                                       <li class="li">Engine id 2 targets general runtime fusion for the NVIDIA Hopper
                                          								architecture.
                                       </li>
                                       <li class="li">Engine id 3 targets multihead attention related fusions for the
                                          								NVIDIA Ampere and NVIDIA Hopper architectures.
                                       </li>
                                    </ul>
                                    <p class="p">This split has enabled the heuristics to have more control over
                                       							kernel selection. Knob information is now more precise for each splitted
                                       							engine. As a result, internal testing coverage has also
                                       						improved.
                                    </p>
                                 </li>
                                 <li class="li liexpand">A new runtime fusion engine (with engine id 4) has been introduced to enable up to 4x
                                    						better compilation time and better epilogue fusion efficiency. It’s
                                    						currently recommended by heuristics for a subset of patterns and problem
                                    						sizes. We expect the coverage to grow further in future releases.
                                 </li>
                                 <li class="li liexpand">Improved FP8 Dgrad heuristics to have generally good out-of-the-box support for both
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> and
                                    							<samp class="ph codeph">CUDNN_DATA_FAST_FLOAT_FOR_FP8</samp> compute type.
                                 </li>
                                 <li class="li liexpand">Enabled a new knob <samp class="ph codeph">CUDNN_KNOB_TYPE_SPLIT_K_SLC</samp> in runtime fusion engines
                                    						that can be used to control Split-K in SM 7.X, SM 8.X or Segment-K in SM 9.X
                                    						convolution or GEMM kernels.
                                 </li>
                                 <li class="li liexpand">Added TF32 heuristics for runtime fusion for the NVIDIA Hopper architecture.</li>
                                 <li class="li liexpand">Added support for the <samp class="ph codeph">DgradDreluBNBwdWeight</samp> fusion pattern
                                    						on NVIDIA Hopper GPUs. For more information, refer to the
                                    							<samp class="ph codeph">DgradDreluBNBwdWeight</samp> pattern listed in the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#dgraddrelubnbwdweight" target="_blank" shape="rect">NVIDIA cuDNN Developer Guide</a>.
                                 </li>
                                 <li class="li liexpand">Two new patterns were added to the ConvBNFprop fusion patterns. These are
                                    						supported on NVIDIA Hopper GPU’s. For more information, refer to the
                                    						ConvBNFprop pattern listed in the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#convbnfprop" target="_blank" shape="rect">NVIDIA cuDNN Developer Guide</a>.
                                 </li>
                                 <li class="li liexpand">Added a new runtime compiled engine with some fusion capabilities to support
                                    						single GPU and single node multi-GPU batch normalization. Previously, this
                                    						engine was statically compiled, but it’s now a runtime compiled engine in
                                    						cuDNN 8.9. For more information, refer to the <samp class="ph codeph">BNAddRelu</samp>
                                    						pattern listed in the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html" target="_blank" shape="rect"><u class="ph u">NVIDIA cuDNN Developer
                                          							Guide</u></a>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-890__section_bws_lm3_ywb"><a name="rel-890__section_bws_lm3_ywb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:<a name="rel-890__ul_zsh_mm3_ywb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-890__ul_zsh_mm3_ywb">
                                 <li class="li liexpand">When cuDNN executed FP8 matrix multiplication and convolution kernels with
                                    						compute type <samp class="ph codeph">CUDNN_DATA_FLOAT</samp>, the numerical precision was
                                    						lower than expected. This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Some engines incorrectly returned a success status when calling
                                    							<samp class="ph codeph">cudnnBackendFinalize()</samp> with a backend engine descriptor
                                    						for a bfloat16 problem on hardware that did not support the bfloat16 type.
                                    						Attempted execution of the problem with
                                    							<samp class="ph codeph">cudnnBackendExecute()</samp> would return the expected status
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp>. This issue has been fixed
                                    						in this release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-890__section_t2m_s5p_nnb"><a name="rel-890__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel-890__ul_mbs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-890__ul_mbs_qgh_fsb">
                                 <li class="li liexpand"><samp class="ph codeph">cudnnGetConvolutionForwardAlgorithm_v7()</samp>,
                                    							<samp class="ph codeph">cudnnGetConvolutionBackwardDataAlgorithm_v7()</samp>, and
                                    							<samp class="ph codeph">cudnnGetConvolutionBackwardFilterAlgorithm_v7()</samp> exhibit
                                    						a memory leak on NVIDIA Hopper GPUs.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 57</samp> for convolution forward
                                    						and <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 62</samp> for convolution
                                    						backward data may have performance regressions for non-zero beta problems.
                                    						However, they are not typically recommended by cuDNN heuristics, so the
                                    						observable impact should be minimal.
                                 </li>
                                 <li class="li liexpand">Use of <samp class="ph codeph">cudnnFusedOpsExecute()</samp> on NVIDIA Volta compatible
                                    						architectures hosted on AArch64 systems may generate incorrect results when
                                    						used with <samp class="ph codeph">cudnnFusedOps_t</samp> set to
                                    							<samp class="ph codeph">CUDNN_FUSED_SCALE_BIAS_ACTIVATION_WGRAD</samp>.
                                 </li>
                                 <li class="li liexpand">With CUDA 11.7, the NVRTC library may cause a small memory leak when using
                                    						the runtime fusion engine. The issue has been addressed in CUDA Toolkit 11.8
                                    						or later. 
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 0</samp> for
                                    							<samp class="ph codeph">DgradDreluBnBwdWeights</samp> may see a performance regression
                                    						when moving from cuDNN 8.8 to cuDNN 8.9.
                                 </li>
                                 <li class="li liexpand">A compiler bug in NVRTC in CUDA version 11.7 and earlier, was causing
                                    						incorrect outputs when computing logical operations on boolean input tensors
                                    						in the runtime fusion engine. A workaround has been integrated in this
                                    						release to avoid the most common issues. However, it is highly recommended
                                    						to update to at least CUDA version 11.7u1 for a fix. Specifically, known
                                    						failure cases are when pointwise operations of mode
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_NOT</samp>,
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_AND</samp> or
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_OR</samp> operates on boolean
                                    						tensors.
                                 </li>
                                 <li class="li liexpand">If cuDNN 8.4.1 or earlier statically links with
                                    							<samp class="ph codeph">libcudart.so</samp> from the CUDA Toolkit 11.7 or later, when
                                    						the LFL feature is activated, the results from
                                    							<samp class="ph codeph">cudnnFind*Algo</samp> will not be accurate.
                                 </li>
                                 <li class="li liexpand">For packed NCHW tensors using the FP16 datatype, cuDNN attempts to run an
                                    						optimized kernel if the values of N, C, H, and W are even. In cuDNN versions
                                    						before 8.4, it is possible that incorrect values are generated if odd values
                                    						for the strides of N or C are used.
                                 </li>
                                 <li class="li liexpand">The documentation for <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnReorderFilterAndBias" target="_blank" shape="rect"><samp class="ph codeph">cudnnReorderFilterAndBias()</samp></a> requires
                                    						corrections for clarity.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX
                                    						3090 compared to 2080 Ti. This includes EfficientNet with up to 6x
                                    						performance difference, UNet up to 1.6x performance difference and Tacotron
                                    						up to 1.6x performance difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    							<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there are known performance regressions up to 2x on
                                    						select configurations for AlexNet-like models on NVIDIA Turing GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior,
                                    						accumulator data types, and so on, have not been documented as of yet.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN 8.4.0 may observe a slowdown in the Single Shot Multibox
                                    						Detector (SSD) model. This will be fixed in a future release.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with filter size
                                    						1x1. The severity would be different depending on which version of the CUDA
                                    						Toolkit the user is using.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with high group
                                    						count. The issue is more severe on V100.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-890__section_dph_zkv_jrb"><a name="rel-890__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and
                              				the NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-890__section_nbs_qgh_fsb"><a name="rel-890__section_nbs_qgh_fsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel-890__ul_obs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-890__ul_obs_qgh_fsb">
                                 <li class="li liexpand">Disabling CUDA context preemption on Windows can sometimes lead to
                                    							<samp class="ph codeph">CUDNN_INTERNAL_ERRORS</samp> being returned from convolution
                                    						runs. When using cuDNN, do not disable CUDA context preemption.
                                 </li>
                                 <li class="li liexpand">When using the cuDNN static library, you will need to use the same
                                    						major.minor version of the CUDA Toolkit by which cuDNN was built to build
                                    						your application. Refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a> for
                                    						the exact supported CUDA versions.
                                 </li>
                                 <li class="li liexpand">In cuDNN 8.9.0, runtime fusion engines (with
                                    							<samp class="ph codeph">CUDNN_BEHAVIOR_NOTE_RUNTIME_COMPILATION</samp>) will only work
                                    						with NVRTC from CUDA Toolkit 11.8, 12.0 and 12.1. They are not guaranteed to
                                    						be forward compatible with future CUDA 12.x Toolkits.
                                 </li>
                                 <li class="li liexpand">Within the cuDNN version 8 backend API, the following engines are known not
                                    						to be thread-safe when executed simultaneously with multiple threads sharing
                                    						the same execution plan:
                                    
                                    
                                    <div class="tablenoborder"><a name="rel-890__table_t2h_lsp_r5b" shape="rect">
                                          <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="rel-890__table_t2h_lsp_r5b" class="table" frame="border" border="1" rules="all">
                                          <caption><span class="tablecap">Table 8. Engines That Are Not Thread-Safe</span></caption>
                                          <thead class="thead" align="left">
                                             <tr class="row">
                                                <th class="entry" valign="top" width="20%" id="d54e9636" rowspan="1" colspan="1"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e9640" rowspan="1" colspan="1"><samp class="ph codeph">convolution forward</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e9644" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward data</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e9648" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward filter</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e9652" rowspan="1" colspan="1"><samp class="ph codeph">cudnnConvolutionBiasActivationForward</samp></th>
                                             </tr>
                                          </thead>
                                          <tbody class="tbody">
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e9636" rowspan="1" colspan="1">A100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e9640" rowspan="1" colspan="1"><a name="rel-890__ul_c53_qsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-890__ul_c53_qsp_r5b">
                                                      <li class="li">36</li>
                                                      <li class="li">38</li>
                                                      <li class="li">45</li>
                                                      <li class="li">46</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e9644" rowspan="1" colspan="1"><a name="rel-890__ul_rh2_ssp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-890__ul_rh2_ssp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">19</li>
                                                      <li class="li">22</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">28</li>
                                                      <li class="li">40</li>
                                                      <li class="li">46</li>
                                                      <li class="li">51</li>
                                                      <li class="li">56</li>
                                                      <li class="li">57</li>
                                                      <li class="li">58</li>
                                                      <li class="li">59</li>
                                                      <li class="li">60</li>
                                                      <li class="li">65</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e9648" rowspan="1" colspan="1"><a name="rel-890__ul_mjw_vsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-890__ul_mjw_vsp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">21</li>
                                                      <li class="li">23</li>
                                                      <li class="li">33</li>
                                                      <li class="li">37</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                      <li class="li">49</li>
                                                      <li class="li">50</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e9652" rowspan="1" colspan="1"><a name="rel-890__ul_mfh_1d3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-890__ul_mfh_1d3_kvb">
                                                      <li class="li">4024</li>
                                                      <li class="li">4026</li>
                                                      <li class="li">4032</li>
                                                      <li class="li">4033</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e9636" rowspan="1" colspan="1">V100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e9640" rowspan="1" colspan="1"><a name="rel-890__ul_qrn_btp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-890__ul_qrn_btp_r5b">
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">12</li>
                                                      <li class="li">16</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">49</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e9644" rowspan="1" colspan="1"><a name="rel-890__ul_ffy_dtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-890__ul_ffy_dtp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">44</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e9648" rowspan="1" colspan="1"><a name="rel-890__ul_lg1_htp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-890__ul_lg1_htp_r5b">
                                                      <li class="li">6</li>
                                                      <li class="li">7</li>
                                                      <li class="li">21</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">52</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e9652" rowspan="1" colspan="1"><a name="rel-890__ul_w1l_cd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-890__ul_w1l_cd3_kvb">
                                                      <li class="li">4002</li>
                                                      <li class="li">4015</li>
                                                      <li class="li">4008</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4030</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e9636" rowspan="1" colspan="1">T4</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e9640" rowspan="1" colspan="1"><a name="rel-890__ul_q3j_jtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-890__ul_q3j_jtp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">14</li>
                                                      <li class="li">16</li>
                                                      <li class="li">18</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">50</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e9644" rowspan="1" colspan="1"><a name="rel-890__ul_ax4_ntp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-890__ul_ax4_ntp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">15</li>
                                                      <li class="li">18</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">37</li>
                                                      <li class="li">39</li>
                                                      <li class="li">44</li>
                                                      <li class="li">45</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e9648" rowspan="1" colspan="1"><a name="rel-890__ul_j3t_rtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-890__ul_j3t_rtp_r5b">
                                                      <li class="li">12</li>
                                                      <li class="li">21</li>
                                                      <li class="li">27</li>
                                                      <li class="li">28</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">53</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e9652" rowspan="1" colspan="1"><a name="rel-890__ul_kc5_jd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-890__ul_kc5_jd3_kvb">
                                                      <li class="li">4015</li>
                                                      <li class="li">4003</li>
                                                      <li class="li">4004</li>
                                                      <li class="li">4009</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4031</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                          </tbody>
                                       </table>
                                    </div>
                                 </li>
                                 <li class="li liexpand">The status returned by <samp class="ph codeph">cudnnBackendFinalize()</samp> or
                                    							<samp class="ph codeph">cudnnBackendExecute()</samp> on a
                                    							<samp class="ph codeph">CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR</samp> may change
                                    						depending on the version of the dynamic dependencies of cuDNN. As of this
                                    						writing, only cuBLAS is known to affect the return status of these function
                                    						calls.
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not
                                    						required to consider padding. Users of cuDNN can witness an unexpected lack
                                    						of problem support when forward convolution spatial dimensions are less than
                                    						the filter size and padding is nonzero but is sufficient to extend spatial
                                    						dimensions to or beyond filter dimensions. This is commonly observed with,
                                    						but not limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand">When performing batch normalization in cuDNN, the operation is allowed to
                                    						proceed if the output tensor strides are overlapping, however, there is no
                                    						guarantee of deterministic results.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 25</samp> for convolution
                                    						backwards data (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp>) does not support
                                    						tensors in which the product N*C*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX =</samp><samp class="ph codeph">1 for convolution
                                       							backwards data</samp> (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp>) does not support
                                    						tensors in which the product N*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. This issue has been present in all previous releases of
                                    						cuDNN and exercising the use case for the engine would show incorrect
                                    						results.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later. It also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples must be installed in a writable location. If not installed in a
                                    						writable location, the samples can crash.
                                 </li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit nondeterministic
                                       							behavior when the cuDNN library is built with CUDA Toolkit 10.2 or
                                       							higher. This is the result of a new buffer management and heuristics in
                                       							the cuBLAS library. As described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This happens
                                       							when two buffer sizes (16 KB and 4 MB) are used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting
                                       							for a buffer of that size to be released, a smaller buffer may be used
                                       							with a different GPU kernel. The kernel selection may affect numerical
                                       							results. The user can eliminate the nondeterministic behavior of cuDNN
                                       							RNN and multihead attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16
                                       							KB each in GPU memory while the second setting creates two buffers of 4
                                       							MB each. The default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors
                                    						in order to run efficiently. As always, cuDNN recommends users to align
                                    						tensors to 16 byte boundaries that will be sufficiently aligned for any
                                    						computational option in cuDNN. Doing otherwise may cause performance
                                    						regressions in cuDNN 8.0.x compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and
                                    						the output is in FP16 (half float), there are cases where the numerical
                                    						accuracy between the different algorithms might differ. cuDNN 8.0.x users
                                    						can target the backend API to query the numerical notes of the algorithms to
                                    						get the information programmatically. There are cases where algo0 and algo1
                                    						will have a reduced precision accumulation when users target the legacy API.
                                    						In all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and
                                    						backward filter, grouped convolution with groups larger than 1 and with odd
                                    						product of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D
                                    						convolution), <samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on
                                    						devices older than NVIDIA Volta. To prevent a potential illegal memory
                                    						access by an instruction that only has a 16-bit version in NVIDIA Volta and
                                    						later, pad at least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use
                                    						texture-based load structure for performance improvements particularly in
                                    						older hardware architectures. Users can opt out of using texture using the
                                    						environmental variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this
                                    						variable is removed. Texture loading is turned off by default. Users who
                                    						want to continue to use texture-based load, can adapt the new backend API,
                                    						and toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check
                                    						API (for example, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command
                                    						explicitly to resolve the undefined symbols from cuDNN static
                                    						libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA
                                    						Pascal and later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-890__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-890__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in
                                    						compatibility mode are tested with compute-sanitizer,
                                    							<samp class="ph codeph">cuGetProcAddress</samp> failures with error code 500 will
                                    						arise due to missing functions. This error can be ignored, or suppressed
                                    						with the <samp class="ph codeph">--report-api-errors no</samp> option, as this is due to
                                    						CUDA backward compatibility checking if a function is usable with the CUDA
                                    						toolkit combination. The functions are introduced in a later version of CUDA
                                    						but are not available on the current platform. The absence of these
                                    						functions is harmless and will not give rise to any functional issues.
                                 </li>
                                 <li class="li liexpand">Users of <samp class="ph codeph">cudnn_cnn_infer_static.a</samp> may need to update their
                                    						application linkage so that symbols absent in that library are subsequently
                                    						made available with <samp class="ph codeph">cudnn_ops_infer_static.a</samp>. On Linux,
                                    						this is specifying the ops library after <samp class="ph codeph">cnn</samp> on the linker
                                    						line. The same applies to <samp class="ph codeph">cudnn_cnn_train_static.a</samp> and
                                    							<samp class="ph codeph">cudnn_ops_train_static.a</samp>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-881"><a name="rel-881" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-881" name="rel-881" shape="rect">1.9.&nbsp;cuDNN Release 8.8.1</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">These are the NVIDIA cuDNN 8.8.1 Release Notes. These Release Notes include fixes
                           		from the previous cuDNN releases as well as the following additional changes.
                        </p>
                        <div class="section" id="rel-881__section_l2m_s4c_2jb"><a name="rel-881__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">These Release Notes are applicable to both cuDNN and NVIDIA JetPack™
                              				users of cuDNN unless appended specifically with <em class="ph i">(not applicable for Jetson
                                 					platforms)</em>.
                           </p>
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-881__section_avj_jd2_3wb"><a name="rel-881__section_avj_jd2_3wb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p"><a name="rel-881__ul_pxv_jd2_3wb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-881__ul_pxv_jd2_3wb">
                                 <li class="li liexpand">In cuDNN 8.8.0, the runtime fusion engine failed with NVRTC from CUDA 12.1. This bug is
                                    						fixed in cuDNN 8.8.1, but the limitation still remains that the runtime
                                    						fusion engine is not forward compatible beyond CUDA 12.1. This limitation
                                    						will be addressed in a future release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-881__section_t2m_s5p_nnb"><a name="rel-881__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel-881__ul_mbs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-881__ul_mbs_qgh_fsb">
                                 <li class="li liexpand">Some engines will incorrectly return a success status when calling
                                    							<samp class="ph codeph">cudnnBackendFinalize()</samp> with a backend engine descriptor
                                    						for a bfloat16 problem on hardware that does not support the bfloat16 type.
                                    						Attempted execution of the problem with
                                    							<samp class="ph codeph">cudnnBackendExecute()</samp> will return the expected status
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp>.
                                 </li>
                                 <li class="li liexpand">Use of <samp class="ph codeph">cudnnFusedOpsExecute()</samp> on NVIDIA Volta compatible
                                    						architectures hosted on AArch64 systems may generate incorrect results when
                                    						used with <samp class="ph codeph">cudnnFusedOps_t</samp> set to
                                    							<samp class="ph codeph">CUDNN_FUSED_SCALE_BIAS_ACTIVATION_WGRAD</samp>.
                                 </li>
                                 <li class="li liexpand">The cuDNN 8.7.0 library may exhibit some slowdowns in <samp class="ph codeph">wgrad</samp>
                                    						calculation for EfficientDet, EfficientNet, Mask R-CNN, ResNet, ResNeXt, and
                                    						SSD layers when it was built with CUDA Toolkit 10.2.
                                 </li>
                                 <li class="li liexpand">With CUDA 11.7, the NVRTC library may cause a small memory leak when using
                                    						the runtime fusion engine. The issue has been addressed in CUDA Toolkit 11.8
                                    						or later. 
                                 </li>
                                 <li class="li liexpand">A compiler bug in NVRTC in CUDA version 11.7 and earlier, was causing
                                    						incorrect outputs when computing logical operations on boolean input tensors
                                    						in the runtime fusion engine. A workaround has been integrated in this
                                    						release to avoid the most common issues. However, it is highly recommended
                                    						to update to at least CUDA version 11.7u1 for a fix. Specifically, known
                                    						failure cases are when pointwise operations of mode
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_NOT</samp>,
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_AND</samp> or
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_OR</samp> operates on boolean
                                    						tensors.
                                 </li>
                                 <li class="li liexpand">If cuDNN 8.4.1 or earlier statically links with
                                    							<samp class="ph codeph">libcudart.so</samp> from the CUDA Toolkit 11.7 or later, when
                                    						the LFL feature is activated, the results from
                                    							<samp class="ph codeph">cudnnFind*Algo</samp> will not be accurate.
                                 </li>
                                 <li class="li liexpand">For packed NCHW tensors using the FP16 datatype, cuDNN attempts to run an
                                    						optimized kernel if the values of N, C, H, and W are even. In cuDNN versions
                                    						before 8.4, it is possible that incorrect values are generated if odd values
                                    						for the strides of N or C are used.
                                 </li>
                                 <li class="li liexpand">The documentation for <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnReorderFilterAndBias" target="_blank" shape="rect"><samp class="ph codeph">cudnnReorderFilterAndBias()</samp></a> requires
                                    						corrections for clarity.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX
                                    						3090 compared to 2080 Ti. This includes EfficientNet with up to 6x
                                    						performance difference, UNet up to 1.6x performance difference and Tacotron
                                    						up to 1.6x performance difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    							<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there are known performance regressions up to 2x on
                                    						select configurations for AlexNet-like models on NVIDIA Turing GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior,
                                    						accumulator data types, and so on, have not been documented as of yet.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN 8.4.0 may observe a slowdown in the Single Shot Multibox
                                    						Detector (SSD) model. This will be fixed in a future release.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with filter size
                                    						1x1. The severity would be different depending on which version of the CUDA
                                    						Toolkit the user is using.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with high group
                                    						count. The issue is more severe on V100.
                                 </li>
                                 <li class="li liexpand">On Windows 10, the knob settings of
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_CUTLASS_ANALYTIC_16816_NHWC_ENGINE</samp>
                                    						and
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_IMPLICIT_PRECOMPUTED_GEMM_CUTLASS_16816_NHWC_ENGINE</samp>
                                    						are incorrect. Therefore, these two engines cannot be configured properly.
                                    						It will impact the performance of convolution cases that use them on a
                                    						Windows system.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-881__section_dph_zkv_jrb"><a name="rel-881__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and
                              				the NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-881__section_nbs_qgh_fsb"><a name="rel-881__section_nbs_qgh_fsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel-881__ul_obs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-881__ul_obs_qgh_fsb">
                                 <li class="li liexpand">In cuDNN 8.8.1 for CUDA 12.x, runtime fusion engines will only work with NVRTC from CUDA
                                    						Toolkit 12.0 and 12.1. It is not forward compatible with future CUDA 12.x
                                    						Toolkits.
                                 </li>
                                 <li class="li liexpand">Within the cuDNN version 8 backend API, the following engines are known not
                                    						to be thread-safe when executed simultaneously with multiple threads sharing
                                    						the same execution plan:
                                    
                                    
                                    <div class="tablenoborder"><a name="rel-881__table_t2h_lsp_r5b" shape="rect">
                                          <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="rel-881__table_t2h_lsp_r5b" class="table" frame="border" border="1" rules="all">
                                          <caption><span class="tablecap">Table 9. Engines That Are Not Thread-Safe</span></caption>
                                          <thead class="thead" align="left">
                                             <tr class="row">
                                                <th class="entry" valign="top" width="20%" id="d54e10674" rowspan="1" colspan="1"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e10678" rowspan="1" colspan="1"><samp class="ph codeph">convolution forward</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e10682" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward data</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e10686" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward filter</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e10690" rowspan="1" colspan="1"><samp class="ph codeph">cudnnConvolutionBiasActivationForward</samp></th>
                                             </tr>
                                          </thead>
                                          <tbody class="tbody">
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e10674" rowspan="1" colspan="1">A100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e10678" rowspan="1" colspan="1"><a name="rel-881__ul_c53_qsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-881__ul_c53_qsp_r5b">
                                                      <li class="li">36</li>
                                                      <li class="li">38</li>
                                                      <li class="li">45</li>
                                                      <li class="li">46</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e10682" rowspan="1" colspan="1"><a name="rel-881__ul_rh2_ssp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-881__ul_rh2_ssp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">19</li>
                                                      <li class="li">22</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">28</li>
                                                      <li class="li">40</li>
                                                      <li class="li">46</li>
                                                      <li class="li">51</li>
                                                      <li class="li">56</li>
                                                      <li class="li">57</li>
                                                      <li class="li">58</li>
                                                      <li class="li">59</li>
                                                      <li class="li">60</li>
                                                      <li class="li">65</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e10686" rowspan="1" colspan="1"><a name="rel-881__ul_mjw_vsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-881__ul_mjw_vsp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">21</li>
                                                      <li class="li">23</li>
                                                      <li class="li">33</li>
                                                      <li class="li">37</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                      <li class="li">49</li>
                                                      <li class="li">50</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e10690" rowspan="1" colspan="1"><a name="rel-881__ul_mfh_1d3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-881__ul_mfh_1d3_kvb">
                                                      <li class="li">4024</li>
                                                      <li class="li">4026</li>
                                                      <li class="li">4032</li>
                                                      <li class="li">4033</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e10674" rowspan="1" colspan="1">V100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e10678" rowspan="1" colspan="1"><a name="rel-881__ul_qrn_btp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-881__ul_qrn_btp_r5b">
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">12</li>
                                                      <li class="li">16</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">49</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e10682" rowspan="1" colspan="1"><a name="rel-881__ul_ffy_dtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-881__ul_ffy_dtp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">44</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e10686" rowspan="1" colspan="1"><a name="rel-881__ul_lg1_htp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-881__ul_lg1_htp_r5b">
                                                      <li class="li">6</li>
                                                      <li class="li">7</li>
                                                      <li class="li">21</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">52</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e10690" rowspan="1" colspan="1"><a name="rel-881__ul_w1l_cd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-881__ul_w1l_cd3_kvb">
                                                      <li class="li">4002</li>
                                                      <li class="li">4015</li>
                                                      <li class="li">4008</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4030</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e10674" rowspan="1" colspan="1">T4</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e10678" rowspan="1" colspan="1"><a name="rel-881__ul_q3j_jtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-881__ul_q3j_jtp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">14</li>
                                                      <li class="li">16</li>
                                                      <li class="li">18</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">50</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e10682" rowspan="1" colspan="1"><a name="rel-881__ul_ax4_ntp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-881__ul_ax4_ntp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">15</li>
                                                      <li class="li">18</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">37</li>
                                                      <li class="li">39</li>
                                                      <li class="li">44</li>
                                                      <li class="li">45</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e10686" rowspan="1" colspan="1"><a name="rel-881__ul_j3t_rtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-881__ul_j3t_rtp_r5b">
                                                      <li class="li">12</li>
                                                      <li class="li">21</li>
                                                      <li class="li">27</li>
                                                      <li class="li">28</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">53</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e10690" rowspan="1" colspan="1"><a name="rel-881__ul_kc5_jd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-881__ul_kc5_jd3_kvb">
                                                      <li class="li">4015</li>
                                                      <li class="li">4003</li>
                                                      <li class="li">4004</li>
                                                      <li class="li">4009</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4031</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                          </tbody>
                                       </table>
                                    </div>
                                 </li>
                                 <li class="li liexpand">The status returned by <samp class="ph codeph">cudnnBackendFinalize()</samp> or
                                    							<samp class="ph codeph">cudnnBackendExecute()</samp> on a
                                    							<samp class="ph codeph">CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR</samp> may change
                                    						depending on the version of the dynamic dependencies of cuDNN. As of this
                                    						writing, only cuBLAS is known to affect the return status of these function
                                    						calls.
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not
                                    						required to consider padding. Users of cuDNN can witness an unexpected lack
                                    						of problem support when forward convolution spatial dimensions are less than
                                    						the filter size and padding is nonzero but is sufficient to extend spatial
                                    						dimensions to or beyond filter dimensions. This is commonly observed with,
                                    						but not limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand">When performing batch normalization in cuDNN, the operation is allowed to
                                    						proceed if the output tensor strides are overlapping, however, there is no
                                    						guarantee of deterministic results.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 25</samp> for convolution
                                    						backwards data (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp>) does not support
                                    						tensors in which the product N*C*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX =</samp><samp class="ph codeph">1 for convolution
                                       							backwards data</samp> (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp>) does not support
                                    						tensors in which the product N*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. This issue has been present in all previous releases of
                                    						cuDNN and exercising the use case for the engine would show incorrect
                                    						results.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later. It also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples must be installed in a writable location. If not installed in a
                                    						writable location, the samples can crash.
                                 </li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit nondeterministic
                                       							behavior when the cuDNN library is built with CUDA Toolkit 10.2 or
                                       							higher. This is the result of a new buffer management and heuristics in
                                       							the cuBLAS library. As described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This happens
                                       							when two buffer sizes (16 KB and 4 MB) are used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting
                                       							for a buffer of that size to be released, a smaller buffer may be used
                                       							with a different GPU kernel. The kernel selection may affect numerical
                                       							results. The user can eliminate the nondeterministic behavior of cuDNN
                                       							RNN and multihead attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16
                                       							KB each in GPU memory while the second setting creates two buffers of 4
                                       							MB each. The default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors
                                    						in order to run efficiently. As always, cuDNN recommends users to align
                                    						tensors to 16 byte boundaries that will be sufficiently aligned for any
                                    						computational option in cuDNN. Doing otherwise may cause performance
                                    						regressions in cuDNN 8.0.x compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and
                                    						the output is in FP16 (half float), there are cases where the numerical
                                    						accuracy between the different algorithms might differ. cuDNN 8.0.x users
                                    						can target the backend API to query the numerical notes of the algorithms to
                                    						get the information programmatically. There are cases where algo0 and algo1
                                    						will have a reduced precision accumulation when users target the legacy API.
                                    						In all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and
                                    						backward filter, grouped convolution with groups larger than 1 and with odd
                                    						product of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D
                                    						convolution), <samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on
                                    						devices older than NVIDIA Volta. To prevent a potential illegal memory
                                    						access by an instruction that only has a 16-bit version in NVIDIA Volta and
                                    						later, pad at least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use
                                    						texture-based load structure for performance improvements particularly in
                                    						older hardware architectures. Users can opt out of using texture using the
                                    						environmental variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this
                                    						variable is removed. Texture loading is turned off by default. Users who
                                    						want to continue to use texture-based load, can adapt the new backend API,
                                    						and toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check
                                    						API (for example, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command
                                    						explicitly to resolve the undefined symbols from cuDNN static
                                    						libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA
                                    						Pascal and later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-881__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-881__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in
                                    						compatibility mode are tested with compute-sanitizer,
                                    							<samp class="ph codeph">cuGetProcAddress</samp> failures with error code 500 will
                                    						arise due to missing functions. This error can be ignored, or suppressed
                                    						with the <samp class="ph codeph">--report-api-errors no</samp> option, as this is due to
                                    						CUDA backward compatibility checking if a function is usable with the CUDA
                                    						toolkit combination. The functions are introduced in a later version of CUDA
                                    						but are not available on the current platform. The absence of these
                                    						functions is harmless and will not give rise to any functional issues.
                                 </li>
                                 <li class="li liexpand">Users of <samp class="ph codeph">cudnn_cnn_infer_static.a</samp> may need to update their
                                    						application linkage so that symbols absent in that library are subsequently
                                    						made available with <samp class="ph codeph">cudnn_ops_infer_static.a</samp>. On Linux,
                                    						this is specifying the ops library after <samp class="ph codeph">cnn</samp> on the linker
                                    						line. The same applies to <samp class="ph codeph">cudnn_cnn_train_static.a</samp> and
                                    							<samp class="ph codeph">cudnn_ops_train_static.a</samp>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-880"><a name="rel-880" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-880" name="rel-880" shape="rect">1.10.&nbsp;cuDNN Release 8.8.0</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">These are the NVIDIA cuDNN 8.8.0 Release Notes. These Release Notes include fixes
                           		from the previous cuDNN releases as well as the following additional changes.
                        </p>
                        <div class="section" id="rel-880__section_l2m_s4c_2jb"><a name="rel-880__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">These Release Notes are applicable to both cuDNN and NVIDIA JetPack™
                              				users of cuDNN unless appended specifically with <em class="ph i">(not applicable for Jetson
                                 					platforms)</em>.
                           </p>
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-880__section_qxx_tqq_kwb"><a name="rel-880__section_qxx_tqq_kwb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Announcements</h3>
                           <div class="p"><a name="rel-880__ul_obv_5qq_kwb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-880__ul_obv_5qq_kwb">
                                 <li class="li liexpand">cuDNN 8.7.0 was the last release supporting NVIDIA Kepler (SM 3.x) devices.
                                    						It has been removed in cuDNN 8.8.0.
                                 </li>
                                 <li class="li liexpand">cuDNN 8.8.0 changed the linking procedure of NVRTC in the static build.
                                    						cuDNN 8.8.0 requires NVRTC to be statically linked in the static build
                                    						rather than the previous dynamic linking. This will also remove support for
                                    						static linking of cuDNN with CUDA toolkits &lt; 11.5. Refer to the updated
                                    						instructions in the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#install-linux" target="_blank" shape="rect">NVIDIA cuDNN Installation Guide</a>.
                                    						There were no changes for the linking procedure in the dynamic build.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-880__section_g43_tbt_wvb"><a name="rel-880__section_g43_tbt_wvb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-880__ul_izn_tbt_wvb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-880__ul_izn_tbt_wvb">
                                 <li class="li liexpand">This release adds CUDA 12 support.</li>
                                 <li class="li liexpand">Added three precompiled engines for instance normalization: instance
                                    						normalization forward training engine, instance normalization forward
                                    						inference engine, and instance normalization backward engine.
                                 </li>
                                 <li class="li liexpand">Added three precompiled engines for layer normalization: layer normalization
                                    						forward training engine, layer normalization forward inference engine, and
                                    						layer normalization backward engine.
                                 </li>
                                 <li class="li liexpand">Added further fusion patterns possible in <samp class="ph codeph">Mha-Fprop</samp>
                                    						fusions, which target causal masking, relative positional embedding bias,
                                    						and cross attention giving cuDNN full support for the T5 model. For more
                                    						information, refer to the <samp class="ph codeph">Mha-Fprop</samp> fusion pattern in the
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#op-fusion" target="_blank" shape="rect">NVIDIA cuDNN Developer Guide</a>.
                                 </li>
                                 <li class="li liexpand">Added support for <samp class="ph codeph">Mha-Bprop</samp> fusions using the runtime
                                    						fusion engine, targeting MHA training. For more information, refer to the
                                    							<samp class="ph codeph">Mha-Bprop</samp> fusion pattern in the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#op-fusion" target="_blank" shape="rect">NVIDIA cuDNN Developer Guide</a>. The
                                    						cuDNN implementation provides a speedup of ~2x-3x in BERT and T5 patterns in
                                    						training over native unfused PyTorch. Samples can also be found in the <a class="xref" href="https://github.com/NVIDIA/cudnn-frontend" target="_blank" shape="rect">cuDNN frontend repository</a>.
                                 </li>
                                 <li class="li liexpand">Double precision is supported for the CTC loss in the cuDNN 8.x API by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnCTCLoss_v8" target="_blank" shape="rect"><samp class="ph codeph">cudnnCTCLoss_v8()</samp></a>, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSetCTCLossDescriptor_v8" target="_blank" shape="rect"><samp class="ph codeph">cudnnSetCTCLossDescriptor_v8()</samp></a> and <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetCTCLossWorkspaceSize_v8" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetCTCLossWorkspaceSize_v8()</samp></a>, and in
                                    						all other cuDNN API versions by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnCTCLoss_v8" target="_blank" shape="rect"><samp class="ph codeph">cudnnCTCLoss()</samp></a>,
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSetCTCLossDescriptor_v8" target="_blank" shape="rect"><samp class="ph codeph">cudnnSetCTCLossDescriptor()</samp></a> and <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetCTCLossWorkspaceSize_v8" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetCTCLossWorkspaceSize()</samp></a>.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_HEUR_MODE_A</samp> supports the following new operation
                                    						graphs: <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#convbnfprop" target="_blank" shape="rect"><samp class="ph codeph">ConvBNfprop</samp></a>,
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#convbnwgrad" target="_blank" shape="rect"><samp class="ph codeph">ConvBNwgrad</samp></a>, and
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#dgraddrelubnbwdweight" target="_blank" shape="rect"><samp class="ph codeph">DgradDreluBNBwdWeight</samp></a>.
                                 </li>
                                 <li class="li liexpand">The <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#dbnapply" target="_blank" shape="rect"><samp class="ph codeph">dBNapply</samp></a> and
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#dualdbnapply" target="_blank" shape="rect"><samp class="ph codeph">DualdBNapply</samp></a>
                                    						patterns are now generally supported by the runtime fusion engine.
                                    						Heuristics query for these two patterns are also now supported. This
                                    						provides more flexible support in data types and also provides a 5% speed up
                                    						compared to the original pre-compiled specialized engines for these patterns
                                    						across a wide range of workloads.
                                 </li>
                                 <li class="li liexpand">Improved performance for 1D NHWC depthwise convolution.</li>
                                 <li class="li liexpand">Improved performance for Tensor Core accelerated FP32 operations on NVIDIA
                                    						Hopper.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-880__section_avj_jd2_3wb"><a name="rel-880__section_avj_jd2_3wb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p"><a name="rel-880__ul_pxv_jd2_3wb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-880__ul_pxv_jd2_3wb">
                                 <li class="li liexpand">Use of <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 0</samp> for convolution,
                                    						backward data, and backward filter batch normalization fusions resulted in a
                                    						performance regression in cuDNN v8.7 on NVIDIA Ampere architecture. This has
                                    						been improved upon in this release.
                                 </li>
                                 <li class="li liexpand">For NCHW spatially packed input tensors,
                                    							<samp class="ph codeph">cudnnBatchNormalizationForwardInference()</samp> with mode
                                    							<samp class="ph codeph">CUDNN_BATCHNORM_SPATIAL</samp> or
                                    							<samp class="ph codeph">CUDNN_BATCHNORM_SPATIAL_PERSISTENT</samp> can now support up
                                    						to size 2147483136 (2^31-512) in the spatial dimension (D)HW. For NCHW
                                    						spatially packed input tensors,
                                    							<samp class="ph codeph">cudnnBatchNormalizationBackward()</samp> with mode
                                    							<samp class="ph codeph">CUDNN_BATCHNORM_SPATIAL</samp> or
                                    							<samp class="ph codeph">CUDNN_BATCHNORM_SPATIAL_PERSISTENT</samp> can now support up
                                    						to size 2147483136 (2^31-512) in the spatial dimension (D)HW for some
                                    						cases.
                                 </li>
                                 <li class="li liexpand">Use of <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 0</samp> for batch
                                    						normalization forwards training and batch normalization backwards could
                                    						generate illegal memory access errors for large tensors. This is fixed in
                                    						this release by limiting the input tensor size to be less than or equal to
                                    						2^31.
                                 </li>
                                 <li class="li liexpand">Documentation was updated for the API function
                                    							<samp class="ph codeph">cudnnReorderFilterAndBias()</samp> to list other possible
                                    						return statuses and what would cause them to be returned.
                                 </li>
                                 <li class="li liexpand">The cuDNN static builds no longer dynamically load NVRTC, and therefore
                                    						requires static linking of NVRTC instead.
                                 </li>
                                 <li class="li liexpand">Fixed incorrect RNN results on Pascal family GPUs with 6.0 or 6.1 compute
                                    						capability, also in the
                                    							<samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_STATIC_SMALL_H</samp> algorithm, in the
                                    						RNN forward and backward APIs, when the input data minibatch was divisible
                                    						by eight, and for certain hidden size values.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-880__section_t2m_s5p_nnb"><a name="rel-880__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel-880__ul_mbs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-880__ul_mbs_qgh_fsb">
                                 <li class="li liexpand">Some engines will incorrectly return a success status when calling
                                    							<samp class="ph codeph">cudnnBackendFinalize()</samp> with a backend engine descriptor
                                    						for a bfloat16 problem on hardware that does not support the bfloat16 type.
                                    						Attempted execution of the problem with
                                    							<samp class="ph codeph">cudnnBackendExecute()</samp> will return the expected status
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp>.
                                 </li>
                                 <li class="li liexpand">Use of <samp class="ph codeph">cudnnFusedOpsExecute()</samp> on NVIDIA Volta compatible
                                    						architectures hosted on AArch64 systems may generate incorrect results when
                                    						used with <samp class="ph codeph">cudnnFusedOps_t</samp> set to
                                    							<samp class="ph codeph">CUDNN_FUSED_SCALE_BIAS_ACTIVATION_WGRAD</samp>.
                                 </li>
                                 <li class="li liexpand">The cuDNN 8.7.0 library may exhibit some slowdowns in <samp class="ph codeph">wgrad</samp>
                                    						calculation for EfficientDet, EfficientNet, Mask R-CNN, ResNet, ResNeXt, and
                                    						SSD layers when it was built with CUDA Toolkit 10.2.
                                 </li>
                                 <li class="li liexpand">With CUDA 11.7, the NVRTC library may cause a small memory leak when using
                                    						the runtime fusion engine. The issue has been addressed in CUDA Toolkit 11.8
                                    						or later. 
                                 </li>
                                 <li class="li liexpand">A compiler bug in NVRTC in CUDA version 11.7 and earlier, was causing
                                    						incorrect outputs when computing logical operations on boolean input tensors
                                    						in the runtime fusion engine. A workaround has been integrated in this
                                    						release to avoid the most common issues. However, it is highly recommended
                                    						to update to at least CUDA version 11.7u1 for a fix. Specifically, known
                                    						failure cases are when pointwise operations of mode
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_NOT</samp>,
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_AND</samp> or
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_OR</samp> operates on boolean
                                    						tensors.
                                 </li>
                                 <li class="li liexpand">If cuDNN 8.4.1 or earlier statically links with
                                    							<samp class="ph codeph">libcudart.so</samp> from the CUDA Toolkit 11.7 or later, when
                                    						the LFL feature is activated, the results from
                                    							<samp class="ph codeph">cudnnFind*Algo</samp> will not be accurate.
                                 </li>
                                 <li class="li liexpand">For packed NCHW tensors using the FP16 datatype, cuDNN attempts to run an
                                    						optimized kernel if the values of N, C, H, and W are even. In cuDNN versions
                                    						before 8.4, it is possible that incorrect values are generated if odd values
                                    						for the strides of N or C are used.
                                 </li>
                                 <li class="li liexpand">The documentation for <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnReorderFilterAndBias" target="_blank" shape="rect"><samp class="ph codeph">cudnnReorderFilterAndBias()</samp></a> requires
                                    						corrections for clarity.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX
                                    						3090 compared to 2080 Ti. This includes EfficientNet with up to 6x
                                    						performance difference, UNet up to 1.6x performance difference and Tacotron
                                    						up to 1.6x performance difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    							<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there are known performance regressions up to 2x on
                                    						select configurations for AlexNet-like models on NVIDIA Turing GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior,
                                    						accumulator data types, and so on, have not been documented as of yet.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN 8.4.0 may observe a slowdown in the Single Shot Multibox
                                    						Detector (SSD) model. This will be fixed in a future release.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with filter size
                                    						1x1. The severity would be different depending on which version of the CUDA
                                    						Toolkit the user is using.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with high group
                                    						count. The issue is more severe on V100.
                                 </li>
                                 <li class="li liexpand">On Windows 10, the knob settings of
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_CUTLASS_ANALYTIC_16816_NHWC_ENGINE</samp>
                                    						and
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_IMPLICIT_PRECOMPUTED_GEMM_CUTLASS_16816_NHWC_ENGINE</samp>
                                    						are incorrect. Therefore, these two engines cannot be configured properly.
                                    						It will impact the performance of convolution cases that use them on a
                                    						Windows system.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-880__section_dph_zkv_jrb"><a name="rel-880__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and
                              				the NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-880__section_nbs_qgh_fsb"><a name="rel-880__section_nbs_qgh_fsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel-880__ul_obs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-880__ul_obs_qgh_fsb">
                                 <li class="li liexpand">cuDNN 8.8.0 runtime fusion will only work with NVRTC from CUDA ToolKit 12.0.
                                    						It is not forward compatible with CUDA Toolkit 12.1 and later.
                                 </li>
                                 <li class="li liexpand">Within the cuDNN version 8 backend API, the following engines are known not
                                    						to be thread-safe when executed simultaneously with multiple threads sharing
                                    						the same execution plan:
                                    
                                    
                                    <div class="tablenoborder"><a name="rel-880__table_t2h_lsp_r5b" shape="rect">
                                          <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="rel-880__table_t2h_lsp_r5b" class="table" frame="border" border="1" rules="all">
                                          <caption><span class="tablecap">Table 10. Engines That Are Not Thread-Safe</span></caption>
                                          <thead class="thead" align="left">
                                             <tr class="row">
                                                <th class="entry" valign="top" width="20%" id="d54e11891" rowspan="1" colspan="1"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e11895" rowspan="1" colspan="1"><samp class="ph codeph">convolution forward</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e11899" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward data</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e11903" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward filter</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e11907" rowspan="1" colspan="1"><samp class="ph codeph">cudnnConvolutionBiasActivationForward</samp></th>
                                             </tr>
                                          </thead>
                                          <tbody class="tbody">
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e11891" rowspan="1" colspan="1">A100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e11895" rowspan="1" colspan="1"><a name="rel-880__ul_c53_qsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-880__ul_c53_qsp_r5b">
                                                      <li class="li">36</li>
                                                      <li class="li">38</li>
                                                      <li class="li">45</li>
                                                      <li class="li">46</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e11899" rowspan="1" colspan="1"><a name="rel-880__ul_rh2_ssp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-880__ul_rh2_ssp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">19</li>
                                                      <li class="li">22</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">28</li>
                                                      <li class="li">40</li>
                                                      <li class="li">46</li>
                                                      <li class="li">51</li>
                                                      <li class="li">56</li>
                                                      <li class="li">57</li>
                                                      <li class="li">58</li>
                                                      <li class="li">59</li>
                                                      <li class="li">60</li>
                                                      <li class="li">65</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e11903" rowspan="1" colspan="1"><a name="rel-880__ul_mjw_vsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-880__ul_mjw_vsp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">21</li>
                                                      <li class="li">23</li>
                                                      <li class="li">33</li>
                                                      <li class="li">37</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                      <li class="li">49</li>
                                                      <li class="li">50</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e11907" rowspan="1" colspan="1"><a name="rel-880__ul_mfh_1d3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-880__ul_mfh_1d3_kvb">
                                                      <li class="li">4024</li>
                                                      <li class="li">4026</li>
                                                      <li class="li">4032</li>
                                                      <li class="li">4033</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e11891" rowspan="1" colspan="1">V100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e11895" rowspan="1" colspan="1"><a name="rel-880__ul_qrn_btp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-880__ul_qrn_btp_r5b">
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">12</li>
                                                      <li class="li">16</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">49</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e11899" rowspan="1" colspan="1"><a name="rel-880__ul_ffy_dtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-880__ul_ffy_dtp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">44</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e11903" rowspan="1" colspan="1"><a name="rel-880__ul_lg1_htp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-880__ul_lg1_htp_r5b">
                                                      <li class="li">6</li>
                                                      <li class="li">7</li>
                                                      <li class="li">21</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">52</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e11907" rowspan="1" colspan="1"><a name="rel-880__ul_w1l_cd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-880__ul_w1l_cd3_kvb">
                                                      <li class="li">4002</li>
                                                      <li class="li">4015</li>
                                                      <li class="li">4008</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4030</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e11891" rowspan="1" colspan="1">T4</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e11895" rowspan="1" colspan="1"><a name="rel-880__ul_q3j_jtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-880__ul_q3j_jtp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">14</li>
                                                      <li class="li">16</li>
                                                      <li class="li">18</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">50</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e11899" rowspan="1" colspan="1"><a name="rel-880__ul_ax4_ntp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-880__ul_ax4_ntp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">15</li>
                                                      <li class="li">18</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">37</li>
                                                      <li class="li">39</li>
                                                      <li class="li">44</li>
                                                      <li class="li">45</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e11903" rowspan="1" colspan="1"><a name="rel-880__ul_j3t_rtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-880__ul_j3t_rtp_r5b">
                                                      <li class="li">12</li>
                                                      <li class="li">21</li>
                                                      <li class="li">27</li>
                                                      <li class="li">28</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">53</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e11907" rowspan="1" colspan="1"><a name="rel-880__ul_kc5_jd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-880__ul_kc5_jd3_kvb">
                                                      <li class="li">4015</li>
                                                      <li class="li">4003</li>
                                                      <li class="li">4004</li>
                                                      <li class="li">4009</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4031</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                          </tbody>
                                       </table>
                                    </div>
                                 </li>
                                 <li class="li liexpand">The status returned by <samp class="ph codeph">cudnnBackendFinalize()</samp> or
                                    							<samp class="ph codeph">cudnnBackendExecute()</samp> on a
                                    							<samp class="ph codeph">CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR</samp> may change
                                    						depending on the version of the dynamic dependencies of cuDNN. As of this
                                    						writing, only cuBLAS is known to affect the return status of these function
                                    						calls.
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not
                                    						required to consider padding. Users of cuDNN can witness an unexpected lack
                                    						of problem support when forward convolution spatial dimensions are less than
                                    						the filter size and padding is nonzero but is sufficient to extend spatial
                                    						dimensions to or beyond filter dimensions. This is commonly observed with,
                                    						but not limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand">When performing batch normalization in cuDNN, the operation is allowed to
                                    						proceed if the output tensor strides are overlapping, however, there is no
                                    						guarantee of deterministic results.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 25</samp> for convolution
                                    						backwards data (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp>) does not support
                                    						tensors in which the product N*C*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX =</samp><samp class="ph codeph">1 for convolution
                                       							backwards data</samp> (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp>) does not support
                                    						tensors in which the product N*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. This issue has been present in all previous releases of
                                    						cuDNN and exercising the use case for the engine would show incorrect
                                    						results.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later. It also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples must be installed in a writable location. If not installed in a
                                    						writable location, the samples can crash.
                                 </li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit nondeterministic
                                       							behavior when the cuDNN library is built with CUDA Toolkit 10.2 or
                                       							higher. This is the result of a new buffer management and heuristics in
                                       							the cuBLAS library. As described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This happens
                                       							when two buffer sizes (16 KB and 4 MB) are used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting
                                       							for a buffer of that size to be released, a smaller buffer may be used
                                       							with a different GPU kernel. The kernel selection may affect numerical
                                       							results. The user can eliminate the nondeterministic behavior of cuDNN
                                       							RNN and multihead attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16
                                       							KB each in GPU memory while the second setting creates two buffers of 4
                                       							MB each. The default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors
                                    						in order to run efficiently. As always, cuDNN recommends users to align
                                    						tensors to 16 byte boundaries that will be sufficiently aligned for any
                                    						computational option in cuDNN. Doing otherwise may cause performance
                                    						regressions in cuDNN 8.0.x compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and
                                    						the output is in FP16 (half float), there are cases where the numerical
                                    						accuracy between the different algorithms might differ. cuDNN 8.0.x users
                                    						can target the backend API to query the numerical notes of the algorithms to
                                    						get the information programmatically. There are cases where algo0 and algo1
                                    						will have a reduced precision accumulation when users target the legacy API.
                                    						In all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and
                                    						backward filter, grouped convolution with groups larger than 1 and with odd
                                    						product of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D
                                    						convolution), <samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on
                                    						devices older than NVIDIA Volta. To prevent a potential illegal memory
                                    						access by an instruction that only has a 16-bit version in NVIDIA Volta and
                                    						later, pad at least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use
                                    						texture-based load structure for performance improvements particularly in
                                    						older hardware architectures. Users can opt out of using texture using the
                                    						environmental variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this
                                    						variable is removed. Texture loading is turned off by default. Users who
                                    						want to continue to use texture-based load, can adapt the new backend API,
                                    						and toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check
                                    						API (for example, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command
                                    						explicitly to resolve the undefined symbols from cuDNN static
                                    						libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA
                                    						Pascal and later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-880__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-880__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in
                                    						compatibility mode are tested with compute-sanitizer,
                                    							<samp class="ph codeph">cuGetProcAddress</samp> failures with error code 500 will
                                    						arise due to missing functions. This error can be ignored, or suppressed
                                    						with the <samp class="ph codeph">--report-api-errors no</samp> option, as this is due to
                                    						CUDA backward compatibility checking if a function is usable with the CUDA
                                    						toolkit combination. The functions are introduced in a later version of CUDA
                                    						but are not available on the current platform. The absence of these
                                    						functions is harmless and will not give rise to any functional issues.
                                 </li>
                                 <li class="li liexpand">Users of <samp class="ph codeph">cudnn_cnn_infer_static.a</samp> may need to update their
                                    						application linkage so that symbols absent in that library are subsequently
                                    						made available with <samp class="ph codeph">cudnn_ops_infer_static.a</samp>. On Linux,
                                    						this is specifying the ops library after <samp class="ph codeph">cnn</samp> on the linker
                                    						line. The same applies to <samp class="ph codeph">cudnn_cnn_train_static.a</samp> and
                                    							<samp class="ph codeph">cudnn_ops_train_static.a</samp>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-880__section_ggy_jxs_wvb"><a name="rel-880__section_ggy_jxs_wvb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Deprecated and Removed Features</h3>
                           <div class="p">The following features are deprecated in cuDNN 8.8.0:<a name="rel-880__ul_kg2_lxs_wvb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-880__ul_kg2_lxs_wvb">
                                 <li class="li">cuDNN 8.7.0 was the last release supporting NVIDIA Kepler (SM 3.x) devices.
                                    						It has been removed in cuDNN 8.8.0.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-870"><a name="rel-870" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-870" name="rel-870" shape="rect">1.11.&nbsp;cuDNN Release 8.7.0</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">These are the NVIDIA cuDNN 8.7.0 Release Notes. These Release Notes include fixes
                           		from the previous cuDNN releases as well as the following additional changes.
                        </p>
                        <div class="section" id="rel-870__section_l2m_s4c_2jb"><a name="rel-870__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">These Release Notes are applicable to both cuDNN and NVIDIA JetPack™
                              				users of cuDNN unless appended specifically with <em class="ph i">(not applicable for Jetson
                                 					platforms)</em>.
                           </p>
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-870__section_kg5_zsh_jvb"><a name="rel-870__section_kg5_zsh_jvb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p"><a name="rel-870__ul_ysf_1th_jvb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-870__ul_ysf_1th_jvb">
                                 <li class="li liexpand">Added the <samp class="ph codeph">cudnnRngDistribution_t</samp>,
                                    							<samp class="ph codeph">CUDNN_BACKEND_OPERATION_RNG_DESCRIPTOR</samp> and
                                    							<samp class="ph codeph">CUDNN_BACKEND_RNG_DESCRIPTOR</samp> functions to the Backend
                                    						API. This new operation helps a cuDNN graph to create a tensor using a
                                    						probability distribution which can then be used as an input to other
                                    						operations. For example, it can be used as a mask in
                                    							<samp class="ph codeph">dropout</samp>. Currently, it has limited support via the
                                    						runtime fusion engine in particular patterns. For more information refer to
                                    						the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#op-fusion" target="_blank" shape="rect">NVIDIA cuDNN Developer Guide</a>.
                                    						Support will be extended in future versions. For more information, refer to
                                    						the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnn-backend-api" target="_blank" shape="rect">NVIDIA cuDNN Backend API</a>
                                    						documentation.
                                 </li>
                                 <li class="li liexpand">Added support for MatMul-MatMul fusions via the runtime fusion engine, targeting MHA
                                    						inference. For more information, refer to the MatMul-MatMul fusion pattern
                                    						in the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#op-fusion" target="_blank" shape="rect">NVIDIA cuDNN Developer Guide</a>. The
                                    						cuDNN implementation provides a speedup of ~4x-4.5x in BERT and T5 patterns
                                    						in inference over native unfused PyTorch.
                                 </li>
                                 <li class="li liexpand">Added native NVIDIA Hopper support for matrix multiplication and its fusions in FP16 mixed
                                    						precision (FP16 I/O with FP32 compute), which improves performance of matmul
                                    						ops on Hopper compared to cuDNN 8.6.0.
                                 </li>
                                 <li class="li liexpand">Added FP8 input support for convolution backward data and backward weights operations, with
                                    						two possible compute precision types <samp class="ph codeph">CUDNN_DATA_FLOAT</samp> and
                                    							<samp class="ph codeph">CUDNN_DATA_FAST_FLOAT_FOR_FP8</samp> (faster but lower
                                    						precision).
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingBackward()</samp></a> enables both
                                    							<samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp> data pointers (together with
                                    						the related tensor descriptor handles) to be <samp class="ph codeph">NULL</samp> for
                                    						avg-pooling. This could save memory footprint and bandwidth.
                                 </li>
                                 <li class="li liexpand">Added support for <samp class="ph codeph">ConvBNfprop</samp> and <samp class="ph codeph">ConvBNwgrad</samp> fusion
                                    						patterns on NVIDIA Hopper GPU’s. For more information, refer to the
                                    							<samp class="ph codeph">ConvBNfprop</samp> and <samp class="ph codeph">ConvBNwgrad</samp> patterns
                                    						listed in the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#convbnfprop" target="_blank" shape="rect">NVIDIA cuDNN Developer Guide</a>.
                                 </li>
                                 <li class="li liexpand">Additional tensor layout support was added for the forward and backwards
                                    						resampling modes that were added in cuDNN 8.6.0.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-870__section_izd_kzg_hvb"><a name="rel-870__section_izd_kzg_hvb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p"><a name="rel-870__ul_ufn_kzg_hvb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-870__ul_ufn_kzg_hvb">
                                 <li class="li liexpand">The backend engine 6000 was not respecting the <samp class="ph codeph">CUDNN_KNOB_TYPE_TILE_SIZE</samp>
                                    						knob that was passed by the user and it had runtime failures on Windows.
                                    						Both of these issues have been fixed in this release.
                                 </li>
                                 <li class="li liexpand">In CUDA graph capture mode, CUDA streams internal to cuDNN were not
                                    						guaranteed to have the same priority as the user stream that is set by
                                    							<samp class="ph codeph">cudnnSetStream()</samp>. This issue is fixed in cuDNN 8.7.0,
                                    						but requires CUDA 11.8 or later.
                                 </li>
                                 <li class="li liexpand">On Turing, Volta, Kepler, and Maxwell GPUs, 3D convolutions used to exhibit some slowdowns
                                    						when the padding size was larger than the filter size. 2D convolutions used
                                    						to encounter an illegal memory access error when the padding size was larger
                                    						than the filter size and the horizontal stride was larger than 1. These
                                    						issues have been fixed in this release.
                                 </li>
                                 <li class="li liexpand">The performance of the runtime fusion engine was suboptimal on Windows. This
                                    						has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN's <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING</samp>
                                    						could see <samp class="ph codeph">CUDNN_STATUS_BAD_PARAM</samp> returned for a problem
                                    						that should otherwise be supported by that choice of algo. This has been
                                    						fixed in this release.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnDropoutForward()</samp> and
                                    							<samp class="ph codeph">cudnnDropoutBackward()</samp> would return incorrect results
                                    						when input and/or output tensors have overlapping strides. This issue has
                                    						been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Users of <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 0</samp> for batch
                                    						normalization forwards training and batch normalization backwards could
                                    						obtain incorrect results when the batch size was greater than 1 and when
                                    						channel count was not evenly divisible by 8. These values of
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp> correspond to newly
                                    						added multi-GPU batch normalization support within cuDNN 8.5. Use of
                                    						single-GPU batch normalization was unaffected by this issue. This limitation
                                    						has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">In cuDNN 8.5 built with CUDA 11.x, all RNN APIs started to use internal CUDA
                                    						streams of the same priority as the user stream passed through the
                                    							<samp class="ph codeph">cudnnSetStream()</samp> function. This update introduced a
                                    						bug. When the internal heuristic decided to transpose RNN weights in
                                    							<samp class="ph codeph">cudnnRNNForward()</samp>or corresponding, deprecated
                                    						functions: <samp class="ph codeph">cudnnRNNForwardInference()</samp>,
                                    							<samp class="ph codeph">cudnnRNNForwardTraining()</samp>,
                                    							<samp class="ph codeph">cudnnRNNForwardInferenceEx()</samp>,
                                    							<samp class="ph codeph">cudnnRNNForwardTrainingEx()</samp>, and
                                    							<samp class="ph codeph">cudnnRNNAlgo_t</samp> was set to
                                    							<samp class="ph codeph">CUDNN_RNN_ALGO_STANDARD</samp>, the matrix transposition was
                                    						performed in the synchronous <samp class="ph codeph">stream 0</samp> instead of a CUDA
                                    						stream of the same priority as user stream. This caused serial execution of
                                    						transpose kernels and excessive synchronization in the initial stage of the
                                    						forward API. The bug affected cuDNN 8.5 and 8.6 built with both CUDA 11.x
                                    						and CUDA 10.2. This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a> did not
                                    						support BFLOAT16. If an input tensor used BFLOAT16, the API would return
                                    							<samp class="ph codeph">BAD_PARAM</samp>. This issue has been fixed in this
                                    						release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-870__section_t2m_s5p_nnb"><a name="rel-870__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel-870__ul_mbs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-870__ul_mbs_qgh_fsb">
                                 <li class="li liexpand">Use of <samp class="ph codeph">cudnnFusedOpsExecute()</samp> on Volta compatible
                                    						architectures hosted on AArch64 systems may generate incorrect results when
                                    						used with <samp class="ph codeph">cudnnFusedOps_t</samp> set to
                                    							<samp class="ph codeph">CUDNN_FUSED_SCALE_BIAS_ACTIVATION_WGRAD</samp>.
                                 </li>
                                 <li class="li liexpand">The cuDNN 8.7.0 library may exhibit some slowdowns in <samp class="ph codeph">wgrad</samp> calculation
                                    						for EfficientDet, EfficientNet, Mask R-CNN, ResNet, ResNeXt, and SSD layers
                                    						when it was built with CUDA Toolkit 10.2.
                                 </li>
                                 <li class="li liexpand">With CUDA 11.7, the NVRTC library may cause a small memory leak when using the runtime
                                    						fusion engine. The issue has been addressed in CUDA Toolkit 11.8 or later. 
                                 </li>
                                 <li class="li liexpand">A compiler bug in NVRTC in CUDA version 11.7 and earlier, was causing
                                    						incorrect outputs when computing logical operations on boolean input tensors
                                    						in the runtime fusion engine. A workaround has been integrated in this
                                    						release to avoid the most common issues. However, it is highly recommended
                                    						to update to at least CUDA version 11.7u1 for a fix. Specifically, known
                                    						failure cases are when pointwise operations of mode
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_NOT</samp>,
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_AND</samp> or
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_OR</samp> operates on boolean
                                    						tensors.
                                 </li>
                                 <li class="li liexpand">If cuDNN 8.4.1 or earlier statically links with
                                    							<samp class="ph codeph">libcudart.so</samp> from the CUDA Toolkit 11.7 or later, when
                                    						the LFL feature is activated, the results from
                                    							<samp class="ph codeph">cudnnFind*Algo</samp> will not be accurate.
                                 </li>
                                 <li class="li liexpand">For packed NCHW tensors using the FP16 datatype, cuDNN attempts to run an
                                    						optimized kernel if the values of N, C, H, and W are even. In cuDNN versions
                                    						before 8.4, it is possible that incorrect values are generated if odd values
                                    						for the strides of N or C are used.
                                 </li>
                                 <li class="li liexpand">The documentation for <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnReorderFilterAndBias" target="_blank" shape="rect"><samp class="ph codeph">cudnnReorderFilterAndBias()</samp></a> requires
                                    						corrections for clarity.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX
                                    						3090 compared to 2080 Ti. This includes EfficientNet with up to 6x
                                    						performance difference, UNet up to 1.6x performance difference and Tacotron
                                    						up to 1.6x performance difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    							<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there are known performance regressions up to 2x on
                                    						select configurations for AlexNet-like models on NVIDIA Turing GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior,
                                    						accumulator data types, and so on, have not been documented as of yet.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN 8.4.0 may observe a slowdown in the Single Shot Multibox
                                    						Detector (SSD) model. This will be fixed in a future release.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with filter size
                                    						1x1. The severity would be different depending on which version of the CUDA
                                    						Toolkit the user is using.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with high group
                                    						count. The issue is more severe on V100.
                                 </li>
                                 <li class="li liexpand">On Windows 10, the knob settings of
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_CUTLASS_ANALYTIC_16816_NHWC_ENGINE</samp>
                                    						and
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_IMPLICIT_PRECOMPUTED_GEMM_CUTLASS_16816_NHWC_ENGINE</samp>
                                    						are incorrect. Therefore, these two engines cannot be configured properly.
                                    						It will impact the performance of convolution cases that use them on a
                                    						Windows system.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-870__section_dph_zkv_jrb"><a name="rel-870__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and
                              				the NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-870__section_nbs_qgh_fsb"><a name="rel-870__section_nbs_qgh_fsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel-870__ul_obs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-870__ul_obs_qgh_fsb">
                                 <li class="li liexpand">Within the cuDNN version 8 backend API, the following engines are known to
                                    						not be thread-safe when executed simultaneously with multiple threads
                                    						sharing the same execution plan:
                                    
                                    
                                    <div class="tablenoborder"><a name="rel-870__table_t2h_lsp_r5b" shape="rect">
                                          <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="rel-870__table_t2h_lsp_r5b" class="table" frame="border" border="1" rules="all">
                                          <caption><span class="tablecap">Table 11. Engines That Are Not Thread-Safe</span></caption>
                                          <thead class="thead" align="left">
                                             <tr class="row">
                                                <th class="entry" valign="top" width="20%" id="d54e13100" rowspan="1" colspan="1"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e13104" rowspan="1" colspan="1"><samp class="ph codeph">convolution forward</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e13108" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward data</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e13112" rowspan="1" colspan="1"><samp class="ph codeph">convolution backward filter</samp></th>
                                                <th class="entry" valign="top" width="20%" id="d54e13116" rowspan="1" colspan="1"><samp class="ph codeph">cudnnConvolutionBiasActivationForward</samp></th>
                                             </tr>
                                          </thead>
                                          <tbody class="tbody">
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e13100" rowspan="1" colspan="1">A100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e13104" rowspan="1" colspan="1"><a name="rel-870__ul_c53_qsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-870__ul_c53_qsp_r5b">
                                                      <li class="li">36</li>
                                                      <li class="li">38</li>
                                                      <li class="li">45</li>
                                                      <li class="li">46</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e13108" rowspan="1" colspan="1"><a name="rel-870__ul_rh2_ssp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-870__ul_rh2_ssp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">19</li>
                                                      <li class="li">22</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">28</li>
                                                      <li class="li">40</li>
                                                      <li class="li">46</li>
                                                      <li class="li">51</li>
                                                      <li class="li">56</li>
                                                      <li class="li">57</li>
                                                      <li class="li">58</li>
                                                      <li class="li">59</li>
                                                      <li class="li">60</li>
                                                      <li class="li">65</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e13112" rowspan="1" colspan="1"><a name="rel-870__ul_mjw_vsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-870__ul_mjw_vsp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">21</li>
                                                      <li class="li">23</li>
                                                      <li class="li">33</li>
                                                      <li class="li">37</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                      <li class="li">49</li>
                                                      <li class="li">50</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e13116" rowspan="1" colspan="1"><a name="rel-870__ul_mfh_1d3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-870__ul_mfh_1d3_kvb">
                                                      <li class="li">4024</li>
                                                      <li class="li">4026</li>
                                                      <li class="li">4032</li>
                                                      <li class="li">4033</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e13100" rowspan="1" colspan="1">V100</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e13104" rowspan="1" colspan="1"><a name="rel-870__ul_qrn_btp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-870__ul_qrn_btp_r5b">
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">12</li>
                                                      <li class="li">16</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">49</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e13108" rowspan="1" colspan="1"><a name="rel-870__ul_ffy_dtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-870__ul_ffy_dtp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">44</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e13112" rowspan="1" colspan="1"><a name="rel-870__ul_lg1_htp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-870__ul_lg1_htp_r5b">
                                                      <li class="li">6</li>
                                                      <li class="li">7</li>
                                                      <li class="li">21</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">52</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e13116" rowspan="1" colspan="1"><a name="rel-870__ul_w1l_cd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-870__ul_w1l_cd3_kvb">
                                                      <li class="li">4002</li>
                                                      <li class="li">4015</li>
                                                      <li class="li">4008</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4030</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="20%" headers="d54e13100" rowspan="1" colspan="1">T4</td>
                                                <td class="entry" valign="top" width="20%" headers="d54e13104" rowspan="1" colspan="1"><a name="rel-870__ul_q3j_jtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-870__ul_q3j_jtp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">14</li>
                                                      <li class="li">16</li>
                                                      <li class="li">18</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">50</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e13108" rowspan="1" colspan="1"><a name="rel-870__ul_ax4_ntp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-870__ul_ax4_ntp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">15</li>
                                                      <li class="li">18</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">37</li>
                                                      <li class="li">39</li>
                                                      <li class="li">44</li>
                                                      <li class="li">45</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e13112" rowspan="1" colspan="1"><a name="rel-870__ul_j3t_rtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-870__ul_j3t_rtp_r5b">
                                                      <li class="li">12</li>
                                                      <li class="li">21</li>
                                                      <li class="li">27</li>
                                                      <li class="li">28</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">53</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="20%" headers="d54e13116" rowspan="1" colspan="1"><a name="rel-870__ul_kc5_jd3_kvb" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-870__ul_kc5_jd3_kvb">
                                                      <li class="li">4015</li>
                                                      <li class="li">4003</li>
                                                      <li class="li">4004</li>
                                                      <li class="li">4009</li>
                                                      <li class="li">4018</li>
                                                      <li class="li">4019</li>
                                                      <li class="li">4020</li>
                                                      <li class="li">4031</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                          </tbody>
                                       </table>
                                    </div>
                                 </li>
                                 <li class="li liexpand">The status returned by <samp class="ph codeph">cudnnBackendFinalize()</samp> or
                                    							<samp class="ph codeph">cudnnBackendExecute()</samp> on a
                                    							<samp class="ph codeph">CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR</samp> may change
                                    						depending on the version of the dynamic dependencies of cuDNN. As of this
                                    						writing, only cuBLAS is known to affect the return status of these function
                                    						calls.
                                 </li>
                                 <li class="li liexpand">The cuDNN static builds load NVRTC dynamically when using the runtime fusion
                                    						engine.
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not
                                    						required to consider padding. Users of cuDNN can witness an unexpected lack
                                    						of problem support when forward convolution spatial dimensions are less than
                                    						the filter size and padding is nonzero but is sufficient to extend spatial
                                    						dimensions to or beyond filter dimensions. This is commonly observed with,
                                    						but not limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand">When performing batch normalization in cuDNN, the operation is allowed to
                                    						proceed if the output tensor strides are overlapping, however, there is no
                                    						guarantee of deterministic results.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 25</samp> for convolution
                                    						backwards data (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp>) does not support
                                    						tensors in which the product N*C*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX =</samp><samp class="ph codeph">1 for convolution
                                       							backwards data</samp> (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp>) does not support
                                    						tensors in which the product N*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. This issue has been present in all previous releases of
                                    						cuDNN and exercising the use case for the engine would show incorrect
                                    						results.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later. It also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples must be installed in a writable location. If not installed in a
                                    						writable location, the samples can crash.
                                 </li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit nondeterministic behavior when the cuDNN
                                       							library is built with CUDA Toolkit 10.2 or higher. This is the result of
                                       							a new buffer management and heuristics in the cuBLAS library. As
                                       							described in  <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect"><u class="ph u">Results
                                             							Reproducibility</u></a>, numerical results may not be deterministic
                                       							when cuBLAS APIs are launched in more than one CUDA stream using the
                                       							same cuBLAS handle. This happens when two buffer sizes (16 KB and 4 MB)
                                       							are used in the default configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting
                                       							for a buffer of that size to be released, a smaller buffer may be used
                                       							with a different GPU kernel. The kernel selection may affect numerical
                                       							results. The user can eliminate the nondeterministic behavior of cuDNN
                                       							RNN and multihead attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16
                                       							KB each in GPU memory while the second setting creates two buffers of 4
                                       							MB each. The default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors
                                    						in order to run efficiently. As always, cuDNN recommends users to align
                                    						tensors to 16 byte boundaries that will be sufficiently aligned for any
                                    						computational option in cuDNN. Doing otherwise may cause performance
                                    						regressions in cuDNN 8.0.x compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and
                                    						the output is in FP16 (half float), there are cases where the numerical
                                    						accuracy between the different algorithms might differ. cuDNN 8.0.x users
                                    						can target the backend API to query the numerical notes of the algorithms to
                                    						get the information programmatically. There are cases where algo0 and algo1
                                    						will have a reduced precision accumulation when users target the legacy API.
                                    						In all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and
                                    						backward filter, grouped convolution with groups larger than 1 and with odd
                                    						product of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D
                                    						convolution), <samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on
                                    						devices older than NVIDIA Volta. To prevent a potential illegal memory
                                    						access by an instruction that only has a 16-bit version in NVIDIA Volta and
                                    						later, pad at least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use
                                    						texture-based load structure for performance improvements particularly in
                                    						older hardware architectures. Users can opt out of using texture using the
                                    						environmental variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this
                                    						variable is removed. Texture loading is turned off by default. Users who
                                    						want to continue to use texture-based load, can adapt the new backend API,
                                    						and toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check API (for example,
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command
                                    						explicitly to resolve the undefined symbols from cuDNN static
                                    						libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA
                                    						Pascal and later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-870__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-870__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in
                                    						compatibility mode are tested with compute-sanitizer,
                                    							<samp class="ph codeph">cuGetProcAddress</samp> failures with error code 500 will
                                    						arise due to missing functions. This error can be ignored, or suppressed
                                    						with the <samp class="ph codeph">--report-api-errors no</samp> option, as this is due to
                                    						CUDA backward compatibility checking if a function is usable with the CUDA
                                    						toolkit combination. The functions are introduced in a later version of CUDA
                                    						but are not available on the current platform. The absence of these
                                    						functions is harmless and will not give rise to any functional issues.
                                 </li>
                                 <li class="li liexpand">Users of <samp class="ph codeph">cudnn_cnn_infer_static.a</samp> may need to update their
                                    						application linkage so that symbols absent in that library are subsequently
                                    						made available with <samp class="ph codeph">cudnn_ops_infer_static.a</samp>. On Linux,
                                    						this is specifying the ops library after <samp class="ph codeph">cnn</samp> on the linker
                                    						line. The same applies to <samp class="ph codeph">cudnn_cnn_train_static.a</samp> and
                                    							<samp class="ph codeph">cudnn_ops_train_static.a</samp>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-860"><a name="rel-860" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-860" name="rel-860" shape="rect">1.12.&nbsp;cuDNN Release 8.6.0</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">These are the NVIDIA cuDNN 8.6.0 Release Notes. These Release Notes include fixes
                           		from the previous cuDNN releases as well as the following additional changes.
                        </p>
                        <div class="section" id="rel-860__section_l2m_s4c_2jb"><a name="rel-860__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">These Release Notes are applicable to both cuDNN and NVIDIA JetPack™
                              				users of cuDNN unless appended specifically with <em class="ph i">(not applicable for Jetson
                                 					platforms)</em>.
                           </p>
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-860__section_lj2_cbq_t5b"><a name="rel-860__section_lj2_cbq_t5b" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-860__ul_jhj_cbq_t5b" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-860__ul_jhj_cbq_t5b">
                                 <li class="li liexpand">Added support for the NVIDIA Hopper™ (H100) architecture.</li>
                                 <li class="li liexpand">Added support for FP8 on H100 using the runtime fusion engine. Support is currently limited
                                    						to convolution forward. We will release code examples in the <a class="xref" href="https://github.com/NVIDIA/cudnn-frontend" target="_blank" shape="rect">cuDNN frontend GitHub repository</a> shortly.
                                 </li>
                                 <li class="li liexpand">Added support for the NVIDIA Ada Lovelace architecture.</li>
                                 <li class="li liexpand">Added support for the following new resampling modes:<a name="rel-860__ul_bcq_5jl_z5b" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-860__ul_bcq_5jl_z5b">
                                       <li class="li">Resampling forward:
                                          									<samp class="ph codeph">CUDNN_RESAMPLE_AVGPOOL_EXCLUDE_PADDING</samp></li>
                                       <li class="li">Resampling backward:
                                          									<samp class="ph codeph">CUDNN_RESAMPLE_AVGPOOL_EXCLUDE_PADDING</samp>,
                                          									<samp class="ph codeph">CUDNN_RESAMPLE_AVGPOOL_INCLUDE_PADDING</samp>, and
                                          									<samp class="ph codeph">CUDNN_RESAMPLE_MAXPOOL</samp></li>
                                    </ul>
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-860__section_h3k_whr_g5b"><a name="rel-860__section_h3k_whr_g5b" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:<a name="rel-860__ul_abd_lqx_p5b" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-860__ul_abd_lqx_p5b">
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp> 58 for forward convolution, 63 for
                                    						backwards data, and 62 for backwards filter used to falsely advertise the
                                    						Tensor Core numerical note on SM 7.2 and SM 7.5 when running FP32 input,
                                    						FP32 output, and FP32 accumulation convolutions. They are fixed in this
                                    						release and correctly advertise non Tensor Core numerical notes.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp> 58 for forward convolution, 63 for
                                    						backwards data, and 62 for backwards filter used to allow tensor alignments
                                    						of less than 16 bytes. To execute the advertised tensor core property, they
                                    						have been fixed to require 16 byte alignment.
                                 </li>
                                 <li class="li liexpand">With the cuDNN version 8 backend API, <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> for
                                    						forward convolution is not thread-safe when being executed simultaneously
                                    						with multi-threads that share the same execution plan. This issue has been
                                    						fixed in this release.
                                 </li>
                                 <li class="li liexpand">cudNN 8.5.0 introduced the approximated version of GELU for both forward and backward paths
                                    						as new pointwise modes. While
                                    							<samp class="ph codeph">CUDNN_POINTWISE_GELU_APPROX_TANH_FWD</samp> introduced a
                                    						performance improvement over <samp class="ph codeph">CUDNN_POINTWISE_GELU_FWD</samp>,
                                    							<samp class="ph codeph">CUDNN_POINTWISE_GELU_APPROX_TANH_BWD</samp> was showing
                                    						regressions compared to <samp class="ph codeph">CUDNN_POINTWISE_GELU_BWD</samp>. This has
                                    						now been addressed, and the approximated version of backward GELU is now
                                    						showing slight improvements.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-860__section_t2m_s5p_nnb"><a name="rel-860__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel-860__ul_mbs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-860__ul_mbs_qgh_fsb">
                                 <li class="li liexpand">On Turing, Volta, Kepler, and Maxwell GPUs, 3D convolutions may exhibit some slowdowns when
                                    						the padding size is larger than the filter size. 2D convolutions may
                                    						encounter an illegal memory access error when the padding size is larger
                                    						than the filter size and the horizontal stride is larger than 1. This issue
                                    						will be resolved in the next release.
                                 </li>
                                 <li class="li liexpand">The performance of the runtime fusion engine is suboptimal on Windows. </li>
                                 <li class="li liexpand">Use of <samp class="ph codeph">cudnnFusedOpsExecute()</samp> on Volta compatible
                                    						architectures hosted on AArch64 systems may generate incorrect results when
                                    						used with <samp class="ph codeph">cudnnFusedOps_t</samp> set to
                                    							<samp class="ph codeph">CUDNN_FUSED_SCALE_BIAS_ACTIVATION_WGRAD</samp>.
                                 </li>
                                 <li class="li liexpand">The cuDNN 8.6.0 library may exhibit some slowdowns in <samp class="ph codeph">wgrad</samp> calculation
                                    						for EfficientDet, EfficientNet, Mask R-CNN, ResNet, ResNeXt, and SSD layers
                                    						when it was built with CUDA Toolkit 10.2.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a> does
                                    						not currently support BFLOAT16. If an input tensor uses BFLOAT16, the API
                                    						will return <samp class="ph codeph">BAD_PARAM</samp>.
                                 </li>
                                 <li class="li liexpand">With CUDA 11.7, the NVRTC library may cause a small memory leak when using
                                    						the runtime fusion engine. The issue has been addressed in the next CUDA
                                    						Toolkit update.
                                 </li>
                                 <li class="li liexpand">A compiler bug in NVRTC in CUDA version 11.7 and earlier, was causing
                                    						incorrect outputs when computing logical operations on boolean input tensors
                                    						in the runtime fusion engine. A workaround has been integrated in this
                                    						release to avoid the most common issues. However, it is highly recommended
                                    						to update to at least CUDA version 11.7u1 for a fix. Specifically, known
                                    						failure cases are when pointwise operations of mode
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_NOT</samp>,
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_AND</samp> or
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_OR</samp> operates on boolean
                                    						tensors.
                                 </li>
                                 <li class="li liexpand">If cuDNN 8.4.1 or earlier statically links with
                                    							<samp class="ph codeph">libcudart.so</samp> from the CUDA Toolkit 11.7 or later, when
                                    						the LFL feature is activated, the results from
                                    							<samp class="ph codeph">cudnnFind*Algo</samp> will not be accurate.
                                 </li>
                                 <li class="li liexpand">For packed NCHW tensors using the FP16 datatype, cuDNN attempts to run an optimized kernel
                                    						if the values of N, C, H, and W are even. In cuDNN versions before 8.4, it
                                    						is possible that incorrect values are generated if odd values for the
                                    						strides of N or C are used.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN's <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING</samp>
                                    						may see <samp class="ph codeph">CUDNN_STATUS_BAD_PARAM</samp> returned for a problem that
                                    						should otherwise be supported by that choice of algo.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnDropoutForward()</samp> and
                                    							<samp class="ph codeph">cudnnDropoutBackward()</samp> will return incorrect results
                                    						when input or output tensors have overlapping strides.
                                 </li>
                                 <li class="li liexpand">The documentation for <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnReorderFilterAndBias" target="_blank" shape="rect"><samp class="ph codeph">cudnnReorderFilterAndBias()</samp></a> requires
                                    						corrections for clarity.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX
                                    						3090 compared to 2080 Ti. This includes EfficientNet with up to 6x
                                    						performance difference, UNet up to 1.6x performance difference and Tacotron
                                    						up to 1.6x performance difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    							<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there are known performance regressions up to 2x on
                                    						select configurations for AlexNet-like models on NVIDIA Turing GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior,
                                    						accumulator data types, and so on, have not been documented as of yet.
                                 </li>
                                 <li class="li liexpand">It is possible, starting in cuDNN 7.6 and up to but not including 8.1.1, to
                                    						leak memory when computing common convolution operations in rare cases.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN version 8.0.2, there is a known 3x performance regression for a single
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> use
                                    						case.
                                 </li>
                                 <li class="li liexpand">In CUDA graph capture mode, CUDA streams internal to cuDNN are not
                                    						guaranteed to have the same priority as the user stream that is set by
                                    							<samp class="ph codeph">cudnnSetStream()</samp>.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingBackward()</samp></a> enables both
                                    							<samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp> data pointers (together with
                                    						the related tensor descriptor handles) to be <samp class="ph codeph">NULL</samp> for
                                    						avg-pooling. This could save memory footprint and bandwidth.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN 8.4.0 may observe a slowdown in the Single Shot Multibox
                                    						Detector (SSD) model. This will be fixed in a future release.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with filter size
                                    						1x1. The severity would be different depending on which version of the CUDA
                                    						Toolkit the user is using.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with high group
                                    						count. The issue is more severe on V100.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-860__section_dph_zkv_jrb"><a name="rel-860__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-860__section_nbs_qgh_fsb"><a name="rel-860__section_nbs_qgh_fsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel-860__ul_obs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-860__ul_obs_qgh_fsb">
                                 <li class="li liexpand">Within the cuDNN version 8 backend API, the following engines are known to not be
                                    						thread-safe when executed simultaneously with multiple threads sharing the
                                    						same execution plan:
                                    
                                    
                                    <div class="tablenoborder"><a name="rel-860__table_t2h_lsp_r5b" shape="rect">
                                          <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="rel-860__table_t2h_lsp_r5b" class="table" frame="border" border="1" rules="all">
                                          <caption><span class="tablecap">Table 12. Engines That Are Not Thread-Safe</span></caption>
                                          <thead class="thead" align="left">
                                             <tr class="row">
                                                <th class="entry" valign="top" width="25%" id="d54e14253" rowspan="1" colspan="1"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp></th>
                                                <th class="entry" valign="top" width="25%" id="d54e14257" rowspan="1" colspan="1"><samp class="ph codeph">Fprop</samp></th>
                                                <th class="entry" valign="top" width="25%" id="d54e14261" rowspan="1" colspan="1"><samp class="ph codeph">Dgrad</samp></th>
                                                <th class="entry" valign="top" width="25%" id="d54e14265" rowspan="1" colspan="1"><samp class="ph codeph">Wgrad</samp></th>
                                             </tr>
                                          </thead>
                                          <tbody class="tbody">
                                             <tr class="row">
                                                <td class="entry" valign="top" width="25%" headers="d54e14253" rowspan="1" colspan="1">A100</td>
                                                <td class="entry" valign="top" width="25%" headers="d54e14257" rowspan="1" colspan="1"><a name="rel-860__ul_c53_qsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-860__ul_c53_qsp_r5b">
                                                      <li class="li">36</li>
                                                      <li class="li">38</li>
                                                      <li class="li">45</li>
                                                      <li class="li">46</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="25%" headers="d54e14261" rowspan="1" colspan="1"><a name="rel-860__ul_rh2_ssp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-860__ul_rh2_ssp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">19</li>
                                                      <li class="li">22</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">28</li>
                                                      <li class="li">40</li>
                                                      <li class="li">46</li>
                                                      <li class="li">51</li>
                                                      <li class="li">56</li>
                                                      <li class="li">57</li>
                                                      <li class="li">58</li>
                                                      <li class="li">59</li>
                                                      <li class="li">60</li>
                                                      <li class="li">65</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="25%" headers="d54e14265" rowspan="1" colspan="1"><a name="rel-860__ul_mjw_vsp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-860__ul_mjw_vsp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">21</li>
                                                      <li class="li">23</li>
                                                      <li class="li">33</li>
                                                      <li class="li">37</li>
                                                      <li class="li">47</li>
                                                      <li class="li">48</li>
                                                      <li class="li">49</li>
                                                      <li class="li">50</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="25%" headers="d54e14253" rowspan="1" colspan="1">V100</td>
                                                <td class="entry" valign="top" width="25%" headers="d54e14257" rowspan="1" colspan="1"><a name="rel-860__ul_qrn_btp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-860__ul_qrn_btp_r5b">
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">10</li>
                                                      <li class="li">12</li>
                                                      <li class="li">16</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">49</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="25%" headers="d54e14261" rowspan="1" colspan="1"><a name="rel-860__ul_ffy_dtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-860__ul_ffy_dtp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">37</li>
                                                      <li class="li">38</li>
                                                      <li class="li">44</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="25%" headers="d54e14265" rowspan="1" colspan="1"><a name="rel-860__ul_lg1_htp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-860__ul_lg1_htp_r5b">
                                                      <li class="li">6</li>
                                                      <li class="li">7</li>
                                                      <li class="li">21</li>
                                                      <li class="li">35</li>
                                                      <li class="li">36</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">52</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                             <tr class="row">
                                                <td class="entry" valign="top" width="25%" headers="d54e14253" rowspan="1" colspan="1">T4</td>
                                                <td class="entry" valign="top" width="25%" headers="d54e14257" rowspan="1" colspan="1"><a name="rel-860__ul_q3j_jtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-860__ul_q3j_jtp_r5b">
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">14</li>
                                                      <li class="li">16</li>
                                                      <li class="li">18</li>
                                                      <li class="li">26</li>
                                                      <li class="li">30</li>
                                                      <li class="li">31</li>
                                                      <li class="li">34</li>
                                                      <li class="li">42</li>
                                                      <li class="li">50</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="25%" headers="d54e14261" rowspan="1" colspan="1"><a name="rel-860__ul_ax4_ntp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-860__ul_ax4_ntp_r5b">
                                                      <li class="li">1</li>
                                                      <li class="li">2</li>
                                                      <li class="li">3</li>
                                                      <li class="li">8</li>
                                                      <li class="li">9</li>
                                                      <li class="li">12</li>
                                                      <li class="li">13</li>
                                                      <li class="li">15</li>
                                                      <li class="li">18</li>
                                                      <li class="li">19</li>
                                                      <li class="li">21</li>
                                                      <li class="li">25</li>
                                                      <li class="li">26</li>
                                                      <li class="li">29</li>
                                                      <li class="li">30</li>
                                                      <li class="li">37</li>
                                                      <li class="li">39</li>
                                                      <li class="li">44</li>
                                                      <li class="li">45</li>
                                                   </ul>
                                                </td>
                                                <td class="entry" valign="top" width="25%" headers="d54e14265" rowspan="1" colspan="1"><a name="rel-860__ul_j3t_rtp_r5b" shape="rect">
                                                      <!-- --></a><ul class="ul" id="rel-860__ul_j3t_rtp_r5b">
                                                      <li class="li">12</li>
                                                      <li class="li">21</li>
                                                      <li class="li">27</li>
                                                      <li class="li">28</li>
                                                      <li class="li">43</li>
                                                      <li class="li">44</li>
                                                      <li class="li">51</li>
                                                      <li class="li">53</li>
                                                   </ul>
                                                </td>
                                             </tr>
                                          </tbody>
                                       </table>
                                    </div>
                                 </li>
                                 <li class="li liexpand">The status returned by <samp class="ph codeph">cudnnBackendFinalize()</samp> or
                                    							<samp class="ph codeph">cudnnBackendExecute()</samp> on a
                                    							<samp class="ph codeph">CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR</samp> may change
                                    						depending on the version of the dynamic dependencies of cuDNN. As of this
                                    						writing, only cuBLAS is known to affect the return status of these function
                                    						calls.
                                 </li>
                                 <li class="li liexpand">The cuDNN static builds load NVRTC dynamically when using the runtime fusion
                                    						engine.
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not
                                    						required to consider padding. Users of cuDNN can witness an unexpected lack
                                    						of problem support when forward convolution spatial dimensions are less than
                                    						the filter size and padding is nonzero but is sufficient to extend spatial
                                    						dimensions to or beyond filter dimensions. This is commonly observed with,
                                    						but not limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand">When performing batch normalization in cuDNN, the operation is allowed to
                                    						proceed if the output tensor strides are overlapping, however, there is no
                                    						guarantee of deterministic results.
                                 </li>
                                 <li class="li liexpand">It is possible, starting in cuDNN 7.6 and up to but not including 8.1.1, to
                                    						leak memory when computing common convolution operations in rare cases.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 25</samp> for convolution backwards data (which
                                    						is part of legacy <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp>) does
                                    						not support tensors in which the product N*C*H*W of the output gradient
                                    						tensor equals to or exceeds 2^31.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX =</samp><samp class="ph codeph">1</samp> for convolution
                                    						backwards data (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp>) does not support
                                    						tensors in which the product N*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. This issue has been present in all previous releases of
                                    						cuDNN and exercising the use case for the engine would show incorrect
                                    						results.
                                 </li>
                                 <li class="li liexpand">Versions of cuDNN before the 8.0 release series do not support the NVIDIA
                                    						Ampere Architecture and will generate incorrect results if used on that
                                    						architecture. Furthermore, if used, training operations can succeed with a
                                    						NaN loss for every epoch.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later. It also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples must be installed in a writable location. If not installed in a
                                    						writable location, the samples can crash.
                                 </li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit nondeterministic behavior when the cuDNN
                                       							library is built with CUDA Toolkit 10.2 or higher. This is the result of
                                       							a new buffer management and heuristics in the cuBLAS library. As
                                       							described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This happens
                                       							when two buffer sizes (16 KB and 4 MB) are used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting
                                       							for a buffer of that size to be released, a smaller buffer may be used
                                       							with a different GPU kernel. The kernel selection may affect numerical
                                       							results. The user can eliminate the nondeterministic behavior of cuDNN
                                       							RNN and multihead attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16
                                       							KB each in GPU memory while the second setting creates two buffers of 4
                                       							MB each. The default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors
                                    						in order to run efficiently. As always, cuDNN recommends users to align
                                    						tensors to 16 byte boundaries that will be sufficiently aligned for any
                                    						computational option in cuDNN. Doing otherwise may cause performance
                                    						regressions in cuDNN 8.0.x compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and
                                    						the output is in FP16 (half float), there are cases where the numerical
                                    						accuracy between the different algorithms might differ. cuDNN 8.0.x users
                                    						can target the backend API to query the numerical notes of the algorithms to
                                    						get the information programmatically. There are cases where algo0 and algo1
                                    						will have a reduced precision accumulation when users target the legacy API.
                                    						In all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and
                                    						backward filter, grouped convolution with groups larger than 1 and with odd
                                    						product of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D
                                    						convolution), <samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on
                                    						devices older than NVIDIA Volta. To prevent a potential illegal memory
                                    						access by an instruction that only has a 16-bit version in NVIDIA Volta and
                                    						later, pad at least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use
                                    						texture-based load structure for performance improvements particularly in
                                    						older hardware architectures. Users can opt out of using texture using the
                                    						environmental variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this
                                    						variable is removed. Texture loading is turned off by default. Users who
                                    						want to continue to use texture-based load, can adapt the new backend API,
                                    						and toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check API (for example,
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command
                                    						explicitly to resolve the undefined symbols from cuDNN static
                                    						libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA
                                    						Pascal and later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-860__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-860__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in
                                    						compatibility mode are tested with compute-sanitizer,
                                    							<samp class="ph codeph">cuGetProcAddress</samp> failures with error code 500 will
                                    						arise due to missing functions. This error can be ignored, or suppressed
                                    						with the <samp class="ph codeph">--report-api-errors no</samp> option, as this is due to
                                    						CUDA backward compatibility checking if a function is usable with the CUDA
                                    						toolkit combination. The functions are introduced in a later version of CUDA
                                    						but are not available on the current platform. The absence of these
                                    						functions is harmless and will not give rise to any functional issues.
                                 </li>
                                 <li class="li liexpand">Users of <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 0</samp> for batch normalization
                                    						forwards training and batch normalization backwards may obtain incorrect
                                    						results when batch size is greater than 1 and when channel count is not
                                    						evenly divisible by 8. These values of
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp> correspond to newly
                                    						added multi-GPU batch normalization support within cuDNN 8.5. Use of
                                    						single-GPU batch normalization is unaffected by this issue. cuDNN will be
                                    						revised to reject incorrectly supported multi-GPU batch normalization
                                    						problems in a future release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-850"><a name="rel-850" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-850" name="rel-850" shape="rect">1.13.&nbsp;cuDNN Release 8.5.0</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">These are the NVIDIA cuDNN 8.5.0 Release Notes. These Release Notes include fixes
                           		from the previous cuDNN releases as well as the following additional changes.
                        </p>
                        <div class="section" id="rel-850__section_l2m_s4c_2jb"><a name="rel-850__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">These Release Notes are applicable to both cuDNN and NVIDIA JetPack™
                              				users of cuDNN unless appended specifically with <em class="ph i">(not applicable for Jetson
                                 					platforms)</em>.
                           </p>
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-850__section_a3q_qth_rtb"><a name="rel-850__section_a3q_qth_rtb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-850__ul_ehz_rth_rtb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-850__ul_ehz_rth_rtb">
                                 <li class="li liexpand">Achieved 30% reduction in library size by removing unused kernels. The
                                    						current cuDNN 8.5.0 library size is 850 MB down from 1.2 GB compared to the
                                    						8.4.x releases.
                                 </li>
                                 <li class="li liexpand">Four new pointwise modes were added:<a name="rel-850__ul_wvq_q3w_h5b" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-850__ul_wvq_q3w_h5b">
                                       <li class="li"><samp class="ph codeph">CUDNN_POINTWISE_GELU_APPROX_TANH_FWD</samp> and
                                          									<samp class="ph codeph">CUDNN_POINTWISE_GELU_APPROX_TANH_BWD</samp>, which are
                                          								used for approximating GELU in the forward and backward pass,
                                          								respectively.
                                       </li>
                                       <li class="li"><samp class="ph codeph">CUDNN_POINTWISE_ERF</samp>, which can be used to piecewise
                                          								create the GELU operator.
                                       </li>
                                       <li class="li"><samp class="ph codeph">CUDNN_POINTWISE_IDENTITY</samp>, which can be used for
                                          								explicitly converting between formats.
                                       </li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand">Improved graph API runtime compilation support:<a name="rel-850__ul_lzc_53w_h5b" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-850__ul_lzc_53w_h5b">
                                       <li class="li">Added support for performant adaptive pooling for NHWC layout
                                          								supporting flexible I/O datatypes. It also supports large tensors
                                          								with more than 4 trillion elements.
                                       </li>
                                       <li class="li">Added support for passing host scalars by value as B tensor in the
                                          								pointwise operations.
                                       </li>
                                       <li class="li">Added support for generating code for more broadcasting patterns in
                                          								the pointwise operations.
                                       </li>
                                       <li class="li">The CPU overhead associated with the subgraph execution has been
                                          								reduced by 30 - 40%.
                                       </li>
                                       <li class="li">NVIDIA Ampere Architecture INT8 conv fusion heuristics have been
                                          								updated to recommend more performant kernel configs for smaller
                                          								problem sizes.
                                       </li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand">Added support for error reporting in the RNN APIs.</li>
                                 <li class="li liexpand">When using cuDNN builds against CUDA 11.x with cuBLAS version &gt;= 11.6 U1,
                                    						all kernels are now guaranteed to be launched in streams whose priorities
                                    						match the user stream that is set by <samp class="ph codeph">cudnnSetStream()</samp>.
                                 </li>
                                 <li class="li liexpand">Documented operation specific constraints for the runtime fusion engine in the newly added
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#op-specific-contraints-runtime-fusion-engine" target="_blank" shape="rect">Operation Specific Constraints for the
                                       							Runtime Fusion Engine</a> section.
                                 </li>
                                 <li class="li liexpand">Double precision support for the CTC loss. </li>
                                 <li class="li liexpand">Added support for Ubuntu 22.04 on x86_64 and AArch64 ARM. For more information, refer to
                                    						the supported <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html#cudnn-versions-linux" target="_blank" shape="rect">Linux versions of cuDNN</a>
                                    						section.
                                 </li>
                                 <li class="li liexpand">Added support for CUDA 11.7. For more information, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">GPU, CUDA Toolkit, and CUDA Driver
                                       							Requirements</a> section.
                                 </li>
                                 <li class="li liexpand">Added the <samp class="ph codeph">cudnnBackendNormFwdPhase_t</samp>,
                                    							<samp class="ph codeph">cudnnBackendNormMode_t</samp>,
                                    							<samp class="ph codeph">CUDNN_BACKEND_OPERATION_NORM_FORWARD_DESCRIPTOR</samp>,
                                    							<samp class="ph codeph">CUDNN_BACKEND_OPERATION_NORM_BACKWARD_DESCRIPTOR</samp>,
                                    							<samp class="ph codeph">cudnnSignalMode_t</samp>,
                                    							<samp class="ph codeph">CUDNN_BACKEND_OPERATION_CONCAT_DESCRIPTOR</samp>, and
                                    							<samp class="ph codeph">CUDNN_BACKEND_OPERATION_SIGNAL_DESCRIPTOR</samp> functions to
                                    						the Backend API. These new operations help a cuDNN graph communicate and/or
                                    						synchronize with another cuDNN graph possibly on a peer GPU. For more
                                    						information, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnn-backend-api" target="_blank" shape="rect">NVIDIA cuDNN Backend API</a>
                                    						documentation.
                                 </li>
                                 <li class="li liexpand">Added new data structure <samp class="ph codeph">cudnnFraction_t</samp> to the Backend API. This more
                                    						precisely describes the size ratio between the I/O images under fractional
                                    						up/downsampling and adaptive pooling use cases. For more information, refer
                                    						to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnn-backend-api" target="_blank" shape="rect">NVIDIA cuDNN Backend API</a>
                                    						documentation.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-850__section_xt1_3lq_btb"><a name="rel-850__section_xt1_3lq_btb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p"><a name="rel-850__ul_trj_3lq_btb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-850__ul_trj_3lq_btb">
                                 <li class="li liexpand">For packed NCHW tensors using the FP16 data-type, cuDNN attempted to run an
                                    						optimized kernel if the values of N, C, H, and W were even. In cuDNN
                                    						versions prior to 8.5, it was possible that incorrect values were generated
                                    						if odd values for N or C were used. Starting in cuDNN 8.5, if an odd value
                                    						for N or C is specified, cuDNN runs with an unoptimized kernel.
                                 </li>
                                 <li class="li liexpand">cuDNN was not enforcing the
                                    							<samp class="ph codeph">CUDNN_ATTR_EXECUTION_PLAN_HANDLE</samp> attribute for the
                                    							<samp class="ph codeph">CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR</samp>. It is now
                                    						enforced in cuDNN 8.5.0. <samp class="ph codeph">cudnnBackendFinalize()</samp> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_BAD_PARAM</samp> if the handle attribute is not
                                    						set.
                                 </li>
                                 <li class="li liexpand">Running depthwise convolutions in NHWC layout with
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION</samp> mode and batch size &gt;= 8 could
                                    						produce incorrect results with cuDNN 8.1 and later. This has been fixed in
                                    						this release.
                                 </li>
                                 <li class="li liexpand">Cases of folding transform which were not supported were erroring out with
                                    							<samp class="ph codeph">BAD_PARAM</samp>, this has been fixed to return the correct
                                    						error code of <samp class="ph codeph">NOT_SUPPORTED</samp>.
                                 </li>
                                 <li class="li liexpand">Improved runtime fusion heuristics for INT8 convolution, correcting small
                                    						problem sizes. 
                                 </li>
                                 <li class="li liexpand">Fixed an issue to ensure <samp class="ph codeph">CUDNN_HEUR_MODE_B</samp> redirects to
                                    							<samp class="ph codeph">CUDNN_HEUR_MODE_A</samp> when unsupported. Frontend version
                                    						0.6.3 has been updated with a similar change to redirect
                                    							<samp class="ph codeph">CUDNN_HEUR_MODE_B</samp> to <samp class="ph codeph">CUDNN_HEUR_MODE_A</samp>
                                    						in older cuDNN versions when <samp class="ph codeph">CUDNN_HEUR_MODE_B</samp> is not
                                    						supported.
                                 </li>
                                 <li class="li liexpand">Fixed an issue in the runtime fusion engine where successive broadcasting
                                    						patterns (for example, scalars broadcasting into vectors, then broadcasting
                                    						into tensors) are not handled correctly and may produce wrong results.
                                 </li>
                                 <li class="li liexpand">In the build for CUDA 11.x, we fixed a couple of issues where some cuDNN internal streams
                                    						were not guaranteed to match the priority of the stream set by
                                    						cudnnSetStream. Now, all internal streams have that guarantee, except in the
                                    						case of CUDA graph capture mode.
                                 </li>
                                 <li class="li liexpand">It was suggested that users of the static library requiring the best
                                    						possible convolution performance use whole-archive linking with the
                                    							<samp class="ph codeph">cnn_infer</samp> and <samp class="ph codeph">cnn_train</samp> static
                                    						sub-libraries. This is no longer needed, however, this will come at a cost
                                    						to the binary size of the application. This linkage requirement will be
                                    						relaxed in a future release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-850__section_h3k_whr_g5b"><a name="rel-850__section_h3k_whr_g5b" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Performance Results</h3>
                           <div class="p">The following table shows the average speed-up of unique cuDNN 3D convolution calls
                              				for each network on V100 and A100 GPUs that satisfies the conditions in Recommended
                              				Settings section of the cuDNN Developer Guide. The end-to-end training performance
                              				will depend on a number of factors, such as framework overhead, kernel run time, and
                              				model architecture type.
                              
                              
                              <div class="tablenoborder"><a name="rel-850__table_hcq_xhr_g5b" shape="rect">
                                    <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="rel-850__table_hcq_xhr_g5b" class="table" frame="border" border="1" rules="all">
                                    <caption><span class="tablecap">Table 13. cuDNN version 8.5.0 compared to 8.4.1</span></caption>
                                    <thead class="thead" align="left">
                                       <tr class="row">
                                          <th class="entry" rowspan="2" valign="top" width="16.666666666666664%" id="d54e15223" colspan="1"><a class="xref" href="https://github.com/NVIDIA/DeepLearningExamples" target="_blank" shape="rect">Model</a></th>
                                          <th class="entry" rowspan="2" valign="top" width="16.666666666666664%" id="d54e15227" colspan="1">Batchsize</th>
                                          <th class="entry" colspan="2" valign="top" id="d54e15230" rowspan="1">A100 8.5.0 vs V100 8.4.1</th>
                                          <th class="entry" colspan="2" valign="top" id="d54e15233" rowspan="1">V100 8.5.0 vs V100
                                             									8.4.1
                                          </th>
                                       </tr>
                                       <tr class="row">
                                          <th class="entry" valign="top" width="16.666666666666664%" id="d54e15239" rowspan="1" colspan="1">FP16</th>
                                          <th class="entry" valign="top" width="16.666666666666664%" id="d54e15242" rowspan="1" colspan="1">FP32</th>
                                          <th class="entry" valign="top" width="16.666666666666664%" id="d54e15245" rowspan="1" colspan="1">FP16</th>
                                          <th class="entry" valign="top" width="16.666666666666664%" id="d54e15248" rowspan="1" colspan="1">FP32</th>
                                       </tr>
                                    </thead>
                                    <tbody class="tbody">
                                       <tr class="row">
                                          <td class="entry" rowspan="4" valign="top" width="16.666666666666664%" headers="d54e15223 d54e15239" colspan="1">V-Net (3D-Image segmentation)</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15227 d54e15242" rowspan="1" colspan="1">2</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15230 d54e15245" rowspan="1" colspan="1">1.1x</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15230 d54e15248" rowspan="1" colspan="1">2.9x</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15233" rowspan="1" colspan="1">1.0x</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15233" rowspan="1" colspan="1">1.0x</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15227 d54e15242" rowspan="1" colspan="1">8</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15230 d54e15245" rowspan="1" colspan="1">1.4x</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15230 d54e15248" rowspan="1" colspan="1">3.4x</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15233" rowspan="1" colspan="1">1.0x</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15233" rowspan="1" colspan="1">1.0x</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15227 d54e15242" rowspan="1" colspan="1">16</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15230 d54e15245" rowspan="1" colspan="1">1.6x</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15230 d54e15248" rowspan="1" colspan="1">3.8x</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15233" rowspan="1" colspan="1">1.0x</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15233" rowspan="1" colspan="1">1.1x</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15227 d54e15242" rowspan="1" colspan="1">32</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15230 d54e15245" rowspan="1" colspan="1">1.8x</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15230 d54e15248" rowspan="1" colspan="1">3.7x</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15233" rowspan="1" colspan="1">1.0x</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15233" rowspan="1" colspan="1">1.0x</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" rowspan="2" valign="top" width="16.666666666666664%" headers="d54e15223 d54e15239" colspan="1">3D-UNet (3D-Image Segmentation)</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15227 d54e15242" rowspan="1" colspan="1">2</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15230 d54e15245" rowspan="1" colspan="1">2.1x</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15230 d54e15248" rowspan="1" colspan="1">6.0x</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15233" rowspan="1" colspan="1">1.0x</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15233" rowspan="1" colspan="1">1.2x</td>
                                       </tr>
                                       <tr class="row">
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15227 d54e15242" rowspan="1" colspan="1">4</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15230 d54e15245" rowspan="1" colspan="1">2.1x</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15230 d54e15248" rowspan="1" colspan="1">5.7x</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15233" rowspan="1" colspan="1">1.0x</td>
                                          <td class="entry" valign="top" width="16.666666666666664%" headers="d54e15233" rowspan="1" colspan="1">1.4x</td>
                                       </tr>
                                    </tbody>
                                 </table>
                              </div>
                           </div>
                        </div>
                        <div class="section" id="rel-850__section_t2m_s5p_nnb"><a name="rel-850__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel-850__ul_mbs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-850__ul_mbs_qgh_fsb">
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a> does
                                    						not currently support BFLOAT16. If an input tensor uses BFLOAT16, the API
                                    						will return <samp class="ph codeph">BAD_PARAM</samp>.
                                 </li>
                                 <li class="li liexpand">With CUDA 11.7, the NVRTC library may cause a small memory leak when using
                                    						the runtime fusion engine. The issue has been addressed in the next CUDA
                                    						Toolkit update.
                                 </li>
                                 <li class="li liexpand">A compiler bug in NVRTC in CUDA version 11.7 and earlier, was causing
                                    						incorrect outputs when computing logical operations on boolean input tensors
                                    						in the runtime fusion engine. A workaround has been integrated in this
                                    						release to avoid the most common issues. However, it is highly recommended
                                    						to update to at least CUDA version 11.7u1 for a fix. Specifically, known
                                    						failure cases are when pointwise operations of mode
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_NOT</samp>,
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_AND</samp> or
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_OR</samp> operates on boolean
                                    						tensors.
                                 </li>
                                 <li class="li liexpand">If cuDNN 8.4.1 or earlier statically links with
                                    							<samp class="ph codeph">libcudart.so</samp> from the CUDA Toolkit 11.7 or later, when
                                    						the LFL feature is activated, the results from
                                    							<samp class="ph codeph">cudnnFind*Algo</samp> will not be accurate.
                                 </li>
                                 <li class="li liexpand">For packed NCHW tensors using the FP16 datatype, cuDNN attempts to run an
                                    						optimized kernel if the values of N, C, H, and W are even. In cuDNN versions
                                    						before 8.4, it is possible that incorrect values are generated if odd values
                                    						for the strides of N or C are used. This issue will be resolved in a future
                                    						release.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN's <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING</samp>
                                    						may see <samp class="ph codeph">CUDNN_STATUS_BAD_PARAM</samp> returned for a problem that
                                    						should otherwise be supported by that choice of algo.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnDropoutForward()</samp> and
                                    							<samp class="ph codeph">cudnnDropoutBackward()</samp> will return incorrect results
                                    						when input or output tensors have overlapping strides.
                                 </li>
                                 <li class="li liexpand">The documentation for <samp class="ph codeph"><u class="ph u">cudnnReorderFilterAndBias()</u></samp> requires
                                    						corrections for clarity.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX
                                    						3090 compared to 2080 Ti. This includes EfficientNet with up to 6x
                                    						performance difference, UNet up to 1.6x performance difference and Tacotron
                                    						up to 1.6x performance difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    							<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there are known performance regressions up to 2x on
                                    						select configurations for AlexNet-like models on NVIDIA Turing GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior,
                                    						accumulator data types, and so on, have not been documented as of yet.
                                 </li>
                                 <li class="li liexpand">It is possible, starting in cuDNN 7.6 and up to but not including 8.1.1, to
                                    						leak memory when computing common convolution operations in rare cases.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN version 8.0.2, there is a known 3x performance regression for a single
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> use
                                    						case.
                                 </li>
                                 <li class="li liexpand">In CUDA graph capture mode, CUDA streams internal to cuDNN are not guaranteed to have the
                                    						same priority as the user stream that is set by
                                    							<samp class="ph codeph">cudnnSetStream()</samp>.
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not
                                    						required to consider padding. Users of cuDNN can witness an unexpected lack
                                    						of problem support when forward convolution spatial dimensions are less than
                                    						the filter size and padding is nonzero, however, is sufficient to extend
                                    						spatial dimensions to or beyond filter dimensions. This is commonly observed
                                    						with, but not limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingBackward()</samp></a> enables both
                                    							<samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp> data pointers (together with
                                    						the related tensor descriptor handles) to be <samp class="ph codeph">NULL</samp> for
                                    						avg-pooling. This could save memory footprint and bandwidth.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN 8.4.0 may observe a slowdown in the Single Shot Multibox
                                    						Detector (SSD) model. This will be fixed in a future release.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with filter size
                                    						1x1. The severity would be different depending on which version of the CUDA
                                    						Toolkit the user is using.
                                 </li>
                                 <li class="li liexpand">There is a known regression when running some convolutions with high group
                                    						count. The issue is more severe on V100.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-850__section_dph_zkv_jrb"><a name="rel-850__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, see the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-850__section_nbs_qgh_fsb"><a name="rel-850__section_nbs_qgh_fsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel-850__ul_obs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-850__ul_obs_qgh_fsb">
                                 <li class="li liexpand">When performing batch normalization in cuDNN, the operation is allowed to
                                    						proceed if the output tensor strides are overlapping, however, there is no
                                    						guarantee of deterministic results.
                                 </li>
                                 <li class="li liexpand">It is possible, starting in cuDNN 7.6 and up to but not including 8.1.1, to
                                    						leak memory when computing common convolution operations in rare cases.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 1025</samp> (which is part of
                                    						legacy <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp>) does not support
                                    						tensors in which the product N*C*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. 
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX =</samp><samp class="ph codeph">1001</samp>
                                    						(which is part of legacy <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp>)
                                    						does not support tensors in which the product N*H*W of the output gradient
                                    						tensor equals to or exceeds 2^31. This issue has been present in all
                                    						previous releases of cuDNN and exercising the use case for the engine would
                                    						show incorrect results.
                                 </li>
                                 <li class="li liexpand">Versions of cuDNN before the 8.0 release series do not support the NVIDIA
                                    						Ampere Architecture and will generate incorrect results if used on that
                                    						architecture. Furthermore, if used, training operations can succeed with a
                                    						NaN loss for every epoch.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later. It also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples must be installed in a writable location. If not installed in a
                                    						writable location, the samples can crash.
                                 </li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit nondeterministic behavior when the cuDNN
                                       							library is built with CUDA Toolkit 10.2 or higher. This is the result of
                                       							a new buffer management and heuristics in the cuBLAS library. As
                                       							described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This happens
                                       							when two buffer sizes (16 KB and 4 MB) are used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting
                                       							for a buffer of that size to be released, a smaller buffer may be used
                                       							with a different GPU kernel. The kernel selection may affect numerical
                                       							results. The user can eliminate the nondeterministic behavior of cuDNN
                                       							RNN and multihead attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16
                                       							KB each in GPU memory while the second setting creates two buffers of 4
                                       							MB each. The default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors
                                    						in order to run efficiently. As always, cuDNN recommends users to align
                                    						tensors to 16 byte boundaries that will be sufficiently aligned for any
                                    						computational option in cuDNN. Doing otherwise may cause performance
                                    						regressions in cuDNN 8.0.x compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and
                                    						the output is in FP16 (half float), there are cases where the numerical
                                    						accuracy between the different algorithms might differ. cuDNN 8.0.x users
                                    						can target the backend API to query the numerical notes of the algorithms to
                                    						get the information programmatically. There are cases where algo0 and algo1
                                    						will have a reduced precision accumulation when users target the legacy API.
                                    						In all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and
                                    						backward filter, grouped convolution with groups larger than 1 and with odd
                                    						product of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D
                                    						convolution), <samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on
                                    						devices older than NVIDIA Volta. To prevent a potential illegal memory
                                    						access by an instruction that only has a 16-bit version in NVIDIA Volta and
                                    						later, pad at least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use
                                    						texture-based load structure for performance improvements particularly in
                                    						older hardware architectures. Users can opt out of using texture using the
                                    						environmental variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this
                                    						variable is removed. Texture loading is turned off by default. Users who
                                    						want to continue to use texture-based load, can adapt the new backend API,
                                    						and toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check API (for example,
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command
                                    						explicitly to resolve the undefined symbols from cuDNN static
                                    						libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA
                                    						Pascal and later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-850__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-850__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in
                                    						compatibility mode are tested with compute-sanitizer,
                                    							<samp class="ph codeph">cuGetProcAddress</samp> failures with error code 500 will
                                    						arise due to missing functions. This error can be ignored, or suppressed
                                    						with the <samp class="ph codeph">--report-api-errors no</samp> option, as this is due to
                                    						CUDA backward compatibility checking if a function is usable with the CUDA
                                    						toolkit combination. The functions are introduced in a later version of CUDA
                                    						but are not available on the current platform. The absence of these
                                    						functions is harmless and will not give rise to any functional issues.
                                 </li>
                                 <li class="li liexpand">Users of <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 11000</samp> and <samp class="ph codeph">12000
                                       						</samp>may obtain incorrect results when batch size is greater than 1 and
                                    						when channel count is not evenly divisible by 8. These values of
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp> correspond to newly
                                    						added multi-GPU batch normalization support within cuDNN 8.5. Use of
                                    						single-GPU batch normalization is unaffected by this issue. cuDNN will be
                                    						revised to reject incorrectly supported multi-GPU batch normalization
                                    						problems in a future release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-841"><a name="rel-841" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-841" name="rel-841" shape="rect">1.14.&nbsp;cuDNN Release 8.4.1</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">These are the NVIDIA cuDNN 8.4.1 Release Notes. These Release Notes include fixes
                           		from the previous cuDNN releases as well as the following additional changes.
                        </p>
                        <div class="section" id="rel-841__section_l2m_s4c_2jb"><a name="rel-841__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">These Release Notes are applicable to both cuDNN and NVIDIA JetPack™
                              				users of cuDNN unless appended specifically with <em class="ph i">(not applicable for Jetson
                                 					platforms)</em>.
                           </p>
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-841__section_a3q_qth_rtb"><a name="rel-841__section_a3q_qth_rtb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-841__ul_ehz_rth_rtb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-841__ul_ehz_rth_rtb">
                                 <li class="li liexpand">Improved runtime subgraph compilation support<a name="rel-841__ul_c52_tth_rtb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-841__ul_c52_tth_rtb">
                                       <li class="li">Added support for <samp class="ph codeph">CUDNN_BACKEND_OPERATION_RESAMPLE_FWD</samp> for the
                                          									<samp class="ph codeph">CUDNN_ATTR_RESAMPLE_MODE</samp> set to
                                          									<samp class="ph codeph">CUDNN_RESAMPLE_AVGPOOL</samp> and
                                          									<samp class="ph codeph">CUDNN_RESAMPLE_MAXPOOL</samp> through the runtime
                                          								fusion engine. It can achieve up to 3x speed up compared to the
                                          								legacy <samp class="ph codeph">cudnnPoolingForward()</samp> API. Pointwise fusions
                                          								to the output of this operation are also supported. Documentation
                                          								about the patterns supported can be found in the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#support-graph-patterns" target="_blank" shape="rect">Supported Graph Patterns</a>
                                          								section. 
                                       </li>
                                       <li class="li">Newly added micro tile sizes for pointwise fusions that provide
                                          								significantly improved performance on smaller problem sizes.
                                       </li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand">The <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html" target="_blank" shape="rect">NVIDIA cuDNN Developer Guide</a> now
                                    						includes an expanded section on supported patterns of the Graph API. It
                                    						takes a systematic approach to explain which graph patterns are supported,
                                    						along with various graphical examples, and details on some of the
                                    						restrictions.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-841__section_xt1_3lq_btb"><a name="rel-841__section_xt1_3lq_btb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p"><a name="rel-841__ul_trj_3lq_btb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-841__ul_trj_3lq_btb">
                                 <li class="li liexpand">A buffer was shared between threads and caused segmentation faults. There
                                    						was previously no way to have a per-thread buffer to avoid these
                                    						segmentation faults. The buffer has been moved to the cuDNN handle. Ensure
                                    						you have a cuDNN handle for each thread because the buffer in the cuDNN
                                    						handle is only for the use of one thread and cannot be shared between two
                                    						threads.
                                 </li>
                                 <li class="li liexpand">Fixed operation graph logging under <samp class="ph codeph">cudnnBackendExecuteGraphVisualize()</samp>
                                    						section upon calling <samp class="ph codeph">cudnnBackendExecute()</samp> on generic
                                    						fusion patterns. Added logging for
                                    							<samp class="ph codeph">CUDNN_BACKEND_OPERATION_MATMUL_DESCRIPTOR</samp> and
                                    							<samp class="ph codeph">CUDNN_BACKEND_MATMUL_DESCRIPTOR</samp>. Fixed logging for
                                    						pointwise mode to show the enum value name.
                                 </li>
                                 <li class="li liexpand">Users specifying backend engines 58, 1063, 2062, and 4039 using
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp> with 1x1 convolutions
                                    						and tensors with more than two GB elements (2G) would see
                                    							<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> in cuDNN 8.3.x. This
                                    						issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">cuDNN returned <samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> from
                                    							<samp class="ph codeph">cudnnConvolutionForward()</samp>,
                                    							<samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp>, or
                                    							<samp class="ph codeph">cudnnConvolutionBackwardData()</samp> when computing
                                    						convolutions with large spatial dimensions and batch sizes. This issue has
                                    						been fixed. Such problems instead return
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> where applicable.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-841__section_t2m_s5p_nnb"><a name="rel-841__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel-841__ul_mbs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-841__ul_mbs_qgh_fsb">
                                 <li class="li liexpand">A compiler bug in NVRTC in CUDA version 11.7 and earlier, was causing incorrect outputs
                                    						when computing logical operations on boolean input tensors in the runtime
                                    						fusion engine. A workaround has been integrated in this release to avoid the
                                    						most common issues. However, it is highly recommended to update to at least
                                    						CUDA version 11.7u1 for a fix. Specifically, known failure cases are when
                                    						pointwise operations of mode <samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_NOT</samp>,
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_AND</samp> or
                                    							<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_OR</samp> operates on boolean
                                    						tensors.
                                 </li>
                                 <li class="li liexpand">cuDNN is not enforcing the <samp class="ph codeph">CUDNN_ATTR_EXECUTION_PLAN_HANDLE</samp>
                                    						attribute for the <samp class="ph codeph">CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR</samp>.
                                    						This issue will be fixed in a future release.
                                 </li>
                                 <li class="li liexpand">If cuDNN 8.4.1 or earlier statically links with
                                    							<samp class="ph codeph">libcudart.so</samp> from the CUDA Toolkit 11.7 or later, when
                                    						the LFL feature is activated, the results from
                                    							<samp class="ph codeph">cudnnFind*Algo</samp> will not be accurate.
                                 </li>
                                 <li class="li liexpand">For packed NCHW tensors using the FP16 datatype, cuDNN attempts to run an optimized kernel
                                    						if the values of N, C, H, and W are even. In cuDNN versions before 8.4, it
                                    						is possible that incorrect values are generated if odd values for the
                                    						strides of N or C are used. This issue will be resolved in a future
                                    						release.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN's <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING</samp>
                                    						may see <samp class="ph codeph">CUDNN_STATUS_BAD_PARAM</samp> returned for a problem that
                                    						should otherwise be supported by that choice of algo.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnDropoutForward()</samp> and
                                    							<samp class="ph codeph">cudnnDropoutBackward()</samp> will return incorrect results
                                    						when input or output tensors have overlapping strides.
                                 </li>
                                 <li class="li liexpand">The documentation for <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnReorderFilterAndBias" target="_blank" shape="rect"><samp class="ph codeph">cudnnReorderFilterAndBias()</samp></a> requires
                                    						corrections for clarity.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX
                                    						3090 compared to 2080 Ti. This includes EfficientNet with up to 6x
                                    						performance difference, UNet up to 1.6x performance difference and Tacotron
                                    						up to 1.6x performance difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    							<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there are known performance regressions up to 2x on
                                    						select configurations for AlexNet-like models on NVIDIA Turing GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior,
                                    						accumulator data types, and so on, have not been documented as of yet.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN version 8.0.2, there is a known 3x performance regression for a single
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> use
                                    						case.
                                 </li>
                                 <li class="li liexpand">CUDA streams internal to cuDNN are not guaranteed to have the same priority
                                    						as the user stream that is set by <samp class="ph codeph">cudnnSetStream()</samp>. We
                                    						recently discovered some issues that break our ability to document
                                    						exceptions to this clearly.
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not
                                    						required to consider padding. Users of cuDNN can witness an unexpected lack
                                    						of problem support when forward convolution spatial dimensions are less than
                                    						the filter size and padding is nonzero, however, is sufficient to extend
                                    						spatial dimensions to or beyond filter dimensions. This is commonly observed
                                    						with, but not limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingBackward()</samp></a> enables both
                                    							<samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp> data pointers (together with
                                    						the related tensor descriptor handles) to be <samp class="ph codeph">NULL</samp> for
                                    						avg-pooling. This could save memory footprint and bandwidth.
                                 </li>
                                 <li class="li liexpand">Users of the static library requiring the best possible convolution performance should use
                                    						whole-archive linking with the <samp class="ph codeph">cnn_infer</samp> and
                                    							<samp class="ph codeph">cnn_train</samp> static sub libraries. This will come at a
                                    						cost to the binary size of the application. This linkage requirement will be
                                    						relaxed in a future release.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN 8.4.0 may observe a slowdown in the Single Shot Multibox
                                    						Detector (SSD) model. This will be fixed in a future release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-841__section_dph_zkv_jrb"><a name="rel-841__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, see the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-841__section_nbs_qgh_fsb"><a name="rel-841__section_nbs_qgh_fsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel-841__ul_obs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-841__ul_obs_qgh_fsb">
                                 <li class="li liexpand">It is possible, starting in cuDNN 7.6 and up to but not including 8.1.1, to
                                    						leak memory when computing common convolution operations in rare cases.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 1025</samp> (which is part of
                                    						legacy <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp>) does not support
                                    						tensors in which the product N*C*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. 
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 1001</samp> (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp>) does not support
                                    						tensors in which the product N*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. This issue has been present in all previous releases of
                                    						cuDNN and exercising the use case for the engine would show incorrect
                                    						results.
                                 </li>
                                 <li class="li liexpand">Versions of cuDNN before the 8.0 release series do not support the NVIDIA
                                    						Ampere Architecture and will generate incorrect results if used on that
                                    						architecture. Furthermore, if used, training operations can succeed with a
                                    						NaN loss for every epoch.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later. It also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples must be installed in a writable location. If not installed in a
                                    						writable location, the samples can crash.
                                 </li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit nondeterministic behavior when the cuDNN
                                       							library is built with CUDA Toolkit 10.2 or higher. This is the result of
                                       							a new buffer management and heuristics in the cuBLAS library. As
                                       							described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This happens
                                       							when two buffer sizes (16 KB and 4 MB) are used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting for a buffer of
                                       							that size to be released, a smaller buffer may be used with a different
                                       							GPU kernel. The kernel selection may affect numerical results. The user
                                       							can eliminate the nondeterministic behavior of cuDNN RNN and multihead
                                       							attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16 KB each in GPU
                                       							memory while the second setting creates two buffers of 4 MB each. The
                                       							default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors in order to run
                                    						efficiently. As always, cuDNN recommends users to align tensors to 16 byte
                                    						boundaries that will be sufficiently aligned for any computational option in
                                    						cuDNN. Doing otherwise may cause performance regressions in cuDNN 8.0.x
                                    						compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and the output is
                                    						in FP16 (half float), there are cases where the numerical accuracy between
                                    						the different algorithms might differ. cuDNN 8.0.x users can target the
                                    						backend API to query the numerical notes of the algorithms to get the
                                    						information programmatically. There are cases where algo0 and algo1 will
                                    						have a reduced precision accumulation when users target the legacy API. In
                                    						all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and backward
                                    						filter, grouped convolution with groups larger than 1 and with odd product
                                    						of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D convolution),
                                    							<samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on devices
                                    						older than NVIDIA Volta. To prevent a potential illegal memory access by an
                                    						instruction that only has a 16-bit version in NVIDIA Volta and later, pad at
                                    						least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use texture-based load
                                    						structure for performance improvements particularly in older hardware
                                    						architectures. Users can opt out of using texture using the environmental
                                    						variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this variable is
                                    						removed. Texture loading is turned off by default. Users who want to
                                    						continue to use texture-based load, can adapt the new backend API, and
                                    						toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check API (for example,
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command explicitly to
                                    						resolve the undefined symbols from cuDNN static libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA Pascal and
                                    						later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-841__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-841__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in
                                    						compatibility mode are tested with compute-sanitizer,
                                    							<samp class="ph codeph">cuGetProcAddress</samp> failures with error code 500 will
                                    						arise due to missing functions. This error can be ignored, or suppressed
                                    						with the <samp class="ph codeph">--report-api-errors no</samp> option, as this is due to
                                    						CUDA backward compatibility checking if a function is usable with the CUDA
                                    						toolkit combination. The functions are introduced in a later version of CUDA
                                    						but are not available on the current platform. The absence of these
                                    						functions is harmless and will not give rise to any functional issues.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-840"><a name="rel-840" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-840" name="rel-840" shape="rect">1.15.&nbsp;cuDNN Release 8.4.0</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">These are the NVIDIA cuDNN 8.4.0 Release Notes. These Release Notes include fixes
                           		from the previous cuDNN releases as well as the following additional changes.
                        </p>
                        <div class="section" id="rel-840__section_l2m_s4c_2jb"><a name="rel-840__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">These Release Notes are applicable to both cuDNN and NVIDIA JetPack™
                              				users of cuDNN unless appended specifically with <em class="ph i">(not applicable for Jetson
                                 					platforms)</em>.
                           </p>
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-840__section_vpl_hyr_btb"><a name="rel-840__section_vpl_hyr_btb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-840__ul_s3b_lyr_btb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-840__ul_s3b_lyr_btb">
                                 <li class="li liexpand"><strong class="ph b">API additions</strong><a name="rel-840__ul_jdk_syr_btb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-840__ul_jdk_syr_btb">
                                       <li class="li liexpand">Added API and support for the <samp class="ph codeph">GEN_INDEX</samp> capability.
                                          									<samp class="ph codeph">CUDNN_POINTWISE_GEN_INDEX</samp> returns the position
                                          								of an element in an input tensor along a given axis. This operation
                                          								is similar to NumPy’s mesh grid operation as it returns a tensor
                                          								with the index of all elements calculated according to the specified
                                          								axis in the original tensor dimensions.
                                       </li>
                                       <li class="li liexpand">Added API and support for the <samp class="ph codeph">BINARY_SELECT</samp>
                                          								capability. <samp class="ph codeph">CUDNN_POINTWISE_BINARY_SELECT</samp> is
                                          								similar to the ternary operation and selects between two input
                                          								elements based on a predicate element.
                                       </li>
                                       <li class="li liexpand">Experimentally supports serialization of execution plans to or from a string
                                          								representation to enable the user to avoid recompilation of the
                                          								fusion kernels. This feature only supports the runtime fusion engine
                                          								currently. Generalized support for additional engines is planned for
                                          								future releases.
                                       </li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><strong class="ph b">Runtime fusion engine improvements</strong><a name="rel-840__ul_l5q_1wn_ctb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-840__ul_l5q_1wn_ctb">
                                       <li class="li liexpand">Previous versions of the runtime fusion engine only supported a minimum 128-bit alignment
                                          								for tensors in all the operations. From this release onwards, the
                                          								minimum alignment requirement has been relaxed down to 32 bit for
                                          								input tensors in matrix multiplication and convolution for NVIDIA
                                          								Ampere Architecture GPUs. For output tensors in any operation and
                                          								input tensors for pointwise operations, the minimum alignment
                                          								requirement has been relaxed down to 8 bit.
                                       </li>
                                       <li class="li liexpand">Added support for ARM servers.</li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><strong class="ph b">Documentation improvements</strong> functions:<a name="rel-840__ul_zp3_wyr_btb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-840__ul_zp3_wyr_btb">
                                       <li class="li liexpand">We added documentation for the following data types and API. <a name="rel-840__ul_cmn_cwn_ctb" shape="rect">
                                             <!-- --></a><ul class="ul" id="rel-840__ul_cmn_cwn_ctb">
                                             <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#CUDNN_BACKEND_OPERATION_REDUCTION_DESCRIPTOR" target="_blank" shape="rect"><samp class="ph codeph">CUDNN_BACKEND_OPERATION_REDUCTION_DESCRIPTOR</samp></a></li>
                                             <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#CUDNN_BACKEND_POINTWISE_DESCRIPTOR" target="_blank" shape="rect"><samp class="ph codeph">CUDNN_BACKEND_POINTWISE_DESCRIPTOR</samp></a></li>
                                             <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#CUDNN_BACKEND_REDUCTION_DESCRIPTOR" target="_blank" shape="rect"><samp class="ph codeph">CUDNN_BACKEND_REDUCTION_DESCRIPTOR</samp></a></li>
                                             <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBackendBehaviorNote_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackendBehaviorNote_t</samp></a></li>
                                             <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBackendTensorReordering_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackendTensorReordering_t</samp></a></li>
                                             <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBnFinalizeStatsMode_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnBnFinalizeStatsMode_t</samp></a></li>
                                             <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPaddingMode_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnPaddingMode_t</samp></a></li>
                                             <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnResampleMode_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnResampleMode_t</samp></a></li>
                                          </ul>
                                       </li>
                                    </ul>
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-840__section_xt1_3lq_btb"><a name="rel-840__section_xt1_3lq_btb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p"><a name="rel-840__ul_trj_3lq_btb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-840__ul_trj_3lq_btb">
                                 <li class="li liexpand">Users of cuDNN’s <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp> when set to 3000
                                    						previously could experience a floating point exception when the filter size
                                    						(filter width * filter height) is greater than or equal to 32. This issue is
                                    						fixed in this release.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN's <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp> when set to 58, 1063, or
                                    						2062 may now use the knob count <samp class="ph codeph">CUDNN_KNOB_TYPE_WORKSPACE</samp>
                                    						to set the allowable workspace of these engines.
                                 </li>
                                 <li class="li liexpand">The documentation of <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardInference" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardInference()</samp></a> and
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardInference" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardInference()</samp></a>
                                    						has been improved for clarity.
                                 </li>
                                 <li class="li liexpand">Previous versions of cuDNN may produce wrong results when used to compute a matrix
                                    						multiplication or fusions containing a matrix multiplication on NVIDIA
                                    						Ampere Architecture based GPUs. This issue has been fixed in this
                                    						release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-840__section_t2m_s5p_nnb"><a name="rel-840__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel-840__ul_mbs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-840__ul_mbs_qgh_fsb">
                                 <li class="li liexpand">Users of cuDNN's <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING</samp>
                                    						may see <samp class="ph codeph">CUDNN_STATUS_BAD_PARAM</samp> returned for a problem that
                                    						should otherwise be supported by that choice of algo.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnConvolutionForward()</samp>,
                                    							<samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp>, and
                                    							<samp class="ph codeph">cudnnConvolutionBackwardData()</samp> may generate illegal
                                    						memory address errors on the NVIDIA Volta and NVIDIA Turing architectures.
                                    						This issue existed in previous 8.3 releases as well.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnDropoutForward()</samp> and
                                    							<samp class="ph codeph">cudnnDropoutBackward()</samp> will return incorrect results
                                    						when input or output tensors have overlapping strides.
                                 </li>
                                 <li class="li liexpand">Users specifying backend engines 58, 1063, 2062, and 4039 using
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp> with 1x1 convolutions
                                    						and tensors with more than two GB elements (2G) will see
                                    							<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> in cuDNN 8.3.x.
                                 </li>
                                 <li class="li liexpand">cuDNN may return <samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> from
                                    							<samp class="ph codeph">cudnnConvolutionForward()</samp>,
                                    							<samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp>, or
                                    							<samp class="ph codeph">cudnnConvolutionBackwardData()</samp> when computing
                                    						convolutions with large spatial dimensions and batch sizes. This issue will
                                    						be addressed in a future release so that such problems will instead return
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> where applicable.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 1025</samp> (which is part of
                                    						legacy <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp>) does not support
                                    						tensors in which the product N*C*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. This issue has been present in all previous releases of
                                    						cuDNN and exercising the use case for the engine results in incorrect
                                    						results.
                                 </li>
                                 <li class="li liexpand">The documentation for <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnReorderFilterAndBias" target="_blank" shape="rect"><samp class="ph codeph">cudnnReorderFilterAndBias()</samp></a> requires
                                    						corrections for clarity.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX
                                    						3090 compared to 2080 Ti. This includes EfficientNet with up to 6x
                                    						performance difference, UNet up to 1.6x performance difference and Tacotron
                                    						up to 1.6x performance difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    							<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there are known performance regressions up to 2x on
                                    						select configurations for AlexNet-like models on NVIDIA Turing GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior,
                                    						accumulator data types, and so on, have not been documented as of yet.
                                 </li>
                                 <li class="li liexpand">It is possible, starting in cuDNN 7.6 and up to but not including 8.1.1, to
                                    						leak memory when computing common convolution operations in rare cases.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN version 8.0.2, there is a known 3x performance regression for a single
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> use
                                    						case.
                                 </li>
                                 <li class="li liexpand">CUDA streams internal to cuDNN are not guaranteed to have the same priority
                                    						as the user stream that is set by <samp class="ph codeph">cudnnSetStream()</samp>. We
                                    						recently discovered some issues that break our ability to document
                                    						exceptions to this clearly.
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not
                                    						required to consider padding. Users of cuDNN can witness an unexpected lack
                                    						of problem support when forward convolution spatial dimensions are less than
                                    						the filter size and padding is nonzero, however, is sufficient to extend
                                    						spatial dimensions to or beyond filter dimensions. This is commonly observed
                                    						with, but not limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingBackward()</samp></a> enables both
                                    							<samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp> data pointers (together with
                                    						the related tensor descriptor handles) to be <samp class="ph codeph">NULL</samp> for
                                    						avg-pooling. This could save memory footprint and bandwidth.
                                 </li>
                                 <li class="li liexpand">Users of the static library requiring the best possible convolution performance should use
                                    						whole-archive linking with the <samp class="ph codeph">cnn_infer</samp> and
                                    							<samp class="ph codeph">cnn_train</samp> static sub libraries. This will come at a
                                    						cost to the binary size of the application. This linkage requirement will be
                                    						relaxed in a future release.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN 8.4.0 may observe a slowdown in the Single Shot Multibox
                                    						Detector (SSD) model. This will be fixed in a future release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-840__section_dph_zkv_jrb"><a name="rel-840__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, see the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-840__section_nbs_qgh_fsb"><a name="rel-840__section_nbs_qgh_fsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel-840__ul_obs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-840__ul_obs_qgh_fsb">
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 1001</samp> (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp>) does not support
                                    						tensors in which the product N*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. This issue has been present in all previous releases of
                                    						cuDNN and exercising the use case for the engine would show incorrect
                                    						results.
                                 </li>
                                 <li class="li liexpand">Versions of cuDNN before the 8.0 release series do not support the NVIDIA Ampere
                                    						Architecture and will generate incorrect results if used on that
                                    						architecture. Furthermore, if used, training operations can succeed with a
                                    						NaN loss for every epoch.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later. It also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples must be installed in a writable location. If not installed in a
                                    						writable location, the samples can crash.
                                 </li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit nondeterministic behavior when the cuDNN
                                       							library is built with CUDA Toolkit 10.2 or higher. This is the result of
                                       							a new buffer management and heuristics in the cuBLAS library. As
                                       							described in  <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This happens
                                       							when two buffer sizes (16 KB and 4 MB) are used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting for a buffer of
                                       							that size to be released, a smaller buffer may be used with a different
                                       							GPU kernel. The kernel selection may affect numerical results. The user
                                       							can eliminate the non-deterministic behavior of cuDNN RNN and multihead
                                       							attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16 KB each in GPU
                                       							memory while the second setting creates two buffers of 4 MB each. The
                                       							default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors in order to run
                                    						efficiently. As always, cuDNN recommends users to align tensors to 16 byte
                                    						boundaries that will be sufficiently aligned for any computational option in
                                    						cuDNN. Doing otherwise may cause performance regressions in cuDNN 8.0.x
                                    						compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and the output is
                                    						in FP16 (half float), there are cases where the numerical accuracy between
                                    						the different algorithms might differ. cuDNN 8.0.x users can target the
                                    						backend API to query the numerical notes of the algorithms to get the
                                    						information programmatically. There are cases where algo0 and algo1 will
                                    						have a reduced precision accumulation when users target the legacy API. In
                                    						all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and backward
                                    						filter, grouped convolution with groups larger than 1 and with odd product
                                    						of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D convolution),
                                    							<samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on devices
                                    						older than NVIDIA Volta. To prevent a potential illegal memory access by an
                                    						instruction that only has a 16-bit version in NVIDIA Volta and later, pad at
                                    						least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use texture-based load
                                    						structure for performance improvements particularly in older hardware
                                    						architectures. Users can opt out of using texture using the environmental
                                    						variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this variable is
                                    						removed. Texture loading is turned off by default. Users who want to
                                    						continue to use texture-based load, can adapt the new backend API, and
                                    						toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check API (for example,
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command explicitly to
                                    						resolve the undefined symbols from cuDNN static libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA Pascal and
                                    						later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-840__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-840__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in
                                    						compatibility mode are tested with compute-sanitizer,
                                    							<samp class="ph codeph">cuGetProcAddress</samp> failures with error code 500 will
                                    						arise due to missing functions. This error can be ignored, or suppressed
                                    						with the <samp class="ph codeph">--report-api-errors no</samp> option, as this is due to
                                    						CUDA backward compatibility checking if a function is usable with the CUDA
                                    						toolkit combination. The functions are introduced in a later version of CUDA
                                    						but are not available on the current platform. The absence of these
                                    						functions is harmless and will not give rise to any functional issues.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-833"><a name="rel-833" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-833" name="rel-833" shape="rect">1.16.&nbsp;cuDNN Release 8.3.3</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">These are the NVIDIA cuDNN 8.3.3 Release Notes. These Release Notes include fixes from
                           		the previous cuDNN releases as well as the following additional changes.
                        </p>
                        <div class="section" id="rel-833__section_l2m_s4c_2jb"><a name="rel-833__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">These Release Notes are applicable to both cuDNN and NVIDIA JetPack™
                              				users of cuDNN unless appended specifically with <em class="ph i">(not applicable for Jetson
                                 					platforms)</em>.
                           </p>
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-833__section_kmq_r5h_fsb"><a name="rel-833__section_kmq_r5h_fsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-833__ul_z4p_s5h_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-833__ul_z4p_s5h_fsb">
                                 <li class="li liexpand">Various improvements were made in the runtime fusion engine:<a name="rel-833__ul_y4y_jyb_rsb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-833__ul_y4y_jyb_rsb">
                                       <li class="li">Added heuristics for convolution + x fusion and matmul + x fusion for NVIDIA Volta and
                                          								NVIDIA Turing architectures.
                                       </li>
                                       <li class="li">Updated the heuristics for matmul + x fusion for NVIDIA Ampere
                                          								Architecture.
                                       </li>
                                       <li class="li">Small performance improvement for the matmul + x fusion.</li>
                                       <li class="li">Compilation time reduction.</li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand">Improved the performance for NHWC INT8 max pooling.</li>
                                 <li class="li liexpand">Updated the list of supported enums in the following data type
                                    							references:<a name="rel-833__ul_lyl_t5h_fsb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-833__ul_lyl_t5h_fsb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBackendAttributeName_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackendAttributeName_t</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBackendAttributeType_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackendAttributeType_t</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBackendDescriptorType_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackendDescriptorType_t</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBackendNumericalNote_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackendNumericalNote_t</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand">Updated and migrated the content from the <em class="ph i">Best Practices For Using cuDNN 3D
                                       							Convolutions</em> to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html" target="_blank" shape="rect">NVIDIA cuDNN Developer Guide</a>. The
                                    						Best Practices document has been deprecated.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-833__section_k4x_zws_lsb"><a name="rel-833__section_k4x_zws_lsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:<a name="rel-833__ul_vwg_1xs_lsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-833__ul_vwg_1xs_lsb">
                                 <li class="li liexpand">Fixed an issue when fusing pointwise operation with a scalar (that is a [1, 1, 1, 1] shaped
                                    						tensor) at the output of a matmul or a convolution. When the output is of
                                    						integer type, the results may be inaccurate or wrong (due to float to INT8
                                    						truncation). After the fix, it will properly round to nearest with
                                    						clamping.
                                 </li>
                                 <li class="li liexpand">Fixed an issue inside the batch norm finalize descriptor where an implementation detail was
                                    						erroneously logged. Such unexpected access could intermittently cause a
                                    						segment fault.
                                 </li>
                                 <li class="li liexpand">Convolution batch norm fusion engines invoked through the graph API only worked with
                                    							<samp class="ph codeph">cudnnBackendTensor</samp> descriptors with dimensions
                                    						specified in “n,c,g,h,w” format. This has been fixed and
                                    							<samp class="ph codeph">cudnnBackendTensors</samp> with dimensions specified any of
                                    						“n,c,h,w” and “n,c,g,h,w” formats can now be passed.
                                 </li>
                                 <li class="li liexpand">Fixed a numerical overflow issue in the computation of softplus activation
                                    						function in the runtime fusion engine that was resulting in
                                    							<samp class="ph codeph">log(exp(x))</samp> being computed as infinity for sufficiently
                                    						large positive values of <samp class="ph codeph">x</samp>.
                                 </li>
                                 <li class="li liexpand">In previous releases, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnTransformFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnTransformFilter()</samp></a> and<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnTransformTensorEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnTransformTensorEx()</samp></a> could produce wrong
                                    						values at some pixels when doing a folding transform. This has been fixed in
                                    						the current release.
                                 </li>
                                 <li class="li liexpand">Documentation has been updated for pooling forward and backward API functions. The
                                    						documentation now discusses which data types and vectorizations are
                                    						supported for the tensor descriptor arguments (this information was
                                    						previously incomplete). For more information, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> and <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingBackward()</samp></a>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-833__section_t2m_s5p_nnb"><a name="rel-833__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel-833__ul_mbs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-833__ul_mbs_qgh_fsb">
                                 <li class="li liexpand"><samp class="ph codeph">cudnnConvolutionForward()</samp>,
                                    							<samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp>, and
                                    							<samp class="ph codeph">cudnnConvolutionBackwardData()</samp> may generate illegal
                                    						memory address errors on the NVIDIA Volta and NVIDIA Turing architectures.
                                    						This issue existed in previous 8.3 releases as well.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnDropoutForward()</samp> and <samp class="ph codeph">cudnnDropoutBackward()</samp> will
                                    						return incorrect results when input or output tensors have overlapping
                                    						strides.
                                 </li>
                                 <li class="li liexpand">Users specifying backend engines 58, 1063, 2062, and 4039 using
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp> with 1x1 convolutions
                                    						and tensors with more than two GB elements (2G) will see
                                    							<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> in cuDNN 8.3.x.
                                 </li>
                                 <li class="li liexpand">cuDNN may return <samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> from
                                    							<samp class="ph codeph">cudnnConvolutionForward()</samp>,
                                    							<samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp>, or
                                    							<samp class="ph codeph">cudnnConvolutionBackwardData()</samp> when computing
                                    						convolutions with large spatial dimensions and batch sizes. This issue will
                                    						be addressed in a future release so that such problems will instead return
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> where applicable.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX = 1025</samp> (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp>) does not support
                                    						tensors in which the product N*C*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. This issue has been present in all previous releases of
                                    						cuDNN and exercising the use case for the engine results in incorrect
                                    						results.
                                 </li>
                                 <li class="li liexpand">The documentation for <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnReorderFilterAndBias" target="_blank" shape="rect"><samp class="ph codeph">cudnnReorderFilterAndBias()</samp></a> requires
                                    						corrections for clarity.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX
                                    						3090 compared to 2080 Ti. This includes EfficientNet with up to 6x
                                    						performance difference, UNet up to 1.6x performance difference and Tacotron
                                    						up to 1.6x performance difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    						<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there are known performance regressions up to 2x on select
                                    						configurations for AlexNet-like models on NVIDIA Turing GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior, accumulator data
                                    						types, and so on,  have not been documented as of yet.
                                 </li>
                                 <li class="li liexpand">It is possible, starting in cuDNN 7.6 and up to but not including 8.1.1, to
                                    						leak memory when computing common convolution operations in rare cases.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN version 8.0.2, there is a known 3x performance regression for a single
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> use
                                    						case.
                                 </li>
                                 <li class="li liexpand">CUDA streams internal to cuDNN are not guaranteed to have the same priority
                                    						as the user stream that is set by <samp class="ph codeph">cudnnSetStream()</samp>. We
                                    						recently discovered some issues that break our ability to document
                                    						exceptions to this clearly.
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not
                                    						required to consider padding. Users of cuDNN can witness an unexpected lack
                                    						of problem support when forward convolution spatial dimensions are less than
                                    						the filter size and padding is nonzero, however, is sufficient to extend
                                    						spatial dimensions to or beyond filter dimensions. This is commonly observed
                                    						with, but not limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingBackward()</samp></a> enables both
                                    							<samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp> data pointers (together with
                                    						the related tensor descriptor handles) to be <samp class="ph codeph">NULL</samp> for
                                    						avg-pooling. This could save memory footprint and bandwidth.
                                 </li>
                                 <li class="li liexpand">Users of the static library requiring the best possible convolution performance should use
                                    						whole-archive linking with the <samp class="ph codeph">cnn_infer</samp> and
                                    							<samp class="ph codeph">cnn_train</samp> static sub libraries. This will come at a
                                    						cost to the binary size of the application. This linkage requirement will be
                                    						relaxed in a future release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-833__section_dph_zkv_jrb"><a name="rel-833__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, see the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-833__section_nbs_qgh_fsb"><a name="rel-833__section_nbs_qgh_fsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel-833__ul_obs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-833__ul_obs_qgh_fsb">
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX =</samp><samp class="ph codeph">1001</samp> (which is part of
                                    						legacy <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp>) does not support
                                    						tensors in which the product N*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. This issue has been present in all previous releases of
                                    						cuDNN and exercising the use case for the engine would show incorrect
                                    						results.
                                 </li>
                                 <li class="li liexpand">Versions of cuDNN before the 8.0 release series do not support the NVIDIA Ampere
                                    						Architecture and will generate incorrect results if used on that
                                    						architecture.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA Toolkit 11.2
                                    						update 1 or later. It also requires the NVRTC from CUDA 11.2 update 1 or
                                    						later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples must be installed in a writable location. If not installed in a writable location,
                                    						the samples can crash.
                                 </li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit nondeterministic behavior when the cuDNN
                                       							library is built with CUDA Toolkit 10.2 or higher. This is the result of
                                       							a new buffer management and heuristics in the cuBLAS library. As
                                       							described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect"><u class="ph u">Results
                                             							Reproducibility</u></a>, numerical results may not be deterministic
                                       							when cuBLAS APIs are launched in more than one CUDA stream using the
                                       							same cuBLAS handle. This happens when two buffer sizes (16 KB and 4 MB)
                                       							are used in the default configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting for a buffer of
                                       							that size to be released, a smaller buffer may be used with a different
                                       							GPU kernel. The kernel selection may affect numerical results. The user
                                       							can eliminate the non-deterministic behavior of cuDNN RNN and multihead
                                       							attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16 KB each in GPU
                                       							memory while the second setting creates two buffers of 4 MB each. The
                                       							default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors in order to run
                                    						efficiently. As always, cuDNN recommends users to align tensors to 16 byte
                                    						boundaries that will be sufficiently aligned for any computational option in
                                    						cuDNN. Doing otherwise may cause performance regressions in cuDNN 8.0.x
                                    						compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and the output is
                                    						in FP16 (half float), there are cases where the numerical accuracy between
                                    						the different algorithms might differ. cuDNN 8.0.x users can target the
                                    						backend API to query the numerical notes of the algorithms to get the
                                    						information programmatically. There are cases where algo0 and algo1 will
                                    						have a reduced precision accumulation when users target the legacy API. In
                                    						all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and backward
                                    						filter, grouped convolution with groups larger than 1 and with odd product
                                    						of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D convolution),
                                    							<samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on devices
                                    						older than NVIDIA Volta. To prevent a potential illegal memory access by an
                                    						instruction that only has a 16-bit version in NVIDIA Volta and later, pad at
                                    						least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use texture-based load
                                    						structure for performance improvements particularly in older hardware
                                    						architectures. Users can opt out of using texture using the environmental
                                    						variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this variable is
                                    						removed. Texture loading is turned off by default. Users who want to
                                    						continue to use texture-based load, can adapt the new backend API, and
                                    						toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check API (for example,
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command explicitly to
                                    						resolve the undefined symbols from cuDNN static libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA Pascal and
                                    						later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-833__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-833__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in
                                    						compatibility mode are tested with compute-sanitizer,
                                    							<samp class="ph codeph">cuGetProcAddress</samp> failures with error code 500 will
                                    						arise due to missing functions. This error can be ignored, or suppressed
                                    						with the <samp class="ph codeph">--report-api-errors no</samp> option, as this is due to
                                    						CUDA backward compatibility checking if a function is usable with the CUDA
                                    						toolkit combination. The functions are introduced in a later version of CUDA
                                    						but are not available on the current platform. The absence of these
                                    						functions is harmless and will not give rise to any functional issues.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-833__section_mfp_4g2_xrb"><a name="rel-833__section_mfp_4g2_xrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Deprecated Features</h3>
                           <div class="p">The following features are deprecated in cuDNN 8.3.3:<a name="rel-833__ul_z2h_pg2_xrb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-833__ul_z2h_pg2_xrb">
                                 <li class="li">We are deprecating the reporting of performance results in the Best Practices For Using
                                    						cuDNN 3D Convolutions and will instead update these Release Notes if there
                                    						is anything interesting to report release-over-release. Starting with cuDNN
                                    						8.4.0, this section will be removed. For past performance tables, refer to
                                    						the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                                 </li>
                                 <li class="li">Updated and migrated the content from the Best Practices For Using cuDNN 3D Convolutions to
                                    						the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html" target="_blank" shape="rect"><u class="ph u">NVIDIA cuDNN Developer
                                          							Guide</u></a>. The Best Practices document has been deprecated.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-832"><a name="rel-832" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-832" name="rel-832" shape="rect">1.17.&nbsp;cuDNN Release 8.3.2</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">This is the NVIDIA cuDNN 8.3.2 Release Notes. This release includes fixes from the
                           		previous cuDNN v8.1.x releases as well as the following additional changes. These Release
                           		Notes are applicable to both cuDNN and NVIDIA JetPack™ users of cuDNN unless
                           		appended specifically with <em class="ph i">(not applicable for Jetson platforms)</em>.
                        </p>
                        <div class="section">
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-832__section_p2s_r1t_xrb"><a name="rel-832__section_p2s_r1t_xrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-832__ul_mnw_r1t_xrb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-832__ul_mnw_r1t_xrb">
                                 <li class="li liexpand">In the runtime fusion engine, pointwise fusion for batched matmul was
                                    						extended to support operations with full tensor in the epilog.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-832__section_mcm_jy3_prb"><a name="rel-832__section_mcm_jy3_prb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Announcements</h3>
                           <div class="p"><a name="rel-832__ul_gv5_jy3_prb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-832__ul_gv5_jy3_prb">
                                 <li class="li liexpand">Debian and RPM local installers are now provided on the <a class="xref" href="https://developer.nvidia.com/cudnn" target="_blank" shape="rect">cuDNN download page</a>. For more information, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html" target="_blank" shape="rect">NVIDIA cuDNN Installation Guide</a>. 
                                 </li>
                                 <li class="li liexpand">Individual cuDNN packages can be found in the CUDA repository:<a class="xref" href="https://developer.download.nvidia.com/compute/cuda/repos/" target="_blank" shape="rect">https://developer.download.nvidia.com/compute/cuda/repos/</a></li>
                                 <li class="li liexpand">As part of a renewed effort to provide tarball and zip archive deliverables for NVIDIA
                                    						products, the format has changed from existing <samp class="ph codeph">.txz</samp>
                                    						archives. For more information about <samp class="ph codeph">*-archive.tar.xz</samp> and
                                    							<samp class="ph codeph">*-archive.zip</samp> deliverables, refer to the <a class="xref" href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#tarball-zipfile-overview" target="_blank" shape="rect">Tarball and Zip Archive
                                       							Deliverables</a> and the <a class="xref" href="https://github.com/NVIDIA/build-system-archive-import-examples" target="_blank" shape="rect">README for
                                       							build-system-archive-import-examples</a>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-832__section_rdt_gjb_wrb"><a name="rel-832__section_rdt_gjb_wrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p"><a name="rel-832__ul_trb_hjb_wrb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-832__ul_trb_hjb_wrb">
                                 <li class="li liexpand">cuDNN multihead attention produces incorrect results in case the
                                    							<samp class="ph codeph">postDropout</samp> feature is enabled. The issue has been
                                    						fixed in this release.
                                 </li>
                                 <li class="li liexpand">Running <samp class="ph codeph">convBiasAct</samp> in
                                    							<samp class="ph codeph">CUDNN_CROSS_CORRELATION</samp> mode could result in incorrect
                                    						results if the <samp class="ph codeph">GroupedDirect</samp> engine is selected.
                                 </li>
                                 <li class="li liexpand">Documentation has been updated for pooling forward and backward API functions, to update
                                    						which data types and vectorizations are supported for the tensor descriptor
                                    						arguments. For more information, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> and <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingBackward()</samp></a>.
                                 </li>
                                 <li class="li liexpand">The documentation in the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#reproducibility" target="_blank" shape="rect">Reproducibility</a> section in the
                                    						NVIDIA cuDNN Developer Guide has been improved upon for clarity.
                                 </li>
                                 <li class="li liexpand">The documentation for the <samp class="ph codeph">CUDNN_BACKEND_OPERATION_MATMUL_DESCRIPTOR</samp>,
                                    							<samp class="ph codeph">CUDNN_BACKEND_OPERATION_RESAMPLE_FWD_DESCRIPTOR</samp>, and
                                    							<samp class="ph codeph">CUDNN_BACKEND_OPERATION_RESAMPLE_BWD_DESCRIPTOR</samp> in the
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnn-backend-api" target="_blank" shape="rect">NVIDIA cuDNN API Reference</a> has
                                    						been improved upon for clarity.
                                 </li>
                                 <li class="li liexpand">Use of <samp class="ph codeph">CUDNN_TENSOR_NCHW_VECT_C</samp> with
                                    							<samp class="ph codeph">cudnnReorderFilterAndBias()</samp> could generate incorrect
                                    						results when the reordered filter data was used incorrectly within cuDNN.
                                    						Direct use of <samp class="ph codeph">cudnnConvolutionForward()</samp> or
                                    							<samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp> without
                                    							<samp class="ph codeph">cudnnReorderFilterAndBias()</samp> was unaffected by this
                                    						issue.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.3.0, there was an overall ~5% regression on
                                    							<samp class="ph codeph">convBiasAct</samp> layers on PG199/PG189. The maximum
                                    						performance regression was around 3x for a select few cases. This issue has
                                    						been fixed in this release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-832__section_t2m_s5p_nnb"><a name="rel-832__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel-832__ul_mbs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-832__ul_mbs_qgh_fsb">
                                 <li class="li liexpand">cuDNN may return <samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> from
                                    							<samp class="ph codeph">cudnnConvolutionForward()</samp>,
                                    							<samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp>, or
                                    							<samp class="ph codeph">cudnnConvolutionBackwardData()</samp> when computing
                                    						convolutions with large spatial dimensions and batch sizes. This issue will
                                    						be addressed in a future release so that such problems will instead return
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> where applicable.
                                 </li>
                                 <li class="li liexpand">Versions of cuDNN before the 8.0 release series do not support the NVIDIA Ampere
                                    						Architecture and will generate incorrect results if used on that
                                    						architecture.
                                 </li>
                                 <li class="li liexpand">Data gradient <samp class="ph codeph">backendEngine</samp> 25 (which is part of legacy
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp>) does not support
                                    						tensors in which the product N*C*H*W of the output gradient tensor equals to
                                    						or exceeds 2^31. This issue has been present in all previous releases of
                                    						cuDNN and exercising the use case for the engine results in incorrect
                                    						results.
                                 </li>
                                 <li class="li liexpand">The documentation for <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnReorderFilterAndBias" target="_blank" shape="rect"><samp class="ph codeph">cudnnReorderFilterAndBias()</samp></a> needs some
                                    						corrections for clarity.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX
                                    						3090 compared to 2080 Ti. This includes EfficientNet with up to 6x
                                    						performance difference, UNet up to 1.6x performance difference and Tacotron
                                    						up to 1.6x performance difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    						<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there are known performance regressions up to 2x on select
                                    						configurations for AlexNet-like models on NVIDIA Turing GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior, accumulator data
                                    						types, and so on, have not been documented as of yet.
                                 </li>
                                 <li class="li liexpand">It is possible, starting in cuDNN 7.6 and up to but not including 8.1.1, to
                                    						leak memory when computing common convolution operations in rare cases.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN version 8.0.2, there is a known 3x performance regression for a single
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> use
                                    						case.
                                 </li>
                                 <li class="li liexpand">CUDA streams internal to cuDNN are not guaranteed to have the same priority
                                    						as the user stream that is set by <samp class="ph codeph">cudnnSetStream()</samp>. We
                                    						recently discovered some issues that break our ability to document
                                    						exceptions to this clearly.
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not
                                    						required to consider padding. Users of cuDNN can witness an unexpected lack
                                    						of problem support when forward convolution spatial dimensions are less than
                                    						the filter size and padding is nonzero, however, is sufficient to extend
                                    						spatial dimensions to or beyond filter dimensions. This is commonly observed
                                    						with, but not limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingBackward()</samp></a> allows both
                                    							<samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp> data pointers (together with
                                    						the related tensor descriptor handles) to be <samp class="ph codeph">NULL</samp> for
                                    						avg-pooling. This could save memory footprint and bandwidth.
                                 </li>
                                 <li class="li liexpand">Users of the static library requiring the best possible convolution performance should use
                                    						whole-archive linking with the <samp class="ph codeph">cnn_infer</samp> and
                                    							<samp class="ph codeph">cnn_train</samp> static sub libraries. This will come at a
                                    						cost to the binary size of the application. This linkage requirement will be
                                    						relaxed in a future release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-832__section_dph_zkv_jrb"><a name="rel-832__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, see the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-832__section_nbs_qgh_fsb"><a name="rel-832__section_nbs_qgh_fsb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel-832__ul_obs_qgh_fsb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-832__ul_obs_qgh_fsb">
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later; it also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples can crash unless they are installed in a writable location.</li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit non-deterministic behavior when the cuDNN
                                       							library is built with CUDA Toolkit 10.2 or higher. This is the result of
                                       							a new buffer management and heuristics in the cuBLAS library. As
                                       							described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This is
                                       							caused by two buffer sizes (16 KB and 4 MB) used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting for a buffer of
                                       							that size to be released, a smaller buffer may be used with a different
                                       							GPU kernel. The kernel selection may affect numerical results. The user
                                       							can eliminate the non-deterministic behavior of cuDNN RNN and multihead
                                       							attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16 KB each in GPU
                                       							memory while the second setting creates two buffers of 4 MB each. The
                                       							default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors in order to run
                                    						efficiently. As always, cuDNN recommends users to align tensors to 16 byte
                                    						boundaries that will be sufficiently aligned for any computational option in
                                    						cuDNN. Doing otherwise may cause performance regressions in cuDNN 8.0.x
                                    						compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and the output is
                                    						in FP16 (half float), there are cases where the numerical accuracy between
                                    						the different algorithms might differ. cuDNN 8.0.x users can target the
                                    						backend API to query the numerical notes of the algorithms to get the
                                    						information programmatically. There are cases where algo0 and algo1 will
                                    						have a reduced precision accumulation when users target the legacy API. In
                                    						all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and backward
                                    						filter, grouped convolution with groups larger than 1 and with odd product
                                    						of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D convolution),
                                    							<samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on devices
                                    						older than NVIDIA Volta. To prevent a potential illegal memory access by an
                                    						instruction that only has a 16-bit version in NVIDIA Volta and later, pad at
                                    						least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use texture-based load
                                    						structure for performance improvements particularly in older hardware
                                    						architectures. Users can opt out of using texture using the environmental
                                    						variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this variable is
                                    						removed. Texture loading is turned off by default. Users who want to
                                    						continue to use texture-based load, can adapt the new backend API, and
                                    						toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check API (for example,
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command explicitly to
                                    						resolve the undefined symbols from cuDNN static libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA Pascal and
                                    						later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-832__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-832__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in
                                    						compatibility mode are tested with compute-sanitizer,
                                    							<samp class="ph codeph">cuGetProcAddress</samp> failures with error code 500 will
                                    						arise due to missing functions. This error can be ignored, or suppressed
                                    						with the <samp class="ph codeph">--report-api-errors no</samp> option, as this is due to
                                    						CUDA backward compatibility checking if a function is usable with the CUDA
                                    						toolkit combination. The functions are introduced in a later version of CUDA
                                    						but are not available on the current platform. The absence of these
                                    						functions is harmless and will not give rise to any functional issues.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-832__section_mfp_4g2_xrb"><a name="rel-832__section_mfp_4g2_xrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Deprecated Features</h3>
                           <div class="p">The following features are deprecated in cuDNN 8.3.2:<a name="rel-832__ul_z2h_pg2_xrb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-832__ul_z2h_pg2_xrb">
                                 <li class="li">We are deprecating the reporting of performance results in the Best Practices For Using
                                    						cuDNN 3D Convolutions and will instead update these Release Notes if there
                                    						is anything interesting to report release-over-release. Starting with cuDNN
                                    						8.4.0, this section will be removed. For past performance tables, refer to
                                    						the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a> &gt; Best
                                    						Practices For Using cuDNN 3D Convolutions.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-831"><a name="rel-831" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-831" name="rel-831" shape="rect">1.18.&nbsp;cuDNN Release 8.3.1</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">This is the NVIDIA cuDNN 8.3.1 Release Notes. This release includes fixes from the
                           		previous cuDNN v8.1.x releases as well as the following additional changes. These Release
                           		Notes are applicable to both cuDNN and NVIDIA JetPack™ users of cuDNN unless
                           		appended specifically with <em class="ph i">(not applicable for Jetson platforms)</em>.
                        </p>
                        <div class="section">
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-831__section_mcm_jy3_prb"><a name="rel-831__section_mcm_jy3_prb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Announcements</h3>
                           <div class="p"><a name="rel-831__ul_gv5_jy3_prb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-831__ul_gv5_jy3_prb">
                                 <li class="li liexpand">Debian and RPM local installers are now provided on the <a class="xref" href="https://developer.nvidia.com/cudnn" target="_blank" shape="rect">cuDNN download page</a>. For more information, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html" target="_blank" shape="rect">NVIDIA cuDNN Installation Guide</a>. 
                                 </li>
                                 <li class="li liexpand">Individual cuDNN packages can be found in the CUDA repository:<a class="xref" href="https://developer.download.nvidia.com/compute/cuda/repos/" target="_blank" shape="rect"><u class="ph u">https://developer.download.nvidia.com/compute/cuda/repos/</u></a></li>
                                 <li class="li liexpand">As part of a renewed effort to provide tarball and zip archive deliverables for NVIDIA
                                    						products, the format has changed from existing <samp class="ph codeph">.txz</samp>
                                    						archives. For more information about <samp class="ph codeph">*-archive.tar.xz</samp> and
                                    							<samp class="ph codeph">*-archive.zip</samp> deliverables, refer to the <a class="xref" href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#tarball-zipfile-overview" target="_blank" shape="rect">Tarball and Zip Archive
                                       							Deliverables</a> and the <a class="xref" href="https://github.com/NVIDIA/build-system-archive-import-examples" target="_blank" shape="rect">README for
                                       							build-system-archive-import-examples</a>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-831__section_kqy_ppr_prb"><a name="rel-831__section_kqy_ppr_prb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-831__ul_sgq_rpr_prb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-831__ul_sgq_rpr_prb">
                                 <li class="li">In the runtime fusion engine:<a name="rel-831__ul_wbc_spr_prb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-831__ul_wbc_spr_prb">
                                       <li class="li liexpand">Pointwise logical and comparison operators are now supported,
                                          								including <samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_AND</samp>,
                                          									<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_OR</samp>,
                                          									<samp class="ph codeph">CUDNN_POINTWISE_LOGICAL_NOT</samp>,
                                          									<samp class="ph codeph">CUDNN_POINTWISE_CMP_EQ</samp>,
                                          									<samp class="ph codeph">CUDNN_POINTWISE_CMP_NEQ</samp>,
                                          									<samp class="ph codeph">CUDNN_POINTWISE_CMP_GT</samp>,
                                          									<samp class="ph codeph">CUDNN_POINTWISE_CMP_GE</samp>,
                                          									<samp class="ph codeph">CUDNN_POINTWISE_CMP_LT</samp>, and
                                          									<samp class="ph codeph">CUDNN_POINTWISE_CMP_LE</samp>. As part of this
                                          								feature, support for loading/storing/computing with boolean tensors
                                          								has also been added.
                                       </li>
                                       <li class="li liexpand">Batch support was added for matmul operation. Also, it is allowed to have batch
                                          								broadcasting. The same matrix A or B can be broadcasted across the
                                          								batch for matmul operation.
                                       </li>
                                       <li class="li liexpand">The leading dimension support (reflected in the strides of the tensors) was added for
                                          								matmul operation. It is allowed to compute matmul operation with
                                          								unpacked tensors.
                                       </li>
                                    </ul>
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-831__section_vr4_zbq_prb"><a name="rel-831__section_vr4_zbq_prb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:<a name="rel-831__ul_orw_zbq_prb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-831__ul_orw_zbq_prb">
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBiasActivationForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp></a>
                                    						could in some cases silently apply a ReLU operation when
                                    							<samp class="ph codeph">Identity</samp> was requested. This issue has been fixed in
                                    						this release.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT_TILING</samp>,
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING</samp>, and
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING</samp> could exhibit
                                    						illegal memory access in cuDNN v8 releases. This issue has been fixed in
                                    						this release.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0</samp> was wrongly marked with
                                    						numerical note
                                    							<samp class="ph codeph">CUDNN_NUMERICAL_NOTE_REDUCED_PRECISION_REDUCTION</samp> when
                                    						output data type is float or double. This issue has been fixed in this
                                    						release.
                                 </li>
                                 <li class="li liexpand">There was an error in the documentation for determinism of <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> by algo.
                                    						This issue has been corrected in this release.
                                 </li>
                                 <li class="li liexpand">When the user selected <samp class="ph codeph">algo0</samp> (<samp class="ph codeph">CUDNN_RNN_ALGO_STANDARD</samp>) in
                                    							<samp class="ph codeph">cudnnRNNBackwardData_v8()</samp> or invoked the legacy
                                    						functions, such as <samp class="ph codeph">cudnnRNNBackwardDataEx()</samp>,
                                    							<samp class="ph codeph">cudnnRNNBackwardData()</samp>, and the number of RNN layers
                                    						was more than eight in a unidirectional model or more than four in a
                                    						bidirectional model, then some internal streams used to parallelize
                                    						computations may be default streams (aka stream <samp class="ph codeph">0</samp>). The
                                    						computational performance would most likely be affected in those cases. This
                                    						issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Calling <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSoftmaxForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSoftmaxForward()</samp></a> with
                                    							<samp class="ph codeph">CUDNN_SOFTMAX_MODE_CHANNEL</samp> mode and N==1 in NCHW layout
                                    						would result in incorrect results in cuDNN 8.3.0. This has been fixed in
                                    						this release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-831__section_t2m_s5p_nnb"><a name="rel-831__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX 3090 compared to
                                    						2080 Ti. This includes EfficientNet with up to 6x performance difference,
                                    						UNet up to 1.6x performance difference and Tacotron up to 1.6x performance
                                    						difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    						<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there are known performance regressions up to 2x on select
                                    						configurations for AlexNet-like models on NVIDIA Turing GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior, accumulator data
                                    						types, and so on, have not been documented as of yet.
                                 </li>
                                 <li class="li liexpand">It is possible, starting in cuDNN 7.6 and up to but not including 8.1.1, to leak memory
                                    						when computing common convolution operations in rare cases.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD model on the
                                    						NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD models running
                                    						on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph capture.</li>
                                 <li class="li liexpand">Compared to cuDNN version 8.0.2, there is a known 3x performance regression for a single
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> use
                                    						case.
                                 </li>
                                 <li class="li liexpand">In general, the internal CUDA streams inside cuDNN will have the same priority as the user
                                    						stream that is set by <samp class="ph codeph">cudnnSetStream()</samp> (instead of always
                                    						having default priority). There are two exceptions:<a name="rel-831__ol_tv5_hzd_qrb" shape="rect">
                                       <!-- --></a><ol class="ol" id="rel-831__ol_tv5_hzd_qrb">
                                       <li class="li">When the user stream is in capture mode (that is,
                                          									<samp class="ph codeph">cudaStreamCaptureStatusActive==1</samp>), the
                                          								cuDNN-owned streams will still have default priority, and
                                       </li>
                                       <li class="li">RNN functions <samp class="ph codeph">cudnnRNNForward()</samp>,
                                          									<samp class="ph codeph">cudnnRNNBackwardData_v8()</samp>,
                                          									<samp class="ph codeph">cudnnRNNBackwardWeights_v8()</samp>, and their legacy
                                          								counterparts still use default priority CUDA streams or higher
                                          								priority streams to launch concurrent and cooperative grids.
                                       </li>
                                    </ol>
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not required to consider
                                    						padding. Users of cuDNN can witness an unexpected lack of problem support
                                    						when forward convolution spatial dimensions are less than the filter size
                                    						and padding is nonzero, however, is sufficient to extend spatial dimensions
                                    						to or beyond filter dimensions. This is commonly observed with, but not
                                    						limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingBackward()</samp></a> allows both
                                    							<samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp> data pointers (together with
                                    						the related tensor descriptor handles) to be <samp class="ph codeph">NULL</samp> for
                                    						avg-pooling. This could save memory footprint and bandwidth.
                                 </li>
                                 <li class="li liexpand">Users of the static library requiring the best possible convolution performance should use
                                    						whole-archive linking with the <samp class="ph codeph">cnn_infer</samp> and
                                    							<samp class="ph codeph">cnn_train</samp> static sub libraries. This will come at a
                                    						cost to the binary size of the application. This linkage requirement will be
                                    						relaxed in a future release.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.3.0, there is an overall ~5% regression on convBiasAct
                                    						layers on PG199/PG189. The maximum performance regression is around 3x for a
                                    						select few cases.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-831__section_dph_zkv_jrb"><a name="rel-831__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, see the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Limitations</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later; it also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples can crash unless they are installed in a writable location.</li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit non-deterministic behavior when the cuDNN
                                       							library is built with CUDA Toolkit 10.2 or higher. This is the result of
                                       							a new buffer management and heuristics in the cuBLAS library. As
                                       							described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This is
                                       							caused by two buffer sizes (16 KB and 4 MB) used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting for a buffer of
                                       							that size to be released, a smaller buffer may be used with a different
                                       							GPU kernel. The kernel selection may affect numerical results. The user
                                       							can eliminate the non-deterministic behavior of cuDNN RNN and multihead
                                       							attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16 KB each in GPU
                                       							memory while the second setting creates two buffers of 4 MB each. The
                                       							default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors in order to run
                                    						efficiently. As always, cuDNN recommends users to align tensors to 16 byte
                                    						boundaries that will be sufficiently aligned for any computational option in
                                    						cuDNN. Doing otherwise may cause performance regressions in cuDNN 8.0.x
                                    						compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and the output is
                                    						in FP16 (half float), there are cases where the numerical accuracy between
                                    						the different algorithms might differ. cuDNN 8.0.x users can target the
                                    						backend API to query the numerical notes of the algorithms to get the
                                    						information programmatically. There are cases where algo0 and algo1 will
                                    						have a reduced precision accumulation when users target the legacy API. In
                                    						all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and backward
                                    						filter, grouped convolution with groups larger than 1 and with odd product
                                    						of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D convolution),
                                    							<samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on devices
                                    						older than NVIDIA Volta. To prevent a potential illegal memory access by an
                                    						instruction that only has a 16-bit version in NVIDIA Volta and later, pad at
                                    						least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use texture-based load
                                    						structure for performance improvements particularly in older hardware
                                    						architectures. Users can opt out of using texture using the environmental
                                    						variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this variable is
                                    						removed. Texture loading is turned off by default. Users who want to
                                    						continue to use texture-based load, can adapt the new backend API, and
                                    						toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check API (for example,
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command explicitly to
                                    						resolve the undefined symbols from cuDNN static libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA Pascal and
                                    						later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-831__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-831__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in
                                    						compatibility mode are tested with compute-sanitizer,
                                    							<samp class="ph codeph">cuGetProcAddress</samp> failures with error code 500 will
                                    						arise due to missing functions. This error can be ignored, or suppressed
                                    						with the <samp class="ph codeph">--report-api-errors no</samp> option, as this is due to
                                    						CUDA backward compatibility checking if a function is usable with the CUDA
                                    						toolkit combination. The functions are introduced in a later version of CUDA
                                    						but are not available on the current platform. The absence of these
                                    						functions is harmless and will not give rise to any functional issues.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-830"><a name="rel-830" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-830" name="rel-830" shape="rect">1.19.&nbsp;cuDNN Release 8.3.0</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">This is the NVIDIA cuDNN 8.3.0 release notes. This release includes fixes from the
                           		previous cuDNN v8.1.x releases as well as the following additional changes. These release
                           		notes are applicable to both cuDNN and NVIDIA JetPack™ users of cuDNN unless
                           		appended specifically with <em class="ph i">(not applicable for Jetson platforms)</em>.
                        </p>
                        <div class="section">
                           <p class="p">For previously released cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/archives/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-830__section_rjp_fpr_grb"><a name="rel-830__section_rjp_fpr_grb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Announcements</h3>
                           <div class="p"><a name="rel-830__ul_spz_fpr_grb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-830__ul_spz_fpr_grb">
                                 <li class="li liexpand">cuDNN version 8.3.0 depends on cuBLAS as a shared library dependency.</li>
                                 <li class="li liexpand">The cuDNN version 8.3.0 <samp class="ph codeph">libcudnn_static.a</samp> deliverable is
                                    						replaced with the following:<a name="rel-830__ul_mfj_3pr_grb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-830__ul_mfj_3pr_grb">
                                       <li class="li"><samp class="ph codeph">libcudnn_ops_infer_static.a </samp></li>
                                       <li class="li"><samp class="ph codeph">libcudnn_ops_train_static.a</samp></li>
                                       <li class="li"><samp class="ph codeph">libcudnn_cnn_infer_static.a</samp></li>
                                       <li class="li"><samp class="ph codeph">libcudnn_cnn_train_static.a</samp></li>
                                       <li class="li"><samp class="ph codeph">libcudnn_adv_infer_static.a</samp></li>
                                       <li class="li"><samp class="ph codeph">libcudnn_adv_train_static.a </samp></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand">cuDNN version 8.3.0 depends on zlib as a shared library dependency. Refer to the zlib
                                    						instructions in the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html" target="_blank" shape="rect">NVIDIA cuDNN Installation Guide</a>
                                    						for instructions.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-830__section_r2f_xkm_jrb"><a name="rel-830__section_r2f_xkm_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-830__ul_skb_ykm_jrb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-830__ul_skb_ykm_jrb">
                                 <li class="li liexpand">WSL 2 is released as a preview feature in this cuDNN 8.3.0.</li>
                                 <li class="li liexpand">Various improvements were made in our multihead attention API:<a name="rel-830__ul_kmn_mmm_jrb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-830__ul_kmn_mmm_jrb">
                                       <li class="li">Added HSH support - FP16 data type with FP32 math precision. Allow
                                          								to achieve FP16 mixed-precision Tensor Core performance without
                                          								sacrificing accuracy.
                                       </li>
                                       <li class="li">Added support to bias gradient computation. Before cuDNN 8.3.0, bias
                                          								was supported only for inference.
                                       </li>
                                       <li class="li">Multihead attention has two dropout layers active in training mode.
                                          								The first dropout operation is applied directly to the softmax
                                          								output. The second dropout operation alters the multihead attention
                                          								output, just before the point where residual connections are added.
                                          								Before cuDNN 8.3.0, only the first dropout layer was supported.
                                       </li>
                                       <li class="li">Significant performance improvement out of the box (no changes are
                                          								required from users) for both multihead attention forward and
                                          								backward paths.
                                       </li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand">Various improvements were made in our runtime fusion engine:<a name="rel-830__ul_zxy_yx5_jrb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-830__ul_zxy_yx5_jrb">
                                       <li class="li">The cuDNN runtime fusion engine now supports resample operations of upsample and
                                          								downsample. Support has been added for 2*2 average pooling with
                                          								stride 2 and upsample by a factor of 2 using bilinear interpolation.
                                          								The datatype supported is FP32 and the compute datatype is FP32. The
                                          								resample operations can also be fused with other operations provided
                                          								the input to the resample operation is located in global
                                          								memory.
                                       </li>
                                       <li class="li">The cuDNN runtime fusion engine is now generalized to accurately obey the intermediate
                                          								storage datatype users specified in the operation graph. The support
                                          								datatypes include INT8, BF16, FP16, INT32, FP32. As a general rule,
                                          								we recommend users to use FP32 as the intermediate storage type that
                                          								provides balanced numerical precision and performance.
                                       </li>
                                       <li class="li">The cuDNN runtime fusion engine now supports batched matmul
                                          								operation with row/column broadcast or row/column reduction
                                          								operations in the epilog.
                                       </li>
                                       <li class="li">The cuDNN runtime fusion engine now does numeric clamping while converting from a data
                                          								type with a larger dynamic range to one with a relatively smaller
                                          								dynamic range to avoid numeric overflows at all times.
                                       </li>
                                       <li class="li">The cuDNN runtime fusion engine extends the support for broadcast
                                          								pointwise operations in the epilogue to now include those between a
                                          								tensor and a scalar value as well.
                                       </li>
                                       <li class="li">Extended general fusion heuristic support to convolution forward and backward data and
                                          								weight gradient operation patterns, with FP16, TF32, INT8 I/O data
                                          								types, to ensure a good heuristic selection to improve
                                          								out-of-the-box performance.
                                       </li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand">Added more detailed error reporting that is accessible from the existing API log or the
                                    						logging callback function. Error and warning severity levels are added into
                                    						the error reporting. Environment variables <samp class="ph codeph">CUDNN_LOGERR_DBG</samp>
                                    						and <samp class="ph codeph">CUDNN_LOGWARN_DBG</samp> can be used to enable these severity
                                    						levels respectively. Within these error severity levels, the error or
                                    						warning message will now include a traceback of the error conditions
                                    						triggered the error as hints for troubleshooting purposes.
                                 </li>
                                 <li class="li liexpand">Engine heuristics now supports a new mode called <samp class="ph codeph">HEUR_MODE_FALLBACK</samp> which
                                    						gives a list of engine configurations that run most of the convolution
                                    						problems without the performance guarantee. Use this mode when all engines
                                    						suggested by heuristics are not supported.
                                 </li>
                                 <li class="li liexpand">In prior cuDNN versions, certain engines required reordered filters for int8x32 format, but
                                    						there was no way to disambiguate whether the filter was reordered. Engines
                                    						that require reordered filters now have a new behavior note
                                    							<samp class="ph codeph">CUDNN_BEHAVIOR_NOTE_REQUIRES_FILTER_REORDER</samp> which
                                    						specifies the tensors must be reordered before being passed to the
                                    						engine.
                                 </li>
                                 <li class="li liexpand">RNN functions <samp class="ph codeph">cudnnRNNBackwardData_v8()</samp>,
                                    							<samp class="ph codeph">cudnnRNNBackwardDataEx()</samp>, and
                                    							<samp class="ph codeph">cudnnRNNBackwardData()</samp>have been improved to internally
                                    						invoke the cooperative group API
                                    							<samp class="ph codeph">cudaLaunchCooperativeKernel()</samp> to launch GPU kernels
                                    						when threads must synchronize across all CUDA<sup>®</sup> thread
                                    						blocks of a grid. Starting in CUDA 11.2, the
                                    							<samp class="ph codeph">cudaLaunchCooperativeKernel()</samp> function is able to run
                                    						multiple cooperative grids concurrently in multiple streams. This feature
                                    						has been used in <samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_STATIC</samp> and
                                    							<samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_DYNAMIC</samp> algorithms to improve
                                    						the computational performance. A method of launching these types of kernels
                                    						using <samp class="ph codeph">cudaLaunchCooperativeKernel()</samp> is more robust in
                                    						preventing potential deadlocks when in rare scenarios when multiple
                                    						cooperative grids are launched concurrently.
                                    <p class="p">cuDNN 8.3 compiled with CUDA
                                       							10.2 must still rely on a regular method of launching kernels. Deadlocks
                                       							are mitigated by employing higher priority CUDA streams. Currently,
                                       								<samp class="ph codeph">cuDNN RNN APIs still</samp> use higher priority streams,
                                       							however, in future cuDNN versions, priorities of auxiliary streams will
                                       							match the priority of the user stream defined by the
                                       								<samp class="ph codeph">cudnnSetStream()</samp> call. Future cuDNN versions will
                                       							also use the <samp class="ph codeph">cudaLaunchCooperativeKernel()</samp> API to
                                       							launch cooperative grids in forward RNN functions such as
                                       								<samp class="ph codeph">cudnnRNNForward()</samp>.
                                    </p>
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-830__section_hvm_1gw_drb"><a name="rel-830__section_hvm_1gw_drb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:<a name="rel-830__ul_svg_bgw_drb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-830__ul_svg_bgw_drb">
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> did not support some tensor shapes that
                                    						were previously specified as supported. This issue has been fixed in this
                                    						release.
                                 </li>
                                 <li class="li liexpand">When using the cuDNN CTC Loss API function, the computed gradients array was
                                    						not zero initialized. This meant it was possible the gradients array
                                    						returned a mix of valid values and uninitialized values. This issue has been
                                    						fixed in this release.
                                 </li>
                                 <li class="li liexpand">Compared to version 8.0.5, legacy convolution APIs increased CPU
                                    						computational costs. On x86, this was measured to be as high as 10
                                    						microseconds. This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnSetStream()</samp> API was generating errors when graph
                                    						capture was enabled. This issue is fixed in this release.
                                 </li>
                                 <li class="li liexpand">There was a known 60% performance regression for ResNet-50 on the GTX 1080
                                    						when run using FP16 data with large batch sizes (over 128). This regression
                                    						has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">In some cases, <samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> generates
                                    						numerically imprecise results when used with algo set to
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1</samp>. This issue is most
                                    						frequently encountered with three-dimensional spatial tensors. Users of the
                                    						backend API may explicitly avoid backend engine 2032 or consider the
                                    						numerical notes of engines and reject any marked as offering reduced
                                    						precision reduction
                                    							(<samp class="ph codeph">CUDNN_NUMERICAL_NOTE_REDUCED_PRECISION_REDUCTION</samp>).
                                 </li>
                                 <li class="li liexpand">For <samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp>, there was
                                    						previously a restriction on aliasing device memory pointers labeled
                                    							<samp class="ph codeph">X</samp> and <samp class="ph codeph">Z</samp> in the documentation of that
                                    						function. This restriction has been relaxed so that <samp class="ph codeph">X</samp> may
                                    						alias <samp class="ph codeph">Z</samp> by pointing to the same device memory location if
                                    						desired. Note that the restriction against aliasing the pointers labeled
                                    							<samp class="ph codeph">X</samp> and <samp class="ph codeph">Y</samp> remains.
                                 </li>
                                 <li class="li liexpand">cuDNN may be observed to contain a small leak related to the use of dlopen.
                                    						Currently, this is believed to be a false positive when indicated by
                                    						valgrind. Should this thinking change, the known issues of this document
                                    						will reflect that understanding in subsequent releases.
                                 </li>
                                 <li class="li liexpand">Previously, on NVIDIA Pascal and Maxwell architectures, users of cuDNN's
                                    						8.X's backend engine 34 with <samp class="ph codeph">CUDNN_CONVOLUTION</samp> mode set for
                                    						forward convolution could witness-illegal memory access when this engine is
                                    						specifically selected outside of heuristic query. This issue has been fixed
                                    						in this release.
                                 </li>
                                 <li class="li liexpand">Previously, on K80 GPUs when <samp class="ph codeph">cudnnConvolutionForward()</samp> is
                                    						used with <samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp>
                                    						algorithm and half I/O data types a silent error might occur when the output
                                    						width Q is 1 and both height and width padding are zero; this particular
                                    						case will now be rejected by cuDNN as not supported in this and all other
                                    						successive releases for this GPU architecture.
                                 </li>
                                 <li class="li liexpand">cuDNN does not package <samp class="ph codeph">libfreeimg</samp> as a static library for
                                    						users of cuDNN's MNIST sample code. The included <samp class="ph codeph">readme.txt</samp>
                                    						file contains instructions on where to locate this dependency and how to
                                    						compile and link this sample.
                                 </li>
                                 <li class="li liexpand">The parameters section for <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardInference" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardInference()</samp></a>
                                    						has been updated to reflect a correct <samp class="ph codeph">*y</samp> description.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6.5, there was a known performance regression on various
                                    						convolutional models using INT8 data types on NVIDIA Volta GPUs. This issue
                                    						has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">A memory leak as well as a possible delayed memory deallocation in the cuDNN
                                    						runtime fusion engine have been fixed.
                                 </li>
                                 <li class="li liexpand">In previous releases of cuDNN 8, user applications might crash in rare
                                    						instances due to large stack allocation requirements; this issue is fixed in
                                    						this release by preferring heap allocation in cases where large stack
                                    						allocations were previously occurring.
                                 </li>
                                 <li class="li liexpand">Some dgrad batchnorm fusion engines were previously not supported on
                                    						Windows. We now support this starting in cuDNN 8.3.0.
                                 </li>
                                 <li class="li liexpand">The documentation for <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnReorderFilterAndBias" target="_blank" shape="rect"><samp class="ph codeph">cudnnReorderFilterAndBias()</samp></a> needed some
                                    						corrections for clarity. The topic has been updated in this release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-830__section_t2m_s5p_nnb"><a name="rel-830__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">When the user selects <samp class="ph codeph">algo0</samp> (<samp class="ph codeph">CUDNN_RNN_ALGO_STANDARD</samp>) in
                                    							<samp class="ph codeph">cudnnRNNBackwardData_v8()</samp> or invokes the legacy
                                    						functions, such as <samp class="ph codeph">cudnnRNNBackwardDataEx()</samp>,
                                    							<samp class="ph codeph">cudnnRNNBackwardData()</samp>, and the number of RNN layers is
                                    						more than eight in a unidirectional model or more than four in a
                                    						bidirectional model, then some internal streams used to parallelize
                                    						computations may be default streams (aka stream <samp class="ph codeph">0</samp>). RNN
                                    							<samp class="ph codeph">algo0</samp> dgrad APIs will not crash and the numerical
                                    						results will be correct but the computational performance will likely be
                                    						affected in those cases.
                                 </li>
                                 <li class="li liexpand">Users of the static library requiring best possible convolution performance should use
                                    						whole-archive linking. This will come at a cost to binary size that will
                                    						require resolution in future releases, either through static sub libraries
                                    						or relaxing the whole-archive linkage requirement altogether.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX 3090 compared to
                                    						2080 Ti. This includes EfficientNet with up to 6x performance difference,
                                    						UNet up to 1.6x performance difference and Tacotron up to 1.6x performance
                                    						difference.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    						<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there are known performance regressions up to 2x on select
                                    						configurations for AlexNet-like models on NVIDIA Turing GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior, accumulator data
                                    						types, and so on, have not been documented as of yet. 
                                 </li>
                                 <li class="li liexpand">It is possible, starting in cuDNN 7.6 and up to but not including 8.1.1, to
                                    						leak memory when computing common convolution operations in rare cases.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture. 
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN version 8.0.2, there is a known 3x performance regression for a single
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> use
                                    						case.
                                 </li>
                                 <li class="li liexpand">The internal CUDA streams inside cuDNN 8.3.0 will have the same priority as the user stream
                                    						that is set by <samp class="ph codeph">cudnnSetStream()</samp> (instead of always having
                                    						default priority). There are two limitations:<a name="rel-830__ol_k5x_jgw_drb" shape="rect">
                                       <!-- --></a><ol class="ol" id="rel-830__ol_k5x_jgw_drb">
                                       <li class="li">When the user stream is in capture mode (that is,
                                          									<samp class="ph codeph">cudaStreamCaptureStatusActive==1</samp>), the
                                          								cuDNN-owned streams will still have default priority, and
                                       </li>
                                       <li class="li">RNN functions <samp class="ph codeph">cudnnRNNForward()</samp>,
                                          									<samp class="ph codeph">cudnnRNNBackwardData_v8()</samp>,
                                          									<samp class="ph codeph">cudnnRNNBackwardWeights_v8()</samp>, and their legacy
                                          								counterparts still use default priority CUDA streams or higher
                                          								priority streams to launch concurrent and cooperative grids.
                                       </li>
                                    </ol>
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not
                                    						required to consider padding. Users of cuDNN can witness an unexpected lack
                                    						of problem support when forward convolution spatial dimensions are less than
                                    						the filter size and padding is nonzero, however, is sufficient to extend
                                    						spatial dimensions to or beyond filter dimensions. This is commonly observed
                                    						with, but not limited to, INT8 convolution kernels.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingBackward()</samp></a> allows both
                                    							<samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp> data pointers (together with
                                    						the related tensor descriptor handles) to be <samp class="ph codeph">NULL</samp> for
                                    						avg-pooling. This could save memory footprint and bandwidth.
                                 </li>
                                 <li class="li liexpand">When applications using cuDNN with an older 11.x CUDA toolkit in compatibility mode are
                                    						tested with compute-sanitizer, <samp class="ph codeph">cuGetProcAddress</samp> failures
                                    						with error code 500 will arise due to missing functions. This error can be
                                    						ignored, or suppressed with the <samp class="ph codeph">--report-api-errors no</samp>
                                    						option, as this is due to CUDA backward compatibility checking if a function
                                    						is usable with the CUDA toolkit combination. The functions are introduced in
                                    						a later version of CUDA but are not available on the current platform. The
                                    						absence of these functions is harmless and will not give rise to any
                                    						functional issues.
                                 </li>
                                 <li class="li liexpand">Calling <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSoftmaxForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSoftmaxForward()</samp></a> with
                                    							<samp class="ph codeph">CUDNN_SOFTMAX_MODE_CHANNEL</samp> mode and N==1 in NCHW layout
                                    						may result in incorrect results. This will be fixed in the next
                                    						release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-830__section_dph_zkv_jrb"><a name="rel-830__section_dph_zkv_jrb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, see the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Limitations</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later; it also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples can crash unless they are installed in a writable location.</li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit non-deterministic behavior when the cuDNN
                                       							library is built with CUDA Toolkit 10.2 or higher. This is the result of
                                       							a new buffer management and heuristics in the cuBLAS library. As
                                       							described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This is
                                       							caused by two buffer sizes (16 KB and 4 MB) used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting for a buffer of
                                       							that size to be released, a smaller buffer may be used with a different
                                       							GPU kernel. The kernel selection may affect numerical results. The user
                                       							can eliminate the non-deterministic behavior of cuDNN RNN and multihead
                                       							attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16 KB each in GPU
                                       							memory while the second setting creates two buffers of 4 MB each. The
                                       							default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors in order to run
                                    						efficiently. As always, cuDNN recommends users to align tensors to 16 byte
                                    						boundaries that will be sufficiently aligned for any computational option in
                                    						cuDNN. Doing otherwise may cause performance regressions in cuDNN 8.0.x
                                    						compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and the output is
                                    						in FP16 (half float), there are cases where the numerical accuracy between
                                    						the different algorithms might differ. cuDNN 8.0.x users can target the
                                    						backend API to query the numerical notes of the algorithms to get the
                                    						information programmatically. There are cases where algo0 and algo1 will
                                    						have a reduced precision accumulation when users target the legacy API. In
                                    						all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and backward
                                    						filter, grouped convolution with groups larger than 1 and with odd product
                                    						of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D convolution),
                                    							<samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on devices
                                    						older than NVIDIA Volta. To prevent a potential illegal memory access by an
                                    						instruction that only has a 16-bit version in NVIDIA Volta and later, pad at
                                    						least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">
                                    <div class="p">Several cuDNN APIs are unable to directly support computations using integer types
                                       								(<samp class="ph codeph">CUDNN_DATA_INT8</samp>,
                                       								<samp class="ph codeph">CUDNN_DATA_INT8x4</samp>,
                                       								<samp class="ph codeph">CUDNN_DATA_INT8x32</samp> or
                                       								<samp class="ph codeph">CUDNN_DATA_INT32</samp>). Floating types (particularly
                                       								<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>) are much more widely supported.
                                       							If an API does not support the desired type, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnTransformTensor" target="_blank" shape="rect"><samp class="ph codeph">cudnnTransformTensor()</samp></a> can be used to
                                       							support the use case by converting to/from a supported type and the
                                       							desired type. Here are the steps for doing so:<a name="rel-830__ol_hz5_r31_dmb" shape="rect">
                                          <!-- --></a><ol class="ol" id="rel-830__ol_hz5_r31_dmb">
                                          <li class="li">Convert all input tensors from their native type to a supported
                                             									type (<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> is recommended).
                                          </li>
                                          <li class="li">Run cuDNN API using the converted input tensors and output
                                             									tensor descriptors set as
                                             									<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>.
                                          </li>
                                          <li class="li">Convert all output tensors from a supported type to your desired
                                             									output type.
                                          </li>
                                       </ol>
                                       <div class="note note"><span class="notetitle">Note:</span> This will require extra memory use for the temporary buffers.
                                          								Further, this will introduce an additional round trip to memory that
                                          								might noticeably impact performance.
                                       </div>
                                    </div>
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use texture-based load
                                    						structure for performance improvements particularly in older hardware
                                    						architectures. Users can opt out of using texture using the environmental
                                    						variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this variable is
                                    						removed. Texture loading is turned off by default. Users who want to
                                    						continue to use texture-based load, can adapt the new backend API, and
                                    						toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check API (for example,
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command explicitly to
                                    						resolve the undefined symbols from cuDNN static libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                                 <li class="li liexpand">The spatial persistent batch normalization API is only available for NVIDIA Pascal and
                                    						later architectures. Pre-Pascal architectures return
                                    							<samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> instead. The affected APIs
                                    							include:<a name="rel-830__ul_c4p_2vy_zqb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-830__ul_c4p_2vy_zqb">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationBackwardExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetBatchNormalizationTrainingExReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationBackwardWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationBackwardWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationForwardTrainingWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationForwardTrainingWorkspaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetNormalizationTrainingReserveSpaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetNormalizationTrainingReserveSpaceSize()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationBackward()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnNormalizationForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnNormalizationForwardTraining()</samp></a></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnAddTensor()</samp> performance may regress from 8.2 to 8.3
                                    						for pre-Pascal architectures.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-824"><a name="rel-824" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-824" name="rel-824" shape="rect">1.20.&nbsp;cuDNN Release 8.2.4</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">This is the cuDNN 8.2.4 release notes. This release includes fixes from the previous
                           		cuDNN v8.1.x releases as well as the following additional changes. These release notes are
                           		applicable to both cuDNN and NVIDIA JetPack users of cuDNN unless appended specifically with
                           			<em class="ph i">(not applicable for Jetson platforms)</em>.
                        </p>
                        <div class="section" id="rel-824__section_l2m_s4c_2jb"><a name="rel-824__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">For previous cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-archived/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-824__section_t2m_s5p_nnb"><a name="rel-824__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">Users of the static library requiring best possible convolution performance should use
                                    						whole-archive linking. This will come at a cost to binary size that will
                                    						require resolution in future releases, either through static sub libraries
                                    						or relaxing the whole-archive linkage requirement altogether.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX 3090 compared to
                                    						2080 Ti. This includes EfficientNet with up to 6x performance difference,
                                    						UNet up to 1.6x performance difference and Tacotron up to 1.6x performance
                                    						difference.
                                 </li>
                                 <li class="li liexpand">Compared to version 8.0.5, legacy convolution APIs have increased CPU
                                    						computational costs. On x86, this has been measured to be as high as 10
                                    						microseconds.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnAddTensor" target="_blank" shape="rect"><samp class="ph codeph">cudnnAddTensor()</samp></a>
                                    						does not support all tensor shapes even though the cuDNN documentation says
                                    						otherwise.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    						<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there are known performance regressions up to 2x on select
                                    						configurations for AlexNet-like models on NVIDIA Turing GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6.5, there are known performance regressions on various
                                    						convolutional models using INT8 data types on NVIDIA Volta GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior, accumulator data
                                    						types, and so on, have not been documented as of yet. 
                                 </li>
                                 <li class="li liexpand">It is possible, starting in cuDNN 7.6 and up to but not including 8.1.1, to
                                    						leak memory when computing common convolution operations in rare cases.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">The documentation for <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnReorderFilterAndBias" target="_blank" shape="rect"><samp class="ph codeph">cudnnReorderFilterAndBias()</samp></a> needs some
                                    						corrections for clarity. 
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">NVIDIA Turing users of cuDNN can observe intermittent illegal memory access
                                    						errors for some convolution workloads. 
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture. 
                                 </li>
                                 <li class="li liexpand">In a multi-GPU setting, with complex scheduling, cuDNN can segfault. It is
                                    						not clear that this is a cuDNN issue, but the issue is under active
                                    						investigation so that the known limitations section of this document can be
                                    						updated in a future release.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN version 8.0.2, there is a known 3x performance regression for a single
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> use
                                    						case.
                                 </li>
                                 <li class="li liexpand">There is a known 60% performance regression for ResNet-50 on the GTX 1080 when run using
                                    						FP16 data with large batch sizes (over 128).
                                 </li>
                                 <li class="li liexpand">Users of the static library requiring the best possible convolution performance should use
                                    						whole-archive linking. This will come at a cost to the binary size that will
                                    						require resolution in a future release, either through static sub libraries
                                    						or relaxing the whole-archive linkage requirement altogether.
                                 </li>
                                 <li class="li liexpand">cuDNN may contain a small memory leak related to the usage of
                                    							<samp class="ph codeph">dlopen()</samp> within the library; this is not confirmed but
                                    						currently under investigation. The possible leak does not affect Windows
                                    						users or users of the static library.
                                 </li>
                                 <li class="li liexpand">On NVIDIA Pascal and Maxwell architectures, users of cuDNN's 8.0 backend engine 34 for
                                    						forward convolution can witness-illegal memory access when this engine is
                                    						specifically selected outside of heuristic query. Heuristics users of these
                                    						architectures will not witness this issue, as happened in previous versions.
                                    						The possibility of the illegal memory access affects all previous versions
                                    						of cuDNN 8.0 and will be fixed in a future release.
                                 </li>
                                 <li class="li liexpand">When using the cuDNN CTC Loss API function, the computed gradients array is
                                    						not zero initialized. When sequence lengths are exceeded, some gradient
                                    						entries are returned uninitialized.
                                 </li>
                                 <li class="li liexpand">The internal CUDA streams inside cuDNN 8.2.4 will have the same priority (instead of the
                                    						default priority) as the user stream that is set by
                                    							<samp class="ph codeph">cudnnSetStream()</samp>, while an exception/limitation is that
                                    						they will have priority as (highest - 1) for the user stream with the
                                    						highest priority. This is true only when the user stream is NOT in capture
                                    						mode (<samp class="ph codeph">cudaStreamCaptureStatusActive</samp>), otherwise the
                                    						behavior does not change.
                                 </li>
                                 <li class="li liexpand">Fusion engine operation mode 16 is not currently supported on Windows; this
                                    						will be fixed in a future release.
                                 </li>
                                 <li class="li liexpand">The functional support criteria of cuDNN's convolution kernels is not required to consider
                                    						padding. Users of cuDNN can witness an unexpected lack of problem support
                                    						when forward convolution spatial dimensions are less than the filter size
                                    						and padding is nonzero, however, is sufficient to extend spatial dimensions
                                    						to or beyond filter dimensions. This is commonly observed with, but not
                                    						limited to, INT8 convolution kernels.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Limitations</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later; it also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples can crash unless they are installed in a writable location.</li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit non-deterministic behavior when the cuDNN
                                       							library is built with CUDA Toolkit 10.2 or higher. This is the result of
                                       							a new buffer management and heuristics in the cuBLAS library. As
                                       							described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This is
                                       							caused by two buffer sizes (16 KB and 4 MB) used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting for a buffer of
                                       							that size to be released, a smaller buffer may be used with a different
                                       							GPU kernel. The kernel selection may affect numerical results. The user
                                       							can eliminate the non-deterministic behavior of cuDNN RNN and multihead
                                       							attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16 KB each in GPU
                                       							memory while the second setting creates two buffers of 4 MB each. The
                                       							default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors in order to run
                                    						efficiently. As always, cuDNN recommends users to align tensors to 16 byte
                                    						boundaries that will be sufficiently aligned for any computational option in
                                    						cuDNN. Doing otherwise may cause performance regressions in cuDNN 8.0.x
                                    						compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and the output is
                                    						in FP16 (half float), there are cases where the numerical accuracy between
                                    						the different algorithms might differ. cuDNN 8.0.x users can target the
                                    						backend API to query the numerical notes of the algorithms to get the
                                    						information programmatically. There are cases where algo0 and algo1 will
                                    						have a reduced precision accumulation when users target the legacy API. In
                                    						all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and backward
                                    						filter, grouped convolution with groups larger than 1 and with odd product
                                    						of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D convolution),
                                    							<samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on devices
                                    						older than NVIDIA Volta. To prevent a potential illegal memory access by an
                                    						instruction that only has a 16-bit version in NVIDIA Volta and later, pad at
                                    						least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">On K80 GPUs, when <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionForward()</samp></a> is used with
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp>
                                    						algorithm and half I/O data types a silent error might occur when the output
                                    						width <samp class="ph codeph">Q</samp> is <samp class="ph codeph">1</samp> and both height and width
                                    						padding are zero.
                                 </li>
                                 <li class="li liexpand">
                                    <div class="p">Several cuDNN APIs are unable to directly support computations using integer types
                                       								(<samp class="ph codeph">CUDNN_DATA_INT8</samp>,
                                       								<samp class="ph codeph">CUDNN_DATA_INT8x4</samp>,
                                       								<samp class="ph codeph">CUDNN_DATA_INT8x32</samp> or
                                       								<samp class="ph codeph">CUDNN_DATA_INT32</samp>). Floating types (particularly
                                       								<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>) are much more widely supported.
                                       							If an API does not support the desired type, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnTransformTensor" target="_blank" shape="rect"><samp class="ph codeph">cudnnTransformTensor()</samp></a> can be used to
                                       							support the use case by converting to/from a supported type and the
                                       							desired type. Here are the steps for doing so:<a name="rel-824__ol_hz5_r31_dmb" shape="rect">
                                          <!-- --></a><ol class="ol" id="rel-824__ol_hz5_r31_dmb">
                                          <li class="li">Convert all input tensors from their native type to a supported
                                             									type (<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> is recommended).
                                          </li>
                                          <li class="li">Run cuDNN API using the converted input tensors and output
                                             									tensor descriptors set as
                                             									<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>.
                                          </li>
                                          <li class="li">Convert all output tensors from a supported type to your desired
                                             									output type.
                                          </li>
                                       </ol>
                                       <div class="note note"><span class="notetitle">Note:</span> This will require extra memory use for the temporary buffers.
                                          								Further, this will introduce an additional round trip to memory that
                                          								might noticeably impact performance.
                                       </div>
                                    </div>
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use texture-based load
                                    						structure for performance improvements particularly in older hardware
                                    						architectures. Users can opt out of using texture using the environmental
                                    						variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this variable is
                                    						removed. Texture loading is turned off by default. Users who want to
                                    						continue to use texture-based load, can adapt the new backend API, and
                                    						toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORT</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check API (for example,
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Starting in cuDNN version 8.1.0, we are no longer shipping the
                                    							<samp class="ph codeph">libfreeimg</samp> static library with the MNIST sample. Users
                                    						can follow the instructions in the <samp class="ph codeph">readme.txt</samp> file to
                                    						download and compile the library separately and link with the MNIST
                                    						sample.
                                 </li>
                                 <li class="li liexpand">For pre-Volta devices, users should align all buffers to at least 4 bytes;
                                    						this applies to half-precision data as well.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command explicitly to
                                    						resolve the undefined symbols from cuDNN static libraries.
                                 </li>
                                 <li class="li liexpand">Starting in version 8.1, cuDNN uses AVX intrinsics on the x86_64
                                    						architecture; users of this architecture without support for AVX intrinsics
                                    						may see illegal instruction errors.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-822"><a name="rel-822" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-822" name="rel-822" shape="rect">1.21.&nbsp;cuDNN Release 8.2.2</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">This is the cuDNN 8.2.2 release notes. This release includes fixes from the previous
                           		cuDNN v8.1.x releases as well as the following additional changes. These release notes are
                           		applicable to both cuDNN and NVIDIA JetPack users of cuDNN unless appended specifically with
                           			<em class="ph i">(not applicable for Jetson platforms)</em>.
                        </p>
                        <div class="section" id="rel-822__section_l2m_s4c_2jb"><a name="rel-822__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">For previous cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-archived/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-822__section_ktn_pvj_bqb"><a name="rel-822__section_ktn_pvj_bqb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-822__ul_aqh_qvj_bqb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-822__ul_aqh_qvj_bqb">
                                 <li class="li liexpand">Experimental runtime fusion heuristics are now supported to facilitate an intelligent and
                                    						efficient heuristic recommendation based on the predicted execution time for
                                    						the runtime fusion engine. The current coverage is limited to fusion
                                    						patterns involving a convolution forward operation in FP16 mixed precision
                                    						configuration on NVIDIA Ampere Architecture GPUs. We will continue to expand
                                    						the support and improve the prediction accuracy in future releases.
                                 </li>
                                 <li class="li liexpand">The cuDNN runtime fusion now supports pure pointwise fusion or pointwise plus reduction
                                    						fusion. It supports FP16/FP32 as I/O type and FP32 as compute type.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, see the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-822__section_xwx_gfb_bqb"><a name="rel-822__section_xwx_gfb_bqb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:<a name="rel-822__ul_gvr_hfb_bqb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-822__ul_gvr_hfb_bqb">
                                 <li class="li liexpand">For platforms that ship a compiler version older than GCC 6 by default,
                                    						linking to static cuDNN using the default compiler is not supported.
                                 </li>
                                 <li class="li liexpand">There was a 15% performance regression for inference on the PyTorch WaveGlow
                                    						model on the NVIDIA Turing architecture. This regression has been
                                    						fixed.
                                 </li>
                                 <li class="li liexpand">The <samp class="ph codeph">convolve_common_engine_int8_NHWC</samp> kernel had an
                                    						undesired FP32 &gt; INT32 truncation before outputting the FP32 result
                                    						directly. This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">In previous cuDNN versions, <samp class="ph codeph">cudnnRNNBackwardData()</samp>,
                                    							<samp class="ph codeph">cudnnRNNBackwardDataEx()</samp>, or
                                    							<samp class="ph codeph">cudnnRNNBackwardData_v8()</samp> could return
                                    							<samp class="ph codeph">CUDNN_STATUS_INTERNAL_ERROR</samp> when
                                    							<samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_STATIC </samp>and <samp class="ph codeph">CUDNN_LSTM
                                       						</samp>were selected. This issue occurred mainly on smaller GPUs, such as
                                    						NVIDIA Turing with 36 or 48 SMs and smaller <samp class="ph codeph">hiddenSize</samp>
                                    						values. This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">NVIDIA Turing GTX 16xx users of cuDNN would observe invalid values in convolution output.
                                    						This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Users would experience NCHW transformations causing a floating point
                                    						exception and the CPU reference code producing incorrect results for tensor
                                    						format <samp class="ph codeph">CUDNN_TENSOR_NCHW_VECT_C</samp>. Corner cases in the
                                    						convolution sample code have been fixed in this release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-822__section_t2m_s5p_nnb"><a name="rel-822__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">Users of the static library requiring best possible convolution performance should use
                                    						whole-archive linking. This will come at a cost to binary size that will
                                    						require resolution in future releases, either through static sub libraries
                                    						or relaxing the whole-archive linkage requirement altogether.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX 3090 compared to
                                    						2080 Ti. This includes EfficientNet with up to 6x performance difference,
                                    						UNet up to 1.6x performance difference and Tacotron up to 1.6x performance
                                    						difference.
                                 </li>
                                 <li class="li liexpand">Compared to version 8.0.5, legacy convolution APIs have increased CPU
                                    						computational costs. On x86, this has been measured to be as high as 10
                                    						microseconds.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnAddTensor" target="_blank" shape="rect"><samp class="ph codeph">cudnnAddTensor()</samp></a>
                                    						does not support all tensor shapes even though the cuDNN documentation says
                                    						otherwise.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    						<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there are known performance regressions up to 2x on select
                                    						configurations for AlexNet-like models on NVIDIA Turing GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6.5, there are known performance regressions on various
                                    						convolutional models using INT8 data types on NVIDIA Volta GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior, accumulator data
                                    						types, and so on, have not been documented as of yet. 
                                 </li>
                                 <li class="li liexpand">It is possible, starting in cuDNN 7.6 and up to but not including 8.1.1, to leak memory
                                    						when computing common convolution operations in rare cases.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">The documentation for <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnReorderFilterAndBias" target="_blank" shape="rect"><samp class="ph codeph">cudnnReorderFilterAndBias()</samp></a> needs some
                                    						corrections for clarity. 
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">NVIDIA Turing users of cuDNN can observe intermittent illegal memory access
                                    						errors for some convolution workloads. 
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture. 
                                 </li>
                                 <li class="li liexpand">In a multi-GPU setting, with complex scheduling, cuDNN can segfault. It is
                                    						not clear that this is a cuDNN issue, but the issue is under active
                                    						investigation so that the known limitations section of this document can be
                                    						updated in a future release.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN version 8.0.2, there is a known 3x performance regression for a single
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> use
                                    						case.
                                 </li>
                                 <li class="li liexpand">There is a known 60% performance regression for ResNet-50 on the GTX 1080 when run using
                                    						FP16 data with large batch sizes (over 128).
                                 </li>
                                 <li class="li liexpand">Users of the static library requiring the best possible convolution performance should use
                                    						whole-archive linking. This will come at a cost to the binary size that will
                                    						require resolution in a future release, either through static sub libraries
                                    						or relaxing the whole-archive linkage requirement altogether.
                                 </li>
                                 <li class="li liexpand">cuDNN may contain a small memory leak related to the usage of
                                    							<samp class="ph codeph">dlopen()</samp> within the library; this is not confirmed but
                                    						currently under investigation. The possible leak does not affect Windows
                                    						users or users of the static library.
                                 </li>
                                 <li class="li liexpand">On NVIDIA Pascal and Maxwell architectures, users of cuDNN's 8.0 backend engine 34 for
                                    						forward convolution can witness-illegal memory access; this affects all
                                    						previous versions of cuDNN 8.0 and will be fixed in a future release.
                                 </li>
                                 <li class="li liexpand">When using the cuDNN CTC Loss API function, the computed gradients array is
                                    						not zero initialized. When sequence lengths are exceeded, some gradient
                                    						entries are returned uninitialized.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Limitations</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later; it also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples can crash unless they are installed in a writable location.</li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit non-deterministic behavior when the cuDNN
                                       							library is built with CUDA Toolkit 10.2 or higher. This is the result of
                                       							a new buffer management and heuristics in the cuBLAS library. As
                                       							described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This is
                                       							caused by two buffer sizes (16 KB and 4 MB) used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting for a buffer of
                                       							that size to be released, a smaller buffer may be used with a different
                                       							GPU kernel. The kernel selection may affect numerical results. The user
                                       							can eliminate the non-deterministic behavior of cuDNN RNN and multihead
                                       							attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16 KB each in GPU
                                       							memory while the second setting creates two buffers of 4 MB each. The
                                       							default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors in order to run
                                    						efficiently. As always, cuDNN recommends users to align tensors to 16 byte
                                    						boundaries that will be sufficiently aligned for any computational option in
                                    						cuDNN. Doing otherwise may cause performance regressions in cuDNN 8.0.x
                                    						compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and the output is
                                    						in FP16 (half float), there are cases where the numerical accuracy between
                                    						the different algorithms might differ. cuDNN 8.0.x users can target the
                                    						backend API to query the numerical notes of the algorithms to get the
                                    						information programmatically. There are cases where algo0 and algo1 will
                                    						have a reduced precision accumulation when users target the legacy API. In
                                    						all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and backward
                                    						filter, grouped convolution with groups larger than 1 and with odd product
                                    						of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D convolution),
                                    							<samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on devices
                                    						older than NVIDIA Volta. To prevent a potential illegal memory access by an
                                    						instruction that only has a 16-bit version in NVIDIA Volta and later, pad at
                                    						least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">On K80 GPUs, when <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionForward()</samp></a> is used with
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp>
                                    						algorithm and half I/O data types a silent error might occur when the output
                                    						width <samp class="ph codeph">Q</samp> is <samp class="ph codeph">1</samp> and both height and width
                                    						padding are zero.
                                 </li>
                                 <li class="li liexpand">
                                    <div class="p">Several cuDNN APIs are unable to directly support computations using integer types
                                       								(<samp class="ph codeph">CUDNN_DATA_INT8</samp>,
                                       								<samp class="ph codeph">CUDNN_DATA_INT8x4</samp>,
                                       								<samp class="ph codeph">CUDNN_DATA_INT8x32</samp> or
                                       								<samp class="ph codeph">CUDNN_DATA_INT32</samp>). Floating types (particularly
                                       								<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>) are much more widely supported.
                                       							If an API does not support the desired type, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnTransformTensor" target="_blank" shape="rect"><samp class="ph codeph">cudnnTransformTensor()</samp></a> can be used to
                                       							support the use case by converting to/from a supported type and the
                                       							desired type. Here are the steps for doing so:<a name="rel-822__ol_hz5_r31_dmb" shape="rect">
                                          <!-- --></a><ol class="ol" id="rel-822__ol_hz5_r31_dmb">
                                          <li class="li">Convert all input tensors from their native type to a supported
                                             									type (<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> is recommended).
                                          </li>
                                          <li class="li">Run cuDNN API using the converted input tensors and output
                                             									tensor descriptors set as
                                             									<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>.
                                          </li>
                                          <li class="li">Convert all output tensors from a supported type to your desired
                                             									output type.
                                          </li>
                                       </ol>
                                       <div class="note note"><span class="notetitle">Note:</span> This will require extra memory use for the temporary buffers.
                                          								Further, this will introduce an additional round trip to memory that
                                          								might noticeably impact performance.
                                       </div>
                                    </div>
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use texture-based load
                                    						structure for performance improvements particularly in older hardware
                                    						architectures. Users can opt out of using texture using the environmental
                                    						variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this variable is
                                    						removed. Texture loading is turned off by default. Users who want to
                                    						continue to use texture-based load, can adapt the new backend API, and
                                    						toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORT</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check API (for example,
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Starting in cuDNN version 8.1.0, we are no longer shipping the
                                    							<samp class="ph codeph">libfreeimg</samp> static library with the MNIST sample. Users
                                    						can follow the instructions in the <samp class="ph codeph">readme.txt</samp> file to
                                    						download and compile the library separately and link with the MNIST
                                    						sample.
                                 </li>
                                 <li class="li liexpand">For pre-Volta devices, users should align all buffers to at least 4 bytes;
                                    						this applies to half-precision data as well.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command explicitly to
                                    						resolve the undefined symbols from cuDNN static libraries.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-822__section_zsq_h4m_4pb"><a name="rel-822__section_zsq_h4m_4pb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Deprecated Features</h3>
                           <div class="p">The following features are deprecated in cuDNN 8.2.2:<a name="rel-822__ul_sbk_34m_4pb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-822__ul_sbk_34m_4pb">
                                 <li class="li liexpand">Support for Ubuntu 16.04 has been deprecated in cuDNN 8.2.2 for CUDA 11.4. For a list of
                                    						supported OS, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                                 </li>
                                 <li class="li liexpand">Support for RHEL7 for ppc64le has been deprecated in cuDNN 8.2.2 for CUDA 11.4. For a list
                                    						of supported OS, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-821"><a name="rel-821" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-821" name="rel-821" shape="rect">1.22.&nbsp;cuDNN Release 8.2.1</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">This is the cuDNN 8.2.1 release notes. This release includes fixes from the previous
                           		cuDNN v8.1.x releases as well as the following additional changes. These release notes are
                           		applicable to both cuDNN and NVIDIA JetPack users of cuDNN unless appended specifically with
                           			<em class="ph i">(not applicable for Jetson platforms)</em>.
                        </p>
                        <div class="section" id="rel-821__section_l2m_s4c_2jb"><a name="rel-821__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">For previous cuDNN documentation, see the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-archived/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-821__section_qhk_vl4_fpb"><a name="rel-821__section_qhk_vl4_fpb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-821__ul_rj1_wl4_fpb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-821__ul_rj1_wl4_fpb">
                                 <li class="li liexpand">The cuDNN runtime fusion engine now supports generating Tensor Core kernels with input
                                    						tensors of:<a name="rel-821__ul_rdz_q5j_spb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-821__ul_rdz_q5j_spb">
                                       <li class="li">Bfloat16 type and compute precision of FP32 (requires compute capability 8.0 or later).
                                          								For Bfloat16 support, convolution I/O channels are required to be a
                                          								multiple of 8.
                                       </li>
                                       <li class="li">INT8 and compute precision of INT32 (requires compute capability 7.5 or later) datatype
                                          								and in NHWC layout. For INT8 support, the convolution I/O channels
                                          								are required to be a multiple of 16, and unlike the
                                          									<samp class="ph codeph">NCHW_VECT_C</samp> kernels, filter and bias reordering
                                          								is not required.
                                       </li>
                                    </ul>
                                    <p class="p">In the fused pointwise/reduction operations, FP32 is the compute
                                       							precision supported.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The cuDNN runtime fusion engine has added experimental Tensor Core kernel generation
                                    						support for NVIDIA Volta (compute capability 7.0) and NVIDIA Xavier (compute
                                    						capability 7.2). The supported input tensor data type is FP16, compute
                                    						precision is FP32, and the supported layout is NHWC. However, reduction
                                    						fusion is not yet supported and we are working on further generalizing the
                                    						support.
                                 </li>
                                 <li class="li liexpand">Equations in the documentation are now supported in Chrome.</li>
                                 <li class="li liexpand">The backend API now supports fused convolution-scale-bias-activation with
                                    						per-channel-scaling by matching the operation graph.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingBackward()</samp></a> allows both
                                    							<samp class="ph codeph">x</samp> and <samp class="ph codeph">y</samp> data pointers (together with
                                    						the related tensor descriptor handles) to be <samp class="ph codeph">NULL</samp> for
                                    						avg-pooling. This could save memory footprint and bandwidth.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-821__section_nvt_c5p_nnb"><a name="rel-821__section_nvt_c5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:
                              <ul class="ul">
                                 <li class="li liexpand">In some cases, NVIDIA Ampere Architecture users of cuDNN 8.1 <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetConvolutionBackwardFilterAlgorithm_v7" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetConvolutionBackwardFilterAlgorithm_v7()</samp></a>
                                    						could receive a workspace that was insufficient for computing the
                                    						calculation with <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a>. This
                                    						issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Many convolution models were experiencing lower performance on NVIDIA RTX 3090 compared to
                                    						2080 Ti. This included ResNet-50 with up to 2x performance difference and
                                    						ResNeXt up to 10x the performance difference. Many of these performance
                                    						issues have been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN version 8.0.5, there was a known 8% performance regression on the SSD
                                    						ResNet-50 model on the NVIDIA Ampere Architecture. This issue has been fixed
                                    						in this release.
                                 </li>
                                 <li class="li liexpand">L4T users of cuDNN could observe
                                    							<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> errors in some cases when
                                    						performing convolutions using
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp>. This
                                    						issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">In cuDNN 8.2.0, if the user runs a Bi-directional RNN network with dropout
                                    						enabled, the user may see non-deterministic outputs. This issue has been
                                    						fixed in 8.2.1.
                                 </li>
                                 <li class="li liexpand">There was a known 18% performance regression for inference on the PyTorch
                                    						ResNet-50 v1.5 model on the NVIDIA Turing architecture. This issue has been
                                    						fixed in this release.
                                 </li>
                                 <li class="li liexpand">Known regressions on certain layers in <a class="xref" href="https://forums.developer.nvidia.com/t/cudnn8-regression-in-algorithm-selection-heuristics/153667" target="_blank" shape="rect">cuDNN 8 regression in algorithm selection
                                       							heuristics</a> have been fixed on NVIDIA Volta and NVIDIA Pascal
                                    						platforms.
                                 </li>
                                 <li class="li liexpand">In older versions of cuDNN, when calling the API <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSetDropoutDescriptor" target="_blank" shape="rect"><samp class="ph codeph">cudnnSetDropoutDescriptor()</samp></a>, a kernel
                                    						launched by this API used to require a substantial amount of GPU memory for
                                    						the stack. The memory is released when the kernel finishes and the stack
                                    						size is changed back in a way that is not thread safe. Starting in the 8.2.1
                                    						release, the extra memory is no longer required, and as a result, the thread
                                    						safety concern is no longer present.
                                 </li>
                                 <li class="li liexpand">In cuDNN 8.1.1, compared to cuDNN 8.1.0, there was a known regression in
                                    						performance of the runtime fusion engine for convolution fused with ReLU in
                                    						the epilog. This was caused due to the generalized support for parameterized
                                    						ReLU. This issue has been fixed since the 8.2.0 release.
                                 </li>
                                 <li class="li liexpand">Since cuDNN 8.0.4 until 8.2.0, certain SKUs of V100 GPU may encounter
                                    							<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> status or unspecified
                                    						launch failure in a subsequent call to <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudaDeviceSynchronize" target="_blank" shape="rect"><samp class="ph codeph">cudaDeviceSynchronize()</samp></a> when running RNN with
                                    						cell mode of <samp class="ph codeph">CUDNN_LSTM</samp> and
                                    							<samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_STATIC</samp> algorithm. This issue has
                                    						been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Between cuDNN 8.1.0 and 8.2.0, if the user runs <samp class="ph codeph">cudnnRNN*()</samp>
                                    						API under CUDA compute sanitizer with
                                    							<samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_STATIC_SMALL_H</samp> algorithm, users
                                    						may see errors like <samp class="ph codeph">Invalid __global__ read</samp> reported by the
                                    						CUDA compute sanitizer. This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.0 preview, there is a known ~12% performance regression on vgg16 when
                                    						run on Jetson Nano and TX2. This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there is a significant performance regression on Darknet when run on
                                    						Jetson Nano. This issue has been fixed in this release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-821__section_t2m_s5p_nnb"><a name="rel-821__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">Users of the static library requiring best possible convolution performance should use
                                    						whole-archive linking. This will come at a cost to binary size that will
                                    						require resolution in future releases, either through static sub libraries
                                    						or relaxing the whole-archive linkage requirement altogether.
                                 </li>
                                 <li class="li liexpand">Some convolution models are experiencing lower performance on NVIDIA RTX 3090 compared to
                                    						2080 Ti. This includes EfficientNet with up to 6x performance difference,
                                    						UNet up to 1.6x performance difference and Tacotron up to 1.6x performance
                                    						difference.
                                 </li>
                                 <li class="li liexpand">Compared to version 8.0.5, legacy convolution APIs have increased CPU computational costs.
                                    						On x86, this has been measured to be as high as 10 microseconds.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnAddTensor" target="_blank" shape="rect"><samp class="ph codeph">cudnnAddTensor()</samp></a>
                                    						does not support all tensor shapes even though the cuDNN documentation says
                                    						otherwise.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    						<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there are known performance regressions up to 2x on select
                                    						configurations for AlexNet-like models on NVIDIA Turing GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6.5, there are known performance regressions on various convolutional
                                    						models using INT8 data types on NVIDIA Volta GPUs.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior, accumulator data
                                    						types, and so on, have not been documented as of yet. 
                                 </li>
                                 <li class="li liexpand">It is possible, starting in cuDNN v7.6 and up to but not including 8.1.1, to leak memory
                                    						when computing common convolution operations in rare cases.
                                 </li>
                                 <li class="li liexpand">There is a known 15% performance regression for inference on the PyTorch WaveGlow model on
                                    						the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">The documentation for <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnReorderFilterAndBias" target="_blank" shape="rect"><samp class="ph codeph">cudnnReorderFilterAndBias()</samp></a> needs some
                                    						corrections for clarity. 
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                                 <li class="li liexpand">NVIDIA Turing GTX 16xx users of cuDNN can observe invalid values in convolution output. </li>
                                 <li class="li liexpand">NVIDIA Turing users of cuDNN can observe intermittent illegal memory access errors for some
                                    						convolution workloads. 
                                 </li>
                                 <li class="li liexpand">FFT and Winograd based algorithms for convolution do not support graph
                                    						capture. 
                                 </li>
                                 <li class="li liexpand">In a multi-GPU setting, with complex scheduling, cuDNN can segfault. It is
                                    						not clear that this is a cuDNN issue, but the issue is under active
                                    						investigation so that the known limitations section of this document can be
                                    						updated in a future release.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN version 8.0.2, there is a known 3x performance regression for a single
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> use
                                    						case.
                                 </li>
                                 <li class="li liexpand">There is a known 60% performance regression for ResNet-50 on the GTX 1080 when run using
                                    						FP16 data with large batch sizes (over 128).
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Limitations</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA Toolkit 11.2
                                    						update 1 or later; it also requires the NVRTC from CUDA 11.2 update 1 or
                                    						later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples can crash unless they are installed in a writable location.</li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit non-deterministic behavior when the cuDNN
                                       							library is built with CUDA Toolkit 10.2 or higher. This is the result of
                                       							a new buffer management and heuristics in the cuBLAS library. As
                                       							described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This is
                                       							caused by two buffer sizes (16 KB and 4 MB) used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting for a buffer of
                                       							that size to be released, a smaller buffer may be used with a different
                                       							GPU kernel. The kernel selection may affect numerical results. The user
                                       							can eliminate the non-deterministic behavior of cuDNN RNN and multihead
                                       							attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>. 
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16 KB each in GPU
                                       							memory while the second setting creates two buffers of 4 MB each. The
                                       							default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte alignment,
                                    						including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors in order to run
                                    						efficiently. As always, cuDNN recommends users to align tensors to 16 byte
                                    						boundaries that will be sufficiently aligned for any computational option in
                                    						cuDNN. Doing otherwise may cause performance regressions in cuDNN 8.0.x
                                    						compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and the output is
                                    						in FP16 (half float), there are cases where the numerical accuracy between
                                    						the different algorithms might differ. cuDNN 8.0.x users can target the
                                    						backend API to query the numerical notes of the algorithms to get the
                                    						information programmatically. There are cases where algo0 and algo1 will
                                    						have a reduced precision accumulation when users target the legacy API. In
                                    						all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and backward
                                    						filter, grouped convolution with groups larger than 1 and with odd product
                                    						of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D convolution),
                                    							<samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on devices
                                    						older than NVIDIA Volta. To prevent a potential illegal memory access by an
                                    						instruction that only has a 16-bit version in NVIDIA Volta and later, pad at
                                    						least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">On K80 GPUs, when <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionForward()</samp></a> is used with
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp>
                                    						algorithm and half I/O data types a silent error might occur when the output
                                    						width <samp class="ph codeph">Q</samp> is <samp class="ph codeph">1</samp> and both height and width
                                    						padding are zero.
                                 </li>
                                 <li class="li liexpand">
                                    <div class="p">Several cuDNN APIs are unable to directly support computations using integer types
                                       								(<samp class="ph codeph">CUDNN_DATA_INT8</samp>,
                                       								<samp class="ph codeph">CUDNN_DATA_INT8x4</samp>,
                                       								<samp class="ph codeph">CUDNN_DATA_INT8x32</samp> or
                                       								<samp class="ph codeph">CUDNN_DATA_INT32</samp>). Floating types (particularly
                                       								<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>) are much more widely supported.
                                       							If an API does not support the desired type, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnTransformTensor" target="_blank" shape="rect"><samp class="ph codeph">cudnnTransformTensor()</samp></a> can be used to
                                       							support the use case by converting to/from a supported type and the
                                       							desired type. Here are the steps for doing so:<a name="rel-821__ol_hz5_r31_dmb" shape="rect">
                                          <!-- --></a><ol class="ol" id="rel-821__ol_hz5_r31_dmb">
                                          <li class="li">Convert all input tensors from their native type to a supported
                                             									type (<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> is recommended).
                                          </li>
                                          <li class="li">Run cuDNN API using the converted input tensors and output
                                             									tensor descriptors set as
                                             									<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>.
                                          </li>
                                          <li class="li">Convert all output tensors from a supported type to your desired
                                             									output type.
                                          </li>
                                       </ol>
                                       <div class="note note"><span class="notetitle">Note:</span> This will require extra memory use for the temporary buffers.
                                          								Further, this will introduce an additional round trip to memory that
                                          								might noticeably impact performance.
                                       </div>
                                    </div>
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are limited to
                                    							<samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use texture-based load
                                    						structure for performance improvements particularly in older hardware
                                    						architectures. Users can opt out of using texture using the environmental
                                    						variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this variable is
                                    						removed. Texture loading is turned off by default. Users who want to
                                    						continue to use texture-based load, can adapt the new backend API, and
                                    						toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORT</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check API (for example,
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Starting in cuDNN version 8.1.0, we are no longer shipping the <samp class="ph codeph">libfreeimg</samp>
                                    						static library with the MNIST sample. Users can follow the instructions in
                                    						the <samp class="ph codeph">readme.txt</samp> file to download and compile the library
                                    						separately and link with the MNIST sample.
                                 </li>
                                 <li class="li liexpand">For pre-Volta devices, users should align all buffers to at least 4 bytes;
                                    						this applies to half-precision data as well.
                                 </li>
                                 <li class="li liexpand">Users of cuDNN must add the dependencies of cuBLAS to the linkers command explicitly to
                                    						resolve the undefined symbols from cuDNN static libraries.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-821__section_zsq_h4m_4pb"><a name="rel-821__section_zsq_h4m_4pb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Deprecated Features</h3>
                           <div class="p">The following features are deprecated in cuDNN 8.2.1:<a name="rel-821__ul_sbk_34m_4pb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-821__ul_sbk_34m_4pb">
                                 <li class="li liexpand">Support for Ubuntu 16.04 will be deprecated in cuDNN 8.2.2 for CUDA 11.4. For a list of
                                    						supported OS, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                                 </li>
                                 <li class="li liexpand">Support for RHEL7 for ppc64le will be deprecated in cuDNN 8.2.2 for CUDA 11.4. For a list
                                    						of supported OS, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-820"><a name="rel-820" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-820" name="rel-820" shape="rect">1.23.&nbsp;cuDNN Release 8.2.0</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">This is the cuDNN 8.2.0 release notes. This release includes fixes from the previous
                           		cuDNN v8.1.x releases as well as the following additional changes. These release notes are
                           		applicable to both cuDNN and NVIDIA JetPack users of cuDNN unless appended specifically with
                           			<em class="ph i">(not applicable for Jetson platforms)</em>.
                        </p>
                        <div class="section" id="rel-820__section_l2m_s4c_2jb"><a name="rel-820__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">For previous cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-archived/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-820__section_qhk_vl4_fpb"><a name="rel-820__section_qhk_vl4_fpb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-820__ul_rj1_wl4_fpb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-820__ul_rj1_wl4_fpb">
                                 <li class="li liexpand">Convolution with the backend API now supports tensor with more than 2**31 elements. The
                                    						size and stride of each tensor dimension are still limited to 32-bit
                                    						values.
                                 </li>
                                 <li class="li liexpand">Convolution Heuristics Generalization has been improved for several GPUs.
                                    						These improvements can be observed in the legacy API and version 8 API. In
                                    						the version 8 API, these improvements are available in both
                                    							<samp class="ph codeph">CUDNN_HEUR_MODE_INSTANT</samp> and
                                    							<samp class="ph codeph">CUDNN_HEUR_MODE_B</samp>.
                                 </li>
                                 <li class="li liexpand">The cuDNN runtime fusion engine now supports generating Tensor Core based
                                    						fusion kernels in the following scenarios:<a name="rel-820__ul_rs4_rpg_jpb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-820__ul_rs4_rpg_jpb">
                                       <li class="li">When there is a scale+bias+ReLU pattern in the graph fused to the <samp class="ph codeph">x</samp>
                                          								input of a convolution forward operation
                                       </li>
                                       <li class="li">When the graph contains 3D convolution forward, backward data, or backward filter
                                          								operation
                                       </li>
                                       <li class="li">When the graph contains a convolution backward data operation with non-unit convolution
                                          								strides
                                       </li>
                                    </ul>
                                    <p class="p">We are working on further generalization of this support.</p>
                                 </li>
                                 <li class="li liexpand">cuDNN C++ frontend has released the 0.2 version that adds more general support to
                                    						activation forward and backward operations, matMul operation, and contains
                                    						various bug fixes and clean ups. A few runtime fusion samples have also been
                                    						added. For more information, refer to <a class="xref" href="https://github.com/NVIDIA/cudnn-frontend/releases/tag/v0.2" target="_blank" shape="rect">GitHub: cuDNN frontend</a>.
                                 </li>
                                 <li class="li liexpand">The new <samp class="ph codeph">RNN ALGO_STANDARD</samp> implementation and heuristics tuning provides
                                    						significant speedup (up to 100%), especially when the overall problem size
                                    						is small (hidden size, batch size, and the number of timesteps).
                                 </li>
                                 <li class="li liexpand">The RNN dropout implementation has been heavily optimized. The new
                                    						implementation brings significant speed-up to all RNN algorithms when
                                    						dropout is enabled.
                                 </li>
                                 <li class="li liexpand">cuDNN RNN has moved to calling cuBLASLt on newer architectures (compute
                                    						capability &gt;= 7.0). As a result, the
                                    							<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> workaround for cuBLAS
                                    						non-deterministic behavior is no longer needed on those architectures. In
                                    						addition, under repeated CUDA graph capture, cuBLASLt no longer allocates
                                    						workspace repeatedly like cuBLAS.
                                 </li>
                                 <li class="li liexpand">In the cuDNN v8 backend API, a new <samp class="ph codeph">CUDNN_ATTR_ENGINE_BEHAVIOR_NOTE</samp>
                                    						attribute has been added. Users can query the engine behaviors using this
                                    						attribute similar to the numerical behaviors queried through the
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_NUMERICAL_NOTE</samp> attribute. Currently,
                                    						the engine behavior note only shows whether an engine does runtime
                                    						compilation or not. More behaviors may be added in future releases.
                                 </li>
                                 <li class="li liexpand">cuDNN API logging for the v8 backend API has been significantly improved. Now more detailed
                                    						information can be printed from the backend data structures, for example,
                                    						tensors, operations, engines, and execution plans. We hope this can improve
                                    						the development and debugging experience of cuDNN. Refer to <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#api-logging" target="_blank" shape="rect">this link</a> for more instructions of
                                    						how to enable API logging.
                                 </li>
                                 <li class="li liexpand">cuDNN now supports SWISH activation in both forward and backward directions. It can be
                                    						configured for use with <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnActivationForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnActivationForward()</samp></a> and <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnActivationBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnActivationBackward()</samp></a> by using
                                    							<samp class="ph codeph">CUDNN_ACTIVATION_SWISH</samp> with <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSetActivationDescriptor" target="_blank" shape="rect"><samp class="ph codeph">cudnnSetActivationDescriptor()</samp></a>. SWISH
                                    						activation's parameter, commonly known as beta, may further be set using the
                                    						newly added API function <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSetActivationDescriptorSwishBeta" target="_blank" shape="rect"><samp class="ph codeph">cudnnSetActivationDescriptorSwishBeta()</samp></a> and
                                    						queried with <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetActivationDescriptorSwishBeta" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetActivationDescriptorSwishBeta()</samp></a>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-820__section_nvt_c5p_nnb"><a name="rel-820__section_nvt_c5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:
                              <ul class="ul">
                                 <li class="li liexpand">There was a performance regression in certain use cases comparing NVIDIA RTX 3090 using
                                    						cuDNN version 8.x to NVIDIA RTX 2080 Ti using cuDNN version 7.x. This
                                    						regression has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">There was a performance regression in the runtime engine for convolution
                                    						fused with ReLU in the epilog in cuDNN 8.1.1. This regression has been fixed
                                    						in this release.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN version 7.6.5, there was a performance regression in
                                    						certain grouped <samp class="ph codeph">ConvolutionBackwardFilter</samp> cases on the
                                    						NVIDIA Volta GPU architecture. This regression has been fixed in this
                                    						release.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT</samp> returned an internal
                                    						error when the number of channels in the filter was greater than or equal to
                                    						65536. This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN version 8.0.2, there was a known 3x performance regression for a single
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> use case.
                                    						This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Although the overall cuDNN library size has improved in cuDNN 8.1.0 with
                                    						CUDA Toolkit 11.2 and greater, as compared to cuDNN 8.0.x, the cuDNN library
                                    						remains large. We have attempted to moderate the severity of this issue in
                                    						this release.
                                 </li>
                                 <li class="li liexpand">Many convolution models were experiencing lower performance on NVIDIA RTX 3090 compared to
                                    						2080 Ti. This included ResNet-50 with up to 2x performance difference,
                                    						ResNeXt up to 10x performance difference and U-Net up to 3x performance
                                    						difference. The performance issues have been fixed in this release.
                                 </li>
                                 <li class="li liexpand">The ResNet-50 native FP32 inference issues have been fixed on NVIDIA Volta, NVIDIA Turing,
                                    						and NVIDIA Ampere Architecture GPUs.
                                 </li>
                                 <li class="li liexpand">We have eliminated anonymous structs in cuDNN public headers
                                    							<samp class="ph codeph">cudnn_cnn_infer.h</samp>, <samp class="ph codeph">cudnn_cnn_train.h</samp>,
                                    						and <samp class="ph codeph">cudnn_ops_infer.h</samp> to allow forward struct declarations.
                                    						The following five <samp class="ph codeph">typedef-s</samp> were updated: <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionFwdAlgoPerf_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionFwdAlgoPerf_t</samp></a>, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBwdDataAlgoPerf_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBwdDataAlgoPerf_t</samp></a>, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBwdFilterAlgoPerf_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBwdFilterAlgoPerf_t</samp></a>, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnAlgorithm_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnAlgorithm_t</samp></a>,
                                    						and <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnDebug_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnDebug_t</samp></a></li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnActivationForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnActivationForward()</samp></a> could generate illegal
                                    						memory access errors for tensors of more than 2**30 elements in the previous
                                    						version of cuDNN 8. This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">In previous releases, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnRNNBackwardWeights" target="_blank" shape="rect"><samp class="ph codeph">cudnnRNNBackwardWeights()</samp></a>, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnRNNBackwardWeightsEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnRNNBackwardWeightsEx()</samp></a>, and <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnRNNBackwardWeights_v8" target="_blank" shape="rect"><samp class="ph codeph">cudnnRNNBackwardWeights_v8()</samp></a> may generate
                                    						wrong and non-deterministic results when dropout is enabled. A stream
                                    						dependency issue has been fixed in the current release so users will no
                                    						longer observe this issue.
                                 </li>
                                 <li class="li liexpand">The heuristics in <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter()" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> have been
                                    						improved for generalized cases. We have observed several convolution cases
                                    						with up to ~100x performance improvements compared to cuDNN version
                                    						8.1.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.4, there was a known ~6% performance regression on ONNX-WaveGlow when
                                    						run on NVIDIA TITAN RTX. This issue has been fixed in this release. 
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there were known performance regressions up to 2x on select
                                    						configurations for AlexNet-like models on NVIDIA Turing GPUs. This issue has
                                    						been fixed in this release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-820__section_t2m_s5p_nnb"><a name="rel-820__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">Users of the static library requiring best possible convolution performance should use
                                    						whole-archive linking. This will come at a cost to binary size that will
                                    						require resolution in future releases, either through static sub libraries
                                    						or relaxing the whole-archive linkage requirement altogether.
                                 </li>
                                 <li class="li liexpand">Compared to version 8.0.5, legacy convolution APIs have increased CPU
                                    						computational costs. On x86, this has been measured to be as high as 10
                                    						microseconds.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnAddTensor" target="_blank" shape="rect"><samp class="ph codeph">cudnnAddTensor()</samp></a>
                                    						does not support all broadcast-able tensor shapes even though the cuDNN
                                    						documentation says otherwise.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.0 Preview, there is a known ~12% performance regression on vgg16 when
                                    						run on Jetson Nano and TX2.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.4, there is a known ~6% performance regression on ONNX-WaveGlow when
                                    						run on NVIDIA TITAN RTX.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there is a significant performance regression on
                                    						Darknet when run on Jetson Nano.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    						<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there are known performance regressions up to 2x on select
                                    						configurations for AlexNet-like models on NVIDIA Turing GPUs.
                                 </li>
                                 <li class="li liexpand">L4T users of cuDNN may observe
                                    							<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> errors in some cases when
                                    						performing convolutions using
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp>. This
                                    						issue is being investigated.
                                 </li>
                                 <li class="li liexpand">Users of the static library requiring the best possible convolution performance should use
                                    						whole-archive linking. This will come at a cost to the binary size that will
                                    						require resolution in a future release, either through static sub libraries
                                    						or relaxing the whole-archive linkage requirement altogether.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6.5, there are known performance regressions on various
                                    						convolutional models using INT8 data types on NVIDIA Volta GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there is a known regression in performance of the
                                    						runtime fusion engine for convolution fused with ReLU in the epilog. This is
                                    						caused due to the generalized support for parameterized ReLU. Further
                                    						optimizations are being worked on.
                                 </li>
                                 <li class="li liexpand">The numeric behavior of INT8 operations including saturation behavior, accumulator data
                                    						types, and so on, have not been documented as of yet. This is being worked
                                    						on and will be resolved in a future release.
                                 </li>
                                 <li class="li liexpand">It is possible, starting in cuDNN v7.6, to leak memory when computing common
                                    						convolution operations in rare cases.
                                 </li>
                                 <li class="li liexpand">There is a known 15% performance regression for inference on the PyTorch
                                    						WaveGlow model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">There is a known 25% performance regression for inference on the PyTorch SSD
                                    						model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">There is a known 18% performance regression for inference on the PyTorch
                                    						ResNet-50 v1.5 model on the NVIDIA Turing architecture.
                                 </li>
                                 <li class="li liexpand">The documentation for <samp class="ph codeph">cudnnReorderFilterAndBias()</samp> needs
                                    						some corrections for clarity. This will be fixed in a future release.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.5, there is a known ~17% performance regression on SSD
                                    						models running on V100.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Limitations</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">The runtime fusion engine is only supported in the cuDNN build based on CUDA
                                    						Toolkit 11.2 update 1 or later; it also requires the NVRTC from CUDA 11.2
                                    						update 1 or later. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples can crash unless they are installed in a writable location.</li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit non-deterministic behavior when the cuDNN
                                       							library is built with CUDA Toolkit 10.2 or higher. This is the result of
                                       							a new buffer management and heuristics in the cuBLAS library. As
                                       							described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This is
                                       							caused by two buffer sizes (16 KB and 4 MB) used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting for a buffer of
                                       							that size to be released, a smaller buffer may be used with a different
                                       							GPU kernel. The kernel selection may affect numerical results. The user
                                       							can eliminate the non-deterministic behavior of cuDNN RNN and multihead
                                       							attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>.
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16 KB each in GPU
                                       							memory while the second setting creates two buffers of 4 MB each. The
                                       							default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors in order to run
                                    						efficiently. As always, cuDNN recommends users to align tensors to 16 byte
                                    						boundaries that will be sufficiently aligned for any computational option in
                                    						cuDNN. Doing otherwise may cause performance regressions in cuDNN 8.0.x
                                    						compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and the output is
                                    						in FP16 (half float), there are cases where the numerical accuracy between
                                    						the different algorithms might differ. cuDNN 8.0.x users can target the
                                    						backend API to query the numerical notes of the algorithms to get the
                                    						information programmatically. There are cases where algo0 and algo1 will
                                    						have a reduced precision accumulation when users target the legacy API. In
                                    						all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests. 
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and backward
                                    						filter, grouped convolution with groups larger than 1 and with odd product
                                    						of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D convolution),
                                    							<samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on devices
                                    						older than NVIDIA Volta. To prevent a potential illegal memory access by an
                                    						instruction that only has a 16-bit version in NVIDIA Volta and later, pad at
                                    						least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">On K80 GPUs, when <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionForward()</samp></a> is used with
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp>
                                    						algorithm and half I/O data types a silent error might occur when the output
                                    						width <samp class="ph codeph">Q</samp> is <samp class="ph codeph">1</samp> and both height and width
                                    						padding are zero.
                                 </li>
                                 <li class="li liexpand">
                                    <div class="p">Several cuDNN APIs are unable to directly support computations using integer types
                                       								(<samp class="ph codeph">CUDNN_DATA_INT8</samp>,
                                       								<samp class="ph codeph">CUDNN_DATA_INT8x4</samp>,
                                       								<samp class="ph codeph">CUDNN_DATA_INT8x32</samp> or
                                       								<samp class="ph codeph">CUDNN_DATA_INT32</samp>). Floating types (particularly
                                       								<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>) are much more widely supported.
                                       							If an API does not support the desired type, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnTransformTensor" target="_blank" shape="rect"><samp class="ph codeph">cudnnTransformTensor()</samp></a> can be used to
                                       							support the use case by converting to/from a supported type and the
                                       							desired type. Here are the steps for doing so:<a name="rel-820__ol_hz5_r31_dmb" shape="rect">
                                          <!-- --></a><ol class="ol" id="rel-820__ol_hz5_r31_dmb">
                                          <li class="li">Convert all input tensors from their native type to a supported
                                             									type (<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> is recommended).
                                          </li>
                                          <li class="li">Run cuDNN API using the converted input tensors and output
                                             									tensor descriptors set as
                                             									<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>.
                                          </li>
                                          <li class="li">Convert all output tensors from a supported type to your desired
                                             									output type.
                                          </li>
                                       </ol>
                                       <div class="note note"><span class="notetitle">Note:</span> This will require extra memory use for the temporary buffers.
                                          								Further, this will introduce an additional round trip to memory that
                                          								might noticeably impact performance.
                                       </div>
                                    </div>
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use texture-based load
                                    						structure for performance improvements particularly in older hardware
                                    						architectures. Users can opt out of using texture using the environmental
                                    						variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this variable is
                                    						removed. Texture loading is turned off by default. Users who want to
                                    						continue to use texture-based load, can adapt the new backend API, and
                                    						toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORT</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check API (for example,
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Starting in cuDNN version 8.1.0, we are no longer shipping the
                                    							<samp class="ph codeph">libfreeimg</samp> static library with the MNIST sample. Users
                                    						can follow the instructions in the <samp class="ph codeph">readme.txt</samp> file to
                                    						download and compile the library separately and link with the MNIST
                                    						sample.
                                 </li>
                                 <li class="li liexpand">For pre-Volta devices, users should align all buffers to at least 4 bytes;
                                    						this applies to half-precision data as well.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-811"><a name="rel-811" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-811" name="rel-811" shape="rect">1.24.&nbsp;cuDNN Release 8.1.1</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">This is the cuDNN 8.1.1 release notes. This release includes fixes from the previous
                           		cuDNN v8.0.x releases as well as the following additional changes. These release notes are
                           		applicable to both cuDNN and NVIDIA JetPack users of cuDNN unless appended specifically with
                           			<em class="ph i">(not applicable for Jetson platforms)</em>.
                        </p>
                        <div class="section" id="rel-811__section_l2m_s4c_2jb"><a name="rel-811__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">For previous cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-archived/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-811__section_rc1_lkw_t4b"><a name="rel-811__section_rc1_lkw_t4b" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-811__ul_j25_lkw_t4b" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-811__ul_j25_lkw_t4b">
                                 <li class="li liexpand">The runtime fusion engine now supports the canonical
                                    							<samp class="ph codeph">NCHW</samp>/<samp class="ph codeph">KCRS</samp>/<samp class="ph codeph">NKPQ</samp> format
                                    						for describing a tensor, in addition to the version 8 format that has the
                                    						explicit group dimension
                                    							<samp class="ph codeph">NGCHW</samp>/<samp class="ph codeph">GKCRS</samp>/<samp class="ph codeph">NGKPQ</samp>
                                    						that is already supported.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine now supports NVIDIA Ampere Architecture cards with compute
                                    						capability 86 (that is, GA10x) in addition to compute capability 80 (GA100)
                                    						and compute capability 75 (Tu10x).
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine fully supports fusing configurable ReLU (a generalization of
                                    						ReLU, clipped ReLU, and leaky ReLU), Tanh, Sigmoid, configurable EluGelu,
                                    						configurable Softplus, and configurable Swish forward and backward
                                    						activations into the epilog of a convolution forward, a convolution backward
                                    						data, or a matrix multiplication operation.
                                 </li>
                                 <li class="li liexpand">The runtime fusion engine now fully supports<samp class="ph codeph"> [N, H, W, C]</samp> to <samp class="ph codeph">[1,
                                       							1, 1, C]</samp> reduction and <samp class="ph codeph">[N, H, W, C]</samp> to
                                    							<samp class="ph codeph">[N, H, W, 1]</samp> reduction on the output of a convolution
                                    						forward, a convolution backward data operation, and <samp class="ph codeph">[1, M,
                                       							N]</samp> to <samp class="ph codeph">[1, M, 1]</samp> and <samp class="ph codeph">[1, M, N]</samp>
                                    						to <samp class="ph codeph">[1, 1, N]</samp> reduction in matrix multiplication operations.
                                    						For convolution backward filter operation, <samp class="ph codeph">[N, H, W, C]</samp> to
                                    							<samp class="ph codeph">[1, H, W, C]</samp> reduction and <samp class="ph codeph">[N, H, W,
                                       							C]</samp> to&gt; <samp class="ph codeph">[N, 1, 1, 1]</samp>reduction are supported.
                                    						The supported reduction operators are
                                    							<samp class="ph codeph">CUDNN_REDUCE_TENSOR_ADD</samp>,
                                    							<samp class="ph codeph">CUDNN_REDUCE_TENSOR_MIN</samp>, and
                                    							<samp class="ph codeph">CUDNN_REDUCE_TENSOR_MAX</samp>.
                                 </li>
                                 <li class="li liexpand">API logging in <samp class="ph codeph">cudnnBackendExecute()</samp> has been greatly improved to print
                                    						out the internal information in descriptors like operation graphs, engines,
                                    						and execution plans.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-811__section_nvt_c5p_nnb"><a name="rel-811__section_nvt_c5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:
                              <ul class="ul">
                                 <li class="li liexpand">In some cases, <samp class="ph codeph">cudnnConvolutionBackwardData()</samp>, on the NVIDIA Turing and
                                    						NVIDIA Volta architectures performs operations on the GPU resulting in
                                    						illegal memory access. This issue was fixed in version 8.0 and subsequent
                                    						releases.
                                 </li>
                                 <li class="li liexpand">When running a convolution forward, convolution backward data/weights, or a matrix
                                    						multiplication fusion with pointwise and reduction operations with engine
                                    						index <samp class="ph codeph">0</samp>, the runtime fusion engine used to be allowed to
                                    						run on the CUDA Toolkit 10.2. However, not all the features it relies upon
                                    						are supported in the CUDA Toolkit 10.2. For better stability and targeted
                                    						optimizations, the engine now requires CUDA Toolkit 11.2 update 1. We have
                                    						blocked the engine from running in cuDNN built against CUDA Toolkit 10.2.
                                    						See the <em class="ph i">Limitations</em> section for more details.
                                 </li>
                                 <li class="li liexpand">The supported check in the runtime fusion engine has been improved to return
                                    						proper error codes in currently unsupported operation graphs, such as:<a name="rel-811__ul_w5l_jgb_54b" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-811__ul_w5l_jgb_54b">
                                       <li class="li">an operation graph that contains more than one convolution of matrix
                                          								multiplication operations
                                       </li>
                                       <li class="li">convolutions with compute type that is not FP32</li>
                                       <li class="li">grouped convolutions</li>
                                       <li class="li">when the convolution mode is not
                                          									<samp class="ph codeph">CUDNN_CROSS_CORRELATION</samp></li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand">When running a convolution backward data/weights fusion with pointwise and
                                    						reduction operations with engine index <samp class="ph codeph">0</samp>, the runtime
                                    						fusion engine may be launching kernel with more shared memory specified than
                                    						necessary, causing sub-optimal performance. This issue has been fixed in
                                    						this release.
                                 </li>
                                 <li class="li liexpand">Execution of a plan for convolution forward operation graph, with engine-global index
                                    							<samp class="ph codeph">1</samp> returned <samp class="ph codeph">CUDNN_STATUS_INTERNAL_ERROR</samp>
                                    						when the filter format is NHWC and padding was larger than zero. This issue
                                    						has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN version 8.0.5, there was a known performance regression of
                                    						10-50% on Tacotron2 and WaveGlow models. This issue has been fixed in this
                                    						release.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp> does not invoke
                                    						TF32 Tensor Core kernels when the math type in the convolution descriptor is
                                    						set to <samp class="ph codeph">CUDNN_DEFAULT_MATH</samp>. This leads to suboptimal
                                    						performance under this math mode. This issue has been fixed in this
                                    						release.
                                 </li>
                                 <li class="li liexpand">Fixed an issue where the version 8 graph API’s execution plan descriptor may internally
                                    						refer to a descriptor outside of the data structure, which can cause
                                    						unexpected errors when the external descriptors have been destroyed. Now all
                                    						the information is recorded within the data structure.
                                 </li>
                                 <li class="li liexpand">There was a performance regression where NHWC was slower than NCHW on 3D convolution up to
                                    						40% on V100 and NVIDIA A100 GPUs. This issue has been fixed in this
                                    						release.
                                 </li>
                                 <li class="li liexpand">There was a performance regression in certain use cases comparing NVIDIA RTX 3090 using
                                    						cuDNN version 8.x to NVIDIA RTX 2080 Ti using cuDNN version 7.x. This
                                    						regression has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN version 7.6, there were known performance regressions up to 2x on select
                                    						configurations for AlexNet-like models on NVIDIA Volta and NVIDIA Ampere
                                    						Architecture GPUs. These regressions has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">When calling:<a name="rel-811__ul_gvj_dhb_54b" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-811__ul_gvj_dhb_54b">
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnRNNForwardInference" target="_blank" shape="rect"><samp class="ph codeph">cudnnRNNForwardInference()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnRNNForwardInferenceEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnRNNForwardInferenceEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnRNNForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnRNNForwardTraining()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnRNNForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnRNNForwardTrainingEx()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnRNNBackwardData" target="_blank" shape="rect"><samp class="ph codeph">cudnnRNNBackwardData()</samp></a></li>
                                       <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnRNNBackwardDataEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnRNNBackwardDataEx()</samp></a></li>
                                    </ul>
                                    
                                    the API will crash when called with
                                    							<samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_DYNAMIC</samp> algo but the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPersistentRNNPlan_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnPersistentRNNPlan_t</samp></a> was not created. This
                                    						has been fixed in this release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-811__section_t2m_s5p_nnb"><a name="rel-811__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">Users of the static library requiring best possible convolution performance should use
                                    						whole-archive linking. This will come at a cost to binary size that will
                                    						require resolution in future releases, either through static sub libraries
                                    						or relaxing the whole-archive linkage requirement altogether.
                                 </li>
                                 <li class="li liexpand">The ResNet-50 native FP32 inference issues have been fixed on NVIDIA Volta and NVIDIA
                                    						Turing. Few performance regressions exist in the NVIDIA Ampere Architecture
                                    						GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to version 8.0.5, legacy convolution APIs have increased CPU
                                    						computational costs. On x86, this has been measured to be as high as 10
                                    						microseconds.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnAddTensor" target="_blank" shape="rect"><samp class="ph codeph">cudnnAddTensor()</samp></a>
                                    						does not support all broadcast-able tensor shapes even though the cuDNN
                                    						documentation says otherwise.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.0 Preview, there is a known ~12% performance regression on vgg16 when
                                    						run on Jetson Nano and TX2.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.4, there is a known ~6% performance regression on ONNX-WaveGlow when
                                    						run on NVIDIA TITAN RTX.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there is a significant performance regression on Darknet when run on
                                    						Jetson Nano.
                                 </li>
                                 <li class="li liexpand">For pre-Volta devices, users should align all buffers to at least 4 bytes;
                                    						this applies to half-precision data as well.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN version 8.0.2, there is a known 3x performance regression for a single
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> use case.
                                    						We are not aware of any popular model that uses this unique use case.
                                 </li>
                                 <li class="li liexpand">Although the overall cuDNN library size has improved in cuDNN 8.1.0 with
                                    						CUDA Toolkit 11.2 and greater as compared to cuDNN 8.0.x, the cuDNN library
                                    						remains large; future releases will attempt to moderate the severity of this
                                    						issue.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT</samp> returns an internal
                                    						error when the number of channels in the filter is greater than or equal to
                                    						65536.
                                 </li>
                                 <li class="li liexpand">Execution of a plan for convolution forward operation graph, with engine-global index
                                    							<samp class="ph codeph">1</samp> returns <samp class="ph codeph">CUDNN_STATUS_INTERNAL_ERROR</samp>
                                    						when the filter format in NHWC and padding is larger than zero.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    						<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there are known performance regressions up to 2x on select
                                    						configurations for AlexNet-like models on NVIDIA Turing GPUs.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnActivationForward()</samp> can cause an illegal memory access
                                    						CUDA error for tensors with more than 2**30 elements.
                                 </li>
                                 <li class="li liexpand">L4T users of cuDNN may observe
                                    							<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> errors in some cases when
                                    						performing convolutions using
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp>. This
                                    						issue is being investigated.
                                 </li>
                                 <li class="li liexpand">Users of the static library requiring the best possible convolution performance should use
                                    						whole-archive linking. This will come at a cost to the binary size that will
                                    						require resolution in a future release, either through static sub libraries
                                    						or relaxing the whole-archive linkage requirement altogether.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there are known performance regressions on certain
                                    							<samp class="ph codeph">dgrad</samp> NHWC configurations from FastPitch and WaveGlow
                                    						models on V100 and NVIDIA A100 GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6.5, there are known performance regressions on various
                                    						convolutional models using INT8 data types on NVIDIA Volta GPUs.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.1.0, there is a known regression in performance of the
                                    						runtime fusion engine for convolution fused with ReLU in the epilog. This is
                                    						caused due to the generalized support for parameterized ReLU. Further
                                    						optimizations are being worked on.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Limitations</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">The runtime fusion engine was only supported in the cuDNN build based on
                                    						CUDA Toolkit 11.2 update 1; it now requires the NVRTC from CUDA 11.2 update
                                    						1. If this condition is not satisfied, the error status of
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> or
                                    							<samp class="ph codeph">CUDNN_STATUS_RUNTIME_PREREQUISITE_MISSING</samp> will be
                                    						returned.
                                 </li>
                                 <li class="li liexpand">Samples can crash unless they are installed in a writable location.</li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit non-deterministic behavior when the cuDNN
                                       							library is built with CUDA Toolkit 10.2 or higher. This is the result of
                                       							a new buffer management and heuristics in the cuBLAS library. As
                                       							described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This is
                                       							caused by two buffer sizes (16 KB and 4 MB) used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting for a buffer of
                                       							that size to be released, a smaller buffer may be used with a different
                                       							GPU kernel. The kernel selection may affect numerical results. The user
                                       							can eliminate the non-deterministic behavior of cuDNN RNN and multihead
                                       							attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>.
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16 KB each in GPU
                                       							memory while the second setting creates two buffers of 4 MB each. The
                                       							default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors in order to run
                                    						efficiently. As always, cuDNN recommends users to align tensors to 16 byte
                                    						boundaries that will be sufficiently aligned for any computational option in
                                    						cuDNN. Doing otherwise may cause performance regressions in cuDNN 8.0.x
                                    						compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and the output is
                                    						in FP16 (half float), there are cases where the numerical accuracy between
                                    						the different algorithms might differ. cuDNN 8.0.x users can target the
                                    						backend API to query the numerical notes of the algorithms to get the
                                    						information programmatically. There are cases where algo0 and algo1 will
                                    						have a reduced precision accumulation when users target the legacy API. In
                                    						all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests. 
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and backward
                                    						filter, grouped convolution with groups larger than 1 and with odd product
                                    						of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D convolution),
                                    							<samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on devices
                                    						older than NVIDIA Volta. To prevent a potential illegal memory access by an
                                    						instruction that only has a 16-bit version in NVIDIA Volta and later, pad at
                                    						least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">On K80 GPUs when <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionForward()</samp></a> is used with
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp>
                                    						algorithm and half I/O data types a silent error might occur when the output
                                    						width <samp class="ph codeph">Q</samp> is <samp class="ph codeph">1</samp> and both height and width
                                    						padding are zero.
                                 </li>
                                 <li class="li liexpand">
                                    <div class="p">Several cuDNN APIs are unable to directly support computations using integer types
                                       								(<samp class="ph codeph">CUDNN_DATA_INT8</samp>,
                                       								<samp class="ph codeph">CUDNN_DATA_INT8x4</samp>,
                                       								<samp class="ph codeph">CUDNN_DATA_INT8x32</samp> or
                                       								<samp class="ph codeph">CUDNN_DATA_INT32</samp>). Floating types (particularly
                                       								<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>) are much more widely supported.
                                       							If an API does not support the desired type, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnTransformTensor" target="_blank" shape="rect"><samp class="ph codeph">cudnnTransformTensor()</samp></a> can be used to
                                       							support the use case by converting to/from a supported type and the
                                       							desired type. Here are the steps for doing so:<a name="rel-811__ol_hz5_r31_dmb" shape="rect">
                                          <!-- --></a><ol class="ol" id="rel-811__ol_hz5_r31_dmb">
                                          <li class="li">Convert all input tensors from their native type to a supported
                                             									type (<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> is recommended).
                                          </li>
                                          <li class="li">Run cuDNN API using the converted input tensors and output
                                             									tensor descriptors set as
                                             									<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>.
                                          </li>
                                          <li class="li">Convert all output tensors from a supported type to your desired
                                             									output type.
                                          </li>
                                       </ol>
                                       <div class="note note"><span class="notetitle">Note:</span> This will require extra memory use for the temporary buffers.
                                          								Further, this will introduce an additional round trip to memory that
                                          								might noticeably impact performance.
                                       </div>
                                    </div>
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use texture-based load
                                    						structure for performance improvements particularly in older hardware
                                    						architectures. Users can opt out of using texture using the environmental
                                    						variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this variable is
                                    						removed. Texture loading is turned off by default. Users who want to
                                    						continue to use texture-based load, can adapt the new backend API, and
                                    						toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORT</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check API (for example,
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Starting in cuDNN version 8.1.0, we are no longer shipping the
                                    							<samp class="ph codeph">libfreeimg</samp> static library with the MNIST sample. Users
                                    						can follow the instructions in the <samp class="ph codeph">readme.txt</samp> file to
                                    						download and compile the library separately and link with the MNIST
                                    						sample.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-810"><a name="rel-810" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-810" name="rel-810" shape="rect">1.25.&nbsp;cuDNN Release 8.1.0</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">This is the cuDNN 8.1.0 release notes. This release includes fixes from the previous
                           		cuDNN v8.0.x releases as well as the following additional changes. These release notes are
                           		applicable to both cuDNN and NVIDIA JetPack users of cuDNN unless appended specifically with
                           			<em class="ph i">(not applicable for Jetson platforms)</em>.
                        </p>
                        <div class="section" id="rel-810__section_l2m_s4c_2jb"><a name="rel-810__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">For previous cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-archived/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-810__section_bvm_4gl_j4b"><a name="rel-810__section_bvm_4gl_j4b" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:<a name="rel-810__ul_l13_pgl_j4b" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-810__ul_l13_pgl_j4b">
                                 <li class="li liexpand">A preview of the cuDNN runtime operation fusion capabilities is included in this release.
                                    						This feature is exposed as a new backend engine in the version 8.0 graph
                                    						API. With runtime op fusion, the engine can generate and compile fused
                                    						tensor-core kernels on the fly for the specified operation graph during the
                                    						execution plan finalization stage. Some of the operation graph patterns
                                    						supported in this preview are: convolution or matrix multiplication
                                    						operation with arbitrary combination of one or more pointwise operations,
                                    						and reduction operations fused onto the output tensor. Examples include but
                                    						are not limited to <samp class="ph codeph">conv-bias-leaky_relu</samp>, and
                                    							<samp class="ph codeph">gemm-bias-gelu</samp>. This feature is supported on GPUs with
                                    						compute capability 7.5 and 8.0. The current implementation supports FP16 I/O
                                    						with FP32 compute or FP32 (TF32) I/O with FP32 compute. In this release, the
                                    						support for this feature is restricted to Linux on x86-64. We will continue
                                    						to work on this feature to provide additional support and improved
                                    						performance. In the meantime, we welcome your feedback. E-mail:<a class="xref" href="mailto:cudnn@nvidia.com" target="_blank" shape="rect">cudnn@nvidia.com</a></li>
                                 <li class="li liexpand">We have released our <a class="xref" href="https://github.com/NVIDIA/cudnn-frontend" target="_blank" shape="rect">C++ frontend via GitHub</a> which implements a
                                    						series of classes wrapping around the v8 backend C API. The user must
                                    						include a few headers to enjoy the convenience from graph construction,
                                    						heuristics query to execution. The frontend also implements a significantly
                                    						improved autotuning feature that can accurately time the executions from a
                                    						list of functionally equivalent implementations and return the fastest
                                    						implementation.
                                 </li>
                                 <li class="li liexpand">Heuristics have been improved for TF32 and <samp class="ph codeph">PSEUDO_HALF</samp> (with Tensor Core
                                    						enabled) convolutions. On one known model, performance was improved 1.3x
                                    						(when not auto-tuning). On select cases in several models, we have seen
                                    						performance improvements up to ~50x.
                                 </li>
                                 <li class="li liexpand">Added support for <samp class="ph codeph">PSEUDO_BFLOAT16_CONFIG</samp> on NVIDIA Ampere Architecture GPU
                                    						for CNNs. While most of the algos/engines that are available for
                                    							<samp class="ph codeph">PSEUDO_HALF_CONFIG</samp> are available for
                                    							<samp class="ph codeph">PSEUDO_BFLOAT16_CONFIG</samp>, a few are not available. The
                                    						available engines for <samp class="ph codeph">PSEUDO_BFLOAT16_CONFIG</samp> can achieve at
                                    						least 90% performance of <samp class="ph codeph">PSEUDO_HALF_CONFIG</samp> for layers in
                                    						standard models. There is a known limitation for layers having three or four
                                    						channels for the filter and convolution of stride 2, such as the first layer
                                    						of ResNet.
                                 </li>
                                 <li class="li liexpand">EfficientNet performances have improved. Depthwise convolution is now
                                    						optimized in NHWC layout in cuDNN 8.1.0. From EfficientNet, we see an
                                    						average of 2.9x speed-up for 5x5 layers, and 1.7x speed-up for 3x3
                                    						layers.
                                 </li>
                                 <li class="li liexpand">Added support for TF32 engines to compute operation graphs that match the fused
                                    						conv-bias-activation pattern. TF32 kernels are also supported in <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBiasActivationForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp></a>
                                    						API.
                                 </li>
                                 <li class="li liexpand">Added support for a new RNN algo <samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_STATIC_SMALL_H</samp>,
                                    						specialized for small hidden sizes. It is expected to be faster than other
                                    						algos for those small hidden sizes.
                                 </li>
                                 <li class="li liexpand">The cuDNN build against CUDA Toolkit 11.2 is now backward compatible with
                                    						earlier CUDA 11 drivers, including 450, 455, in addition to the 460
                                    						driver.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-810__section_nvt_c5p_nnb"><a name="rel-810__section_nvt_c5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:
                              <ul class="ul">
                                 <li class="li liexpand">Kernel <samp class="ph codeph">calc_bias_diff_nhwc_packed</samp>has a known functional
                                    						issue from 8.0.2. It happens when running
                                    							<samp class="ph codeph">cudnnConvolutionBackwardBias</samp>(<samp class="ph codeph">bgrad</samp>)
                                    						with <samp class="ph codeph">NHWC/NDHWC</samp> packed tensors and even <samp class="ph codeph">C
                                       							&amp;&amp; (C &gt;= 6)</samp>. This issue was fixed in this release.
                                 </li>
                                 <li class="li liexpand">Kernel <samp class="ph codeph">convolve_common_engine_int8</samp> may cause accuracy degradation when
                                    						running <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBiasActivationForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp></a>
                                    						with INT8 in cuDNN version 8.0.5 because there was not a rounding when
                                    						converting the results from single to INT8. This issue was fixed in this
                                    						release.
                                 </li>
                                 <li class="li liexpand">On some NVIDIA Turing GPUs, when users are running persistent RNN with
                                    							<samp class="ph codeph">hiddenSize</samp> greater than or equal to 768 but less than
                                    						1024, users may get incorrect results and refer to CUDA error 719,
                                    							<samp class="ph codeph">cudaErrorLaunchFailure</samp> the next time they call
                                    							<samp class="ph codeph">cudaDeviceSynchronize()</samp>. This bug has been fixed in
                                    						this release.
                                 </li>
                                 <li class="li liexpand">Calling <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBiasActivationForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp></a> or
                                    						executing a cuDNN backend plan for fused convolution-bias-activation
                                    						operation graphs, can lead to a memory leak. This issue is fixed in the
                                    						current release.
                                 </li>
                                 <li class="li liexpand">On Windows, calling API <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetFoldedConvBackwardDataDescriptors" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetFoldedConvBackwardDataDescriptors()</samp></a>
                                    						results in failure to find symbols. This issue has been present in all
                                    						versions since cuDNN version 8.0.0 and is fixed in this release.
                                 </li>
                                 <li class="li liexpand">The backend convolution operation had external dependencies on the user created backend
                                    						tensor descriptors even after finalization. Deletion of the tensor
                                    						descriptors might cause the operation to seg-fault when constructing the
                                    						operation graph. This bug affects all versions since cuDNN version 8.0.0,
                                    						and has been fixed in 8.1.0.
                                 </li>
                                 <li class="li liexpand">Many convolution models were experiencing lower performance on NVIDIA RTX 3090 compared to
                                    						2080 Ti. This included ResNet-50 with up to 2x performance difference,
                                    						ResNeXt up to 10x performance difference and U-Net up to 3x performance
                                    						difference. The performance issues have been fixed in this release.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-810__section_t2m_s5p_nnb"><a name="rel-810__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">Users of the static library requiring best possible convolution performance should use
                                    						whole-archive linking. This will come at a cost to binary size that will
                                    						require resolution in future releases, either through static sub libraries
                                    						or relaxing the whole-archive linkage requirement altogether.
                                 </li>
                                 <li class="li liexpand">The ResNet-50 native FP32 inference issues have been fixed on NVIDIA Volta and NVIDIA
                                    						Turing. Few performance regressions exist in the NVIDIA Ampere Architecture
                                    						GPU.
                                 </li>
                                 <li class="li liexpand">Compared to version 8.0.5, legacy convolution APIs have increased CPU computational costs.
                                    						On x86, this has been measured to be as high as 10 microseconds.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnAddTensor" target="_blank" shape="rect"><samp class="ph codeph">cudnnAddTensor()</samp></a>
                                    						does not support all broadcast-able tensor shapes even though the cuDNN
                                    						documentation says otherwise.
                                 </li>
                                 <li class="li liexpand">Users have reported that in RNN training with non-zero dropout rate, and if the RNN network
                                    						is unidirectional, the output of <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnRNNBackwardWeights" target="_blank" shape="rect"><samp class="ph codeph">cudnnRNNBackwardWeights()</samp></a> may be
                                    						non-deterministic. We are still investigating this issue.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.0 Preview, there is a known ~12% performance regression on vgg16 when
                                    						run on Nano and TX2.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.4, there is a known ~6% performance regression on ONNX-WaveGlow when
                                    						run on NVIDIA TITAN RTX.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there is a significant performance regression on
                                    						Darknet when run on Nano.
                                 </li>
                                 <li class="li liexpand">For pre-Volta devices, users should align all buffers to at least 4 bytes; this applies to
                                    						half-precision data as well.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN version 8.0.2, there is a known 3x performance regression for a single
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> use case.
                                    						We are not aware of any popular model that uses this unique use case.
                                 </li>
                                 <li class="li liexpand">Although the overall cuDNN library size has improved in cuDNN 8.1.0 with CUDA Toolkit 11.2
                                    						and greater as compared to cuDNN 8.0.x, the cuDNN library remains large;
                                    						future releases will attempt to moderate the severity of this issue.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT</samp> returns an internal
                                    						error when the number of channels in the filter is greater than or equal to
                                    						65536.
                                 </li>
                                 <li class="li liexpand">Execution of a plan for convolution forward operation graph, with engine-global index
                                    							<samp class="ph codeph">1</samp> returns <samp class="ph codeph">CUDNN_STATUS_INTERNAL_ERROR</samp>
                                    						when the filter format in NHWC and padding is larger than zero.
                                 </li>
                                 <li class="li liexpand">Convolutions (<samp class="ph codeph">ConvolutionForward</samp>,
                                    						<samp class="ph codeph">ConvolutionBackwardData</samp>, and
                                    							<samp class="ph codeph">ConvolutionBackwardFilter</samp>) may experience performance
                                    						regressions when run with math type
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> on
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> data (<em class="ph i">input</em> and
                                    						<em class="ph i">output</em>).
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there are known performance regressions up to 2x on select
                                    						configurations for AlexNet-like models on NVIDIA Turing, NVIDIA Volta, and
                                    						NVIDIA Ampere Architecture GPUs.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Limitations</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">Samples can crash unless they are installed in a writable location.</li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit non-deterministic behavior when the cuDNN
                                       							library is built with CUDA Toolkit 10.2 or higher. This is the result of
                                       							a new buffer management and heuristics in the cuBLAS library. As
                                       							described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This is
                                       							caused by two buffer sizes (16 KB and 4 MB) used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting for a buffer of
                                       							that size to be released, a smaller buffer may be used with a different
                                       							GPU kernel. The kernel selection may affect numerical results. The user
                                       							can eliminate the non-deterministic behavior of cuDNN RNN and multihead
                                       							attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>.
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16 KB each in GPU
                                       							memory while the second setting creates two buffers of 4 MB each. The
                                       							default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multistream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors in order to run
                                    						efficiently. As always, cuDNN recommends users to align tensors to 16 byte
                                    						boundaries that will be sufficiently aligned for any computational option in
                                    						cuDNN. Doing otherwise may cause performance regressions in cuDNN 8.0.x
                                    						compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and the output is
                                    						in FP16 (half float), there are cases where the numerical accuracy between
                                    						the different algorithms might differ. cuDNN 8.0.x users can target the
                                    						backend API to query the numerical notes of the algorithms to get the
                                    						information programmatically. There are cases where algo0 and algo1 will
                                    						have a reduced precision accumulation when users target the legacy API. In
                                    						all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests. 
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and backward
                                    						filter, grouped convolution with groups larger than 1 and with odd product
                                    						of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D convolution),
                                    							<samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on devices
                                    						older than NVIDIA Volta. To prevent a potential illegal memory access by an
                                    						instruction that only has a 16-bit version in NVIDIA Volta and later, pad at
                                    						least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">On K80 GPUs, when <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionForward()</samp></a> is used with
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp>
                                    						algorithm and half I/O data types a silent error might occur when the output
                                    						width <samp class="ph codeph">Q</samp> is <samp class="ph codeph">1</samp> and both height and width
                                    						padding are zero.
                                 </li>
                                 <li class="li liexpand">
                                    <div class="p">Several cuDNN APIs are unable to directly support computations using integer types
                                       								(<samp class="ph codeph">CUDNN_DATA_INT8</samp>,
                                       								<samp class="ph codeph">CUDNN_DATA_INT8x4</samp>,
                                       								<samp class="ph codeph">CUDNN_DATA_INT8x32</samp> or
                                       								<samp class="ph codeph">CUDNN_DATA_INT32</samp>). Floating types (particularly
                                       								<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>) are much more widely supported.
                                       							If an API does not support the desired type, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnTransformTensor" target="_blank" shape="rect"><samp class="ph codeph">cudnnTransformTensor()</samp></a> can be used to
                                       							support the use case by converting to/from a supported type and the
                                       							desired type. Here are the steps for doing so:<a name="rel-810__ol_hz5_r31_dmb" shape="rect">
                                          <!-- --></a><ol class="ol" id="rel-810__ol_hz5_r31_dmb">
                                          <li class="li">Convert all input tensors from their native type to a supported
                                             									type (<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> is recommended).
                                          </li>
                                          <li class="li">Run cuDNN API using the converted input tensors and output
                                             									tensor descriptors set as
                                             									<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>.
                                          </li>
                                          <li class="li">Convert all output tensors from a supported type to your desired
                                             									output type.
                                          </li>
                                       </ol>
                                       <div class="note note"><span class="notetitle">Note:</span> This will require extra memory use for the temporary buffers.
                                          								Further, this will introduce an additional round trip to memory that
                                          								might noticeably impact performance.
                                       </div>
                                    </div>
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use texture-based load
                                    						structure for performance improvements particularly in older hardware
                                    						architectures. Users can opt out of using texture using the environmental
                                    						variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this variable is
                                    						removed. Texture loading is turned off by default. Users who want to
                                    						continue to use texture-based load, can adapt the new backend API, and
                                    						toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSpatialTfSamplerBackward" target="_blank" shape="rect"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp></a> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORT</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check API (for example,
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnOpsInferVersionCheck" target="_blank" shape="rect"><samp class="ph codeph">cudnnOpsInferVersionCheck()</samp></a>) to load the
                                    						kernels in the sub library before opening graph capture.
                                 </li>
                                 <li class="li liexpand">Starting in cuDNN version 8.1.0, we are no longer shipping the
                                    							<samp class="ph codeph">libfreeimg</samp> static library with the MNIST sample. Users
                                    						can follow the instructions in the <samp class="ph codeph">readme.txt</samp> file to
                                    						download and compile the library separately and link with the MNIST
                                    						sample.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-805"><a name="rel-805" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-805" name="rel-805" shape="rect">1.26.&nbsp;cuDNN Release 8.0.5</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">This is the cuDNN 8.0.5 release notes. This release includes fixes from the previous
                           		cuDNN v8.0.x releases as well as the following additional changes. These release notes are
                           		applicable to both cuDNN and NVIDIA JetPack users of cuDNN unless appended specifically with
                           			<em class="ph i">(not applicable for Jetson platforms)</em>.
                        </p>
                        <div class="section" id="rel-805__section_l2m_s4c_2jb"><a name="rel-805__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">For previous cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-archived/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-805__section_r1j_15p_nnb"><a name="rel-805__section_r1j_15p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:
                              <ul class="ul">
                                 <li class="li liexpand">RNN now supports zero-length sequences within the batch when the RNN data layout is
                                    							<samp class="ph codeph">CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_UNPACKED</samp> or
                                    							<samp class="ph codeph">CUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED</samp>. For more
                                    						information, see <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSetRNNDataDescriptor" target="_blank" shape="rect"><samp class="ph codeph">cudnnSetRNNDataDescriptor()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Users can now set the environment variable <samp class="ph codeph">CUDNN_CONV_WSCAP_DBG</samp> to a value
                                    						in MiB to limit the workspace size returned by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionForwardGetWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionForwardGetWorkspaceSize()</samp></a>,
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardDataGetWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardDataGetWorkspaceSize()</samp></a>,
                                    						and <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilterGetWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilterGetWorkspaceSize()</samp></a>.
                                    						Limiting the workspace might result in performance lost.
                                 </li>
                                 <li class="li liexpand">Significant performance improvements were made for NVIDIA RTX 3090 for many models on many
                                    						configurations.
                                 </li>
                                 <li class="li liexpand">Performance improvements were made:<a name="rel-805__ul_xbk_ykl_4nb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-805__ul_xbk_ykl_4nb">
                                       <li class="li">For EfficientNet, when run using NHWC FP16 Tenor Core configurations on V100 and A100 GPU
                                          								architectures.
                                       </li>
                                       <li class="li">For PilotNet, AH-Net, MobileNet V3 on V100 and A100 GPU
                                          								architectures.
                                       </li>
                                       <li class="li">For various 3D convolution cases on NVIDIA RTX 8000.</li>
                                    </ul>
                                 </li>
                                 <li class="li liexpand">Support for the 3D <samp class="ph codeph">NDHWC</samp> layout was added in <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a>.
                                 </li>
                                 <li class="li liexpand">Added instructions for installing cuDNN using the Package Manager for Linux and RHEL users.
                                    						For step-by-step instructions, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#cudnn-package-manager-installation-overview" target="_blank" shape="rect">Package Manager Installation</a>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-805__section_nvt_c5p_nnb"><a name="rel-805__section_nvt_c5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:
                              <ul class="ul">
                                 <li class="li liexpand"><samp class="ph codeph">cudnnBackendFinalize(descriptor)</samp>, where <samp class="ph codeph">descriptor</samp> is of
                                    						type <samp class="ph codeph">CUDNN_BACKEND_ENGINE_DESCRIPTOR()</samp> or
                                    							<samp class="ph codeph">CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR()</samp>, might result
                                    						in a hang if the operation graph has backward filter operation and the user
                                    						links against <samp class="ph codeph">libcudnn.so</samp> (<samp class="ph codeph">cudnn64.dll</samp> on
                                    						Windows). This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Call to <samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp> might result in a memory
                                    						leak in release 8.0.1. This issue has been fixed.
                                 </li>
                                 <li class="li liexpand">Performance regression on the U-Net Industrial network on NVIDIA Volta for certain batch
                                    						sizes has been fixed.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnRNN*()</samp> with LSTM mode may produce incorrect results on the
                                    							<samp class="ph codeph">cy</samp> outputs when clipping is enabled on all GPUs. This
                                    						issue also exists in previous cuDNN releases since version 7.2.1. This issue
                                    						has been fixed in this release.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnRNNForward*</samp> with LSTM mode may produce incorrect results in case of
                                    						clipping when <samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_STATIC</samp> is used. This
                                    						issue also exists in previous cuDNN releases since version 7.2.1. This issue
                                    						has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">In previous cuDNN versions, <samp class="ph codeph">cudnnRNNBackwardData()</samp> or
                                    							<samp class="ph codeph">cudnnRNNBackwardDataEx()</samp>may produce non-deterministic
                                    						outputs when running configurations such as <samp class="ph codeph">hiddenSize=128</samp>
                                    						or less, LSTM cell type, and FP32 with
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp>. This issue has
                                    						been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there was a known ~6% performance regression on Inception V3 and
                                    						ResNet-50 models when run using NHWC FP16 configurations on various NVIDIA
                                    						Turing and NVIDIA TITAN V architectures. This issue has been fixed in this
                                    						release.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN v8.0.3, there was a known ~18% performance regression on
                                    						the U-Net Industrial model when run using NCHW TF32 configurations on V100
                                    						and A100 GPU architectures. This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand"><em class="ph i">Updated: November 25, 2020</em><p class="p">When calling
                                       								<samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp> with INT8x4
                                       							or INT8x32 I/O tensors, it could result in
                                       								<samp class="ph codeph">CUDNN_STATUS_BAD_PARAM</samp> in 8.0.4. This issue has
                                       							been fixed in this release.
                                    </p>
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-805__section_t2m_s5p_nnb"><a name="rel-805__section_t2m_s5p_nnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">When using <samp class="ph codeph">cudnnRNN*</samp> APIs with the problem sizes (input size, hidden size)
                                    						not being multiples of 16 for FP16 tensors or multiples of 8 for FP32
                                    						tensors, users encountered a return status of
                                    							<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> in cudnn built against
                                    						CUDA 11.0. This issue has been fixed with cuDNN built against CUDA
                                    						11.1.
                                 </li>
                                 <li class="li liexpand">The ResNet-50 native FP32 inference issues have been fixed on NVIDIA Volta and NVIDIA
                                    						Turing. Few performance regressions exist in the NVIDIA Ampere Architecture
                                    						GPU.
                                 </li>
                                 <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnAddTensor" target="_blank" shape="rect"><samp class="ph codeph">cudnnAddTensor()</samp></a>
                                    						does not support all broadcast-able tensor shapes even though the cuDNN
                                    						documentation says otherwise.
                                 </li>
                                 <li class="li liexpand">Users have reported that in RNN training with non-zero dropout rate, and if the RNN network
                                    						is unidirectional, the output of <samp class="ph codeph">cudnnRNNBackwardWeights()</samp>
                                    						may be non-deterministic. We are still investigating this issue.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnPoolingForward()</samp> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_AVG</samp> might output NaN for pixel in output
                                    						tensor outside the value recommended by
                                    							<samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp> or
                                    							<samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp>.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.0 Preview, there is a known ~12% performance regression on vgg16 when
                                    						run on Nano and TX2.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 8.0.4, there is a known ~6% performance regression on ONNX-WaveGlow when
                                    						run on NVIDIA TITAN RTX.
                                 </li>
                                 <li class="li liexpand">Compared to cuDNN 7.6, there is a significant performance regression on
                                    						Darknet when run on Nano.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Limitations</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">Samples can crash unless they are installed in a writable location.</li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit non-deterministic behavior when the cuDNN
                                       							library is built with CUDA Toolkit 10.2 or higher. This is the result of
                                       							a new buffer management and heuristics in the cuBLAS library. As
                                       							described in  <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This is
                                       							caused by two buffer sizes (16 KB and 4 MB) used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting for a buffer of
                                       							that size to be released, a smaller buffer may be used with a different
                                       							GPU kernel. The kernel selection may affect numerical results. The user
                                       							can eliminate the non-deterministic behavior of cuDNN RNN and multihead
                                       							attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>.
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16 KB each in GPU
                                       							memory while the second setting creates two buffers of 4 MB each. The
                                       							default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte alignment,
                                    						including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN require increased alignment on tensors in order to run
                                    						efficiently. As always, cuDNN recommends users to align tensors to 16 byte
                                    						boundaries that will be sufficiently aligned for any computational option in
                                    						cuDNN. Doing otherwise may cause performance regressions in cuDNN 8.0.x
                                    						compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and the output is
                                    						in FP16 (half float), there are cases where the numerical accuracy between
                                    						the different algorithms might differ. cuDNN 8.0.x users can target the
                                    						backend API to query the numerical notes of the algorithms to get the
                                    						information programmatically. There are cases where algo0 and algo1 will
                                    						have a reduced precision accumulation when users target the legacy API. In
                                    						all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests. 
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and backward
                                    						filter, grouped convolution with groups larger than 1 and with odd product
                                    						of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D convolution),
                                    							<samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on devices
                                    						older than NVIDIA Volta. To prevent a potential illegal memory access by an
                                    						instruction that only has a 16-bit version in NVIDIA Volta and later, pad at
                                    						least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">On K80 GPUs, when <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionForward()</samp></a> is used with
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp>
                                    						algorithm and half I/O data types a silent error might occur when the output
                                    						width <samp class="ph codeph">Q</samp> is <samp class="ph codeph">1</samp> and both height and width
                                    						padding are zero.
                                 </li>
                                 <li class="li liexpand">
                                    <div class="p">Several cuDNN APIs are unable to directly support computations using integer types
                                       								(<samp class="ph codeph">CUDNN_DATA_INT8</samp>,
                                       								<samp class="ph codeph">CUDNN_DATA_INT8x4</samp>,
                                       								<samp class="ph codeph">CUDNN_DATA_INT8x32</samp> or
                                       								<samp class="ph codeph">CUDNN_DATA_INT32</samp>). Floating types (particularly
                                       								<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>) are much more widely supported.
                                       							If an API does not support the desired type,
                                       								<samp class="ph codeph">cudnnTransformTensor()</samp> can be used to support the
                                       							use case by converting to/from a supported type and the desired type.
                                       							Here are the steps for doing so:<a name="rel-805__ol_hz5_r31_dmb" shape="rect">
                                          <!-- --></a><ol class="ol" id="rel-805__ol_hz5_r31_dmb">
                                          <li class="li">Convert all input tensors from their native type to a supported
                                             									type (<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> is recommended).
                                          </li>
                                          <li class="li">Run cuDNN API using the converted input tensors and output
                                             									tensor descriptors set as
                                             									<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>.
                                          </li>
                                          <li class="li">Convert all output tensors from a supported type to your desired
                                             									output type.
                                          </li>
                                       </ol>
                                       <div class="note note"><span class="notetitle">Note:</span> This will require extra memory use for the temporary buffers.
                                          								Further, this will introduce an additional round trip to memory that
                                          								might noticeably impact performance.
                                       </div>
                                    </div>
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use texture-based load
                                    						structure for performance improvements particularly in older hardware
                                    						architectures. Users can opt out of using texture using the environmental
                                    						variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this variable is
                                    						removed. Texture loading is turned off by default. Users who want to
                                    						continue to use texture-based load, can adapt the new backend API, and
                                    						toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnSpatialTfSamplerBackward()</samp> returns
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORT</samp> when the number of channels
                                    						exceeds 1024.
                                 </li>
                                 <li class="li liexpand">When using graph-capture, users should call the sub library version check API (for example,
                                    							<samp class="ph codeph"> cudnnOpsInferVersionChec()</samp>) to load the kernels in the
                                    						sub library before opening graph capture.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-804"><a name="rel-804" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-804" name="rel-804" shape="rect">1.27.&nbsp;cuDNN Release 8.0.4</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">This is the cuDNN 8.0.4 release notes. This release includes fixes from the previous
                           		cuDNN v8.0.x releases as well as the following additional changes. These release notes are
                           		applicable to both cuDNN and NVIDIA JetPack users of cuDNN unless appended specifically with
                           			<em class="ph i">(not applicable for Jetson platforms)</em>.
                        </p>
                        <div class="section" id="rel-804__section_l2m_s4c_2jb"><a name="rel-804__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">For previous cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-archived/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-804__section_f2l_vsd_cnb"><a name="rel-804__section_f2l_vsd_cnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:
                              <dl class="dl">
                                 <dt class="dt dlterm">GA102 support with improved convolution performance</dt>
                                 <dd class="dd">Now includes convolution heuristics targeting the NVIDIA GA102 GPU. <em class="ph i">(not applicable for
                                       								Jetson platforms)</em></dd>
                                 <dt class="dt dlterm">RNN API v8 sample</dt>
                                 <dd class="dd">The new RNN sample illustrating the usage of the new RNN version 8 API has been added. The
                                    							sample's workflow consists of the several routines to create RNN
                                    							descriptors, create RNN data descriptors, set up weight space, and
                                    							compute routines. The sample takes several input parameters that can set
                                    							up different RNN configurations and input data specifications (data
                                    							type, cell mode, bias mode, and so on).
                                 </dd>
                                 <dt class="dt dlterm">RNN functional and performance improvements</dt>
                                 <dd class="dd"><a name="rel-804__ul_dsp_lbn_cnb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-804__ul_dsp_lbn_cnb">
                                       <li class="li">When using RNN with padded data, the RNN padding and padding removal kernel at the
                                          									beginning and at end of the computation may achieve up to 4x
                                          									speedup, and has been generalized to work with any batch
                                          									sizes.
                                       </li>
                                       <li class="li">Updated the following API functions return codes:<a name="rel-804__ul_z5f_tbn_cnb" shape="rect">
                                             <!-- --></a><ul class="ul" id="rel-804__ul_z5f_tbn_cnb">
                                             <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBuildRNNDynamic" target="_blank" shape="rect"><samp class="ph codeph">cudnnBuildRNNDynamic()</samp></a></li>
                                             <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnCreatePersistentRNNPlan" target="_blank" shape="rect"><samp class="ph codeph">cudnnCreatePersistentRNNPlan()</samp></a></li>
                                             <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnRNNBackwardData_v8" target="_blank" shape="rect"><samp class="ph codeph">cudnnRNNBackwardData_v8()</samp></a></li>
                                             <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnRNNBackwardData" target="_blank" shape="rect"><samp class="ph codeph">cudnnRNNBackwardData()</samp></a></li>
                                             <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnRNNBackwardDataEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnRNNBackwardDataEx()</samp></a></li>
                                             <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSetDropoutDescriptor" target="_blank" shape="rect"><samp class="ph codeph">cudnnSetDropoutDescriptor()</samp></a></li>
                                          </ul>
                                       </li>
                                    </ul>
                                 </dd>
                                 <dt class="dt dlterm">ARM Server Base System Architecture (SBSA)</dt>
                                 <dd class="dd">Added support for ARM SBSA for Linux.</dd>
                              </dl>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Limitations</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">Samples can crash unless they are installed in a writable location.</li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit non-deterministic behavior when the cuDNN
                                       							8.0.4 library is built with CUDA Toolkit 10.2 or higher. This is the
                                       							result of a new buffer management and heuristics in the cuBLAS library.
                                       							As described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This is
                                       							caused by two buffer sizes (16 KB and 4 MB) used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting for a buffer of
                                       							that size to be released, a smaller buffer may be used with a different
                                       							GPU kernel. The kernel selection may affect numerical results. The user
                                       							can eliminate the non-deterministic behavior of cuDNN RNN and multihead
                                       							attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>.
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16 KB each in GPU
                                       							memory while the second setting creates two buffers of 4 MB each. The
                                       							default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte
                                    						alignment, including INT8 data in the cuDNN library. 
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN 8.0.4 now require increased alignment on tensors in
                                    						order to run efficiently. As always, cuDNN recommends users to align tensors
                                    						to 128-bit boundaries that will be sufficiently aligned for any
                                    						computational option in cuDNN. Doing otherwise may cause performance
                                    						regressions in cuDNN 8.0.4 compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and the output is
                                    						in FP16 (half float), there are cases where the numerical accuracy between
                                    						the different algorithms might differ. cuDNN 8.0.4 users can target the
                                    						backend API to query the numerical notes of the algorithms to get the
                                    						information programmatically. There are cases where algo0 and algo1 will
                                    						have a reduced precision accumulation when users target the legacy API. In
                                    						all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and backward
                                    						filter, grouped convolution with groups larger than 1 and with odd product
                                    						of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D convolution),
                                    							<samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on devices
                                    						older than NVIDIA Volta. To prevent a potential illegal memory access by an
                                    						instruction that only has a 16-bit version in NVIDIA Volta and later, pad at
                                    						least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">On K80 GPUs, when <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBiasActivationForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionForward()</samp></a> is used with
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp>
                                    						algorithm and half I/O data types a silent error might occur when the output
                                    						width <samp class="ph codeph">Q</samp> is <samp class="ph codeph">1</samp> and both height and width
                                    						padding are zero.
                                 </li>
                                 <li class="li liexpand">Several cuDNN APIs are unable to directly support computations using integer types
                                    							(<samp class="ph codeph">CUDNN_DATA_INT8</samp>, <samp class="ph codeph">CUDNN_DATA_INT8x4</samp>,
                                    							<samp class="ph codeph">CUDNN_DATA_INT8x32</samp> or
                                    						<samp class="ph codeph">CUDNN_DATA_INT32</samp>). Floating types (particularly
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>) are much more widely supported. If an
                                    						API does not support the desired type,
                                    							<samp class="ph codeph">cudnnTransformTensor()</samp> can be used to support the use
                                    						case by converting to/from a supported type and the desired type. Here are
                                    						the steps for doing so:<a name="rel-804__ol_hz5_r31_dmb" shape="rect">
                                       <!-- --></a><ol class="ol" id="rel-804__ol_hz5_r31_dmb">
                                       <li class="li">Convert all input tensors from their native type to a supported type
                                          									(<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> is recommended).
                                       </li>
                                       <li class="li">Run cuDNN API using the converted input tensors and output tensor
                                          								descriptors set as <samp class="ph codeph">CUDNN_DATA_FLOAT</samp>.
                                       </li>
                                       <li class="li">Convert all output tensors from a supported type to your desired
                                          								output type.
                                       </li>
                                    </ol>
                                    <div class="note note"><span class="notetitle">Note:</span> This will require extra memory use for the temporary buffers.
                                       							Further, this will introduce an additional round trip to memory that
                                       							might noticeably impact performance.
                                    </div>
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are
                                    						limited to <samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use texture-based load
                                    						structure for performance improvements particularly in older hardware
                                    						architectures. Users can opt out of using texture using the environmental
                                    						variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this variable is
                                    						removed. Texture loading is turned off by default. Users who want to
                                    						continue to use texture-based load, can adapt the new backend API, and
                                    						toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-804__section_ct1_x5s_cnb"><a name="rel-804__section_ct1_x5s_cnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Deprecated Features</h3>
                           <div class="p">The following features are deprecated in cuDNN 8.0.4:<a name="rel-804__ul_qhv_x5s_cnb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-804__ul_qhv_x5s_cnb">
                                 <li class="li">Support for Ubuntu 18.04 ppc64le builds will be dropped post cuDNN
                                    						8.0.4.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-804__section_bjg_dtd_cnb"><a name="rel-804__section_bjg_dtd_cnb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> and
                                    							<samp class="ph codeph">cudnnGetConvolutionBackwardFilterWorkspaceSize()</samp> can
                                    						result in a segmentation fault in multi-threaded usage due to a race
                                    						condition. This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">The <samp class="ph codeph">libfreeimage.a</samp> library in the RHEL 8 ppc64le RPM package was for the
                                    						wrong architecture. This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">In previous cuDNN versions, <samp class="ph codeph">cudnnRNNBackwardData()</samp> or
                                    							<samp class="ph codeph">cudnnRNNBackwardDataEx()</samp> may return
                                    							<samp class="ph codeph">CUDNN_STATUS_INTERNAL_ERROR</samp>, NaN-s, or
                                    						non-deterministic finite values when
                                    							<samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_STATIC</samp> was selected. These
                                    						issues occurred mainly on smaller GPUs, such as NVIDIA Turing with 30 or 36
                                    						SMs and smaller <samp class="ph codeph">hiddenSize</samp> values. Most of those issues
                                    						have been fixed in this release. However, configurations such as
                                    							<samp class="ph codeph">hiddenSize=128</samp>, LSTM, FP32 with
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> may still output
                                    						non-deterministic results.
                                 </li>
                                 <li class="li liexpand">There was an issue in upgrading the cuDNN version using the RPM and Debian packages in the
                                    						8.0.3 version. This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">The ResNet-50 native FP32 inference issues have been fixed on NVIDIA Volta and NVIDIA
                                    						Turing. Few performance regressions exist in the NVIDIA Ampere Architecture
                                    						GPU.
                                 </li>
                                 <li class="li liexpand">cuDNN exhibited performance regressions for GoogLeNet and U-Net on V100. This issue has
                                    						been fixed in this release.
                                 </li>
                                 <li class="li liexpand">cuDNN exhibited performance regressions for VGG16 on GA100. This issue has been fixed in
                                    						this release.
                                 </li>
                                 <li class="li liexpand">The performance regression across Tacotron2 and WaveGlow seen on the NVIDIA Turing
                                    						architecture have been fixed.
                                 </li>
                                 <li class="li liexpand">The performance regressions in the FastPitch network seen on the NVIDIA Volta and NVIDIA
                                    						Turing architecture have been fixed.
                                 </li>
                                 <li class="li liexpand">The cuDNN API unconditionally triggers CUDA context initialization. This causes unnecessary
                                    						host-side performance overhead. This is an issue that was introduced in
                                    						cuDNN version 8.0.2. This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Some ResNet-50 and SSD mixed precision inference use-cases may have performance regressions
                                    						compared to cuDNN 7.6 on V100. V-Net 3D models might have performance
                                    						regressions on NVIDIA Turing based architectures. This issue has been fixed
                                    						in this release.
                                 </li>
                                 <li class="li liexpand">Previous cuDNN 8 releases exhibited performance regressions when compared to version 7.6,
                                    						for some important convolutional networks on the NVIDIA Pascal GPU
                                    						architecture. In particular, the performance regressions of ResNet-50 seen
                                    						previously on NVIDIA Pascal with cuDNN versions 8.0.3 and earlier, are fixed
                                    						with this release.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp> could result in incorrect results
                                    						when the <samp class="ph codeph">alpha2</samp> value is zero and the device buffer
                                    							<samp class="ph codeph">zData</samp> contains NaN. This issue has been fixed in this
                                    						release.
                                 </li>
                                 <li class="li liexpand">When using <samp class="ph codeph">cudnnRNN*Ex()</samp> APIs, if the layout of RNN data is
                                    							<samp class="ph codeph">CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_UNPACKED</samp> or
                                    							<samp class="ph codeph">CUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED</samp>, and if the
                                    						batch size is larger than 6144 on NVIDIA Volta or NVIDIA Ampere Architecture
                                    						A100 GPUs, or larger than 4096 on NVIDIA Turing GPUs,
                                    							<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> would be returned. This
                                    						issue has been fixed in this release. cuDNN supports arbitrary batch
                                    						size.
                                 </li>
                                 <li class="li liexpand">When the user upgraded from cuDNN 8.0.2 to 8.0.3 through the Debian or RPM package, users
                                    						had to manually uninstall the old <samp class="ph codeph">libcudnn8-doc</samp> package
                                    						before they installed <samp class="ph codeph">libcudnn8-samples_*.deb/rpm</samp>,
                                    						otherwise a file conflict could happen. This has been fixed and is no longer
                                    						the case in the 8.0.4 release.
                                 </li>
                                 <li class="li liexpand">Performance regressions on NVIDIA Turing, NVIDIA Volta, and NVIDIA Pascal architectures for
                                    						True Half convolutions have been resolved.
                                 </li>
                                 <li class="li liexpand">When using <samp class="ph codeph">cudnnRNN*</samp> APIs with the problem sizes (input
                                    						size, hidden size) not being multiples of 16 for FP16 tensors or multiples
                                    						of 8 for FP32 tensors, users encountered a return status of
                                    							<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED in cudnn built against cuda
                                       							11.0</samp>. This issue has been fixed with cuDNN built against CUDA
                                    						11.1.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">When using <samp class="ph codeph">cudnnRNN*</samp> APIs with the problem sizes (input size, hidden size)
                                    						not being multiples of 16 for FP16 tensors or multiples of 8 for FP32
                                    						tensors, users encountered a return status of
                                    							<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp>. This issue affects
                                    						earlier cuDNN 8.0.1 Preview and cuDNN 8.0.2 releases built against CUDA
                                    						11.0.
                                 </li>
                                 <li class="li liexpand">There is a known minor performance regression on small batch sizes for ResNet-50 native
                                    						FP32 inference that exists on the NVIDIA Ampere Architecture GPU.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-803"><a name="rel-803" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-803" name="rel-803" shape="rect">1.28.&nbsp;cuDNN Release 8.0.3</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">This is the cuDNN 8.0.3 release notes. This release includes fixes from the previous
                           		cuDNN v8.0.x releases as well as the following additional changes. These release notes are
                           		applicable to both cuDNN and NVIDIA JetPack users of cuDNN unless appended specifically with
                           			<em class="ph i">(not applicable for Jetson platforms)</em>.
                        </p>
                        <div class="section" id="rel-803__section_l2m_s4c_2jb"><a name="rel-803__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">For previous cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-archived/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-803__section_jpr_jww_3mb"><a name="rel-803__section_jpr_jww_3mb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">
                              <dl class="dl">
                                 <dt class="dt dltermexpand">cuDNN backend API</dt>
                                 <dd class="dd">
                                    <div class="p">Documentation for the cuDNN backend API has been included in this release. Users specify
                                       								the computational case, set up an execution plan for it, and execute
                                       								the computation using numerous descriptors. The typical use pattern
                                       								for a descriptor with attributes consists of the following sequence
                                       								of API calls:<a name="rel-803__ol_vgn_wsf_4mb" shape="rect">
                                          <!-- --></a><ol class="ol" id="rel-803__ol_vgn_wsf_4mb">
                                          <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#backend-cudnnBackendCreateDescriptor" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackendCreateDescriptor()</samp></a>
                                             										creates a descriptor of a specified type.
                                          </li>
                                          <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#backend-cudnnBackendSetAttribute" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackendSetAttribute()</samp></a>
                                             										sets the values of a settable attribute for the descriptor.
                                             										All required attributes must be set before the next
                                             										step.
                                          </li>
                                          <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#backend-cudnnBackendFinalize" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackendFinalize()</samp></a>
                                             										finalizes the descriptor.
                                          </li>
                                          <li class="li"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#backend-cudnnBackendGetAttribute" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackendGetAttribute()</samp></a>
                                             										gets the values of an attribute from a finalized
                                             										descriptor.
                                          </li>
                                       </ol>
                                    </div>
                                    <p class="p">For more information, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnn-backend-api" target="_blank" shape="rect">NVIDIA cuDNN Backend
                                          								API</a>.
                                    </p>
                                 </dd>
                              </dl>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Limitations</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">Samples can crash unless they are installed in a writable location.</li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit non-deterministic behavior when the cuDNN
                                       							8.0.3 library is built with CUDA Toolkit 10.2 or higher. This is the
                                       							result of a new buffer management and heuristics in the cuBLAS library.
                                       							As described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This is
                                       							caused by two buffer sizes (16 KB and 4 MB) used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting for a buffer of
                                       							that size to be released, a smaller buffer may be used with a different
                                       							GPU kernel. The kernel selection may affect numerical results. The user
                                       							can eliminate the non-deterministic behavior of cuDNN RNN and multihead
                                       							attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>.
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16 KB each in GPU
                                       							memory while the second setting creates two buffers of 4 MB each. The
                                       							default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte alignment,
                                    						including INT8 data in the cuDNN library. 
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN 8.0.3 now require increased alignment on tensors in
                                    						order to run efficiently. As always, cuDNN recommends users to align tensors
                                    						to 128-bit boundaries that will be sufficiently aligned for any
                                    						computational option in cuDNN. Doing otherwise may cause performance
                                    						regressions in cuDNN 8.0.3 compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and the output is
                                    						in FP16 (half float), there are cases where the numerical accuracy between
                                    						the different algorithms might differ. cuDNN 8.0.3 users can target the
                                    						backend API to query the numerical notes of the algorithms to get the
                                    						information programmatically. There are cases where algo0 and algo1 will
                                    						have a reduced precision accumulation when users target the legacy API. In
                                    						all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and backward
                                    						filter, grouped convolution with groups larger than 1 and with odd product
                                    						of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D convolution),
                                    							<samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on devices
                                    						older than NVIDIA Volta. To prevent a potential illegal memory access by an
                                    						instruction that only has a 16-bit version in NVIDIA Volta and later, pad at
                                    						least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">On K80 GPUs, when <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBiasActivationForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionForward()</samp></a> is used with
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp>
                                    						algorithm and half I/O data types a silent error might occur when the output
                                    						width <samp class="ph codeph">Q</samp> is <samp class="ph codeph">1</samp> and both height and width
                                    						padding are zero.
                                 </li>
                                 <li class="li liexpand">Several cuDNN APIs are unable to directly support computations using integer types
                                    							(<samp class="ph codeph">CUDNN_DATA_INT8</samp>, <samp class="ph codeph">CUDNN_DATA_INT8x4</samp>,
                                    							<samp class="ph codeph">CUDNN_DATA_INT8x32</samp> or
                                    						<samp class="ph codeph">CUDNN_DATA_INT32</samp>). Floating types (particularly
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>) are much more widely supported. If an
                                    						API does not support the desired type,
                                    							<samp class="ph codeph">cudnnTransformTensor()</samp> can be used to support the use
                                    						case by converting to/from a supported type and the desired type. Here are
                                    						the steps for doing so:<a name="rel-803__ol_hz5_r31_dmb" shape="rect">
                                       <!-- --></a><ol class="ol" id="rel-803__ol_hz5_r31_dmb">
                                       <li class="li">Convert all input tensors from their native type to a supported type
                                          									(<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> is recommended).
                                       </li>
                                       <li class="li">Run cuDNN API using the converted input tensors and output tensor
                                          								descriptors set as <samp class="ph codeph">CUDNN_DATA_FLOAT</samp>.
                                       </li>
                                       <li class="li">Convert all output tensors from a supported type to your desired
                                          								output type.
                                       </li>
                                    </ol>
                                    <div class="note note"><span class="notetitle">Note:</span> This will require extra memory use for the temporary buffers.
                                       							Further, this will introduce an additional round trip to memory that
                                       							might noticeably impact performance.
                                    </div>
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are limited to
                                    							<samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use texture-based load
                                    						structure for performance improvements particularly in older hardware
                                    						architectures. Users can opt out of using texture using the environmental
                                    						variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this variable is
                                    						removed. Texture loading is turned off by default. Users who want to
                                    						continue to use texture-based load, can adapt the new backend API, and
                                    						toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                                 <li class="li liexpand">In the backend API, convolution forward engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX=1</samp> is not supported when
                                    						the product (<samp class="ph codeph">channels * height * width</samp>) of the input image
                                    						exceeds 536,870,912 that is 2^29.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-803__section_pg5_wwr_qmb"><a name="rel-803__section_pg5_wwr_qmb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">For <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a>, the 3D
                                    						convolution table, <samp class="ph codeph">wDesc: _NCHW</samp>, <samp class="ph codeph">_ALGO_1</samp>
                                    						and <samp class="ph codeph">FFT_TILING</samp> had incorrect data fields. This has been
                                    						fixed in this release.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with pooling mode
                                    							<samp class="ph codeph">CUDNN_POOLING_MAX</samp> might return incorrect result when
                                    						one of the spatial dimensions has negative padding and the output tensor is
                                    						larger than the value recommended by <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPoolingNdForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPoolingNdForwardOutputDim()</samp></a> or
                                    							<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnGetPooling2dForwardOutputDim" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetPooling2dForwardOutputDim()</samp></a>. This
                                    						issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">In <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnPoolingForward()</samp></a> with average-pooling,
                                    						when the output tensor data is INT8 type, it is possible for some pixels
                                    						result to be off by 1. Note that <samp class="ph codeph">cudnnPoolingForward()</samp>
                                    						rounds to the nearest-even integer. This issue has been fixed in this
                                    						release.
                                 </li>
                                 <li class="li liexpand">The performance of <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBiasActivationForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp></a> for
                                    						INT8x4 use cases on NVIDIA Volta and NVIDIA Turing, INT8x32 use cases on
                                    						NVIDIA Turing, FP32, and pseudo-FP16 use cases on NVIDIA Volta, NVIDIA
                                    						Turing, and NVIDIA Ampere Architecture GPU have been improved.
                                 </li>
                                 <li class="li liexpand">We have updated our public headers to fully reflect the documented dependencies between the
                                    						six sub libraries.
                                 </li>
                                 <li class="li liexpand">There were <samp class="ph codeph">libcudnn_ops/cnn/adv_infer/train_static.a</samp> binaries in the cuDNN
                                    						Debian and tgz packages. Users were advised not to link against those and
                                    						link against <samp class="ph codeph">libcudnn_static.a</samp> instead. Those binaries have
                                    						been removed from the release packages.
                                 </li>
                                 <li class="li liexpand">On NVIDIA Volta and NVIDIA Pascal architectures, performance regressions were present for
                                    						various <samp class="ph codeph">TRUE_HALF</samp> convolutions. This has been fixed in this
                                    						release.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, API functions
                                    							<samp class="ph codeph">cudnnGetConvolution*Algorithm_v7()</samp> return a workspace
                                    						size in the result for <samp class="ph codeph">algo1</samp> that is inconsistent with the
                                    						result of the corresponding <samp class="ph codeph">cudnnGet*Workspace()</samp> calls if
                                    						the math type of the convolution descriptor is set to
                                    							<samp class="ph codeph">CUDNN_FMA_MATH</samp>. This issue has been fixed in this
                                    						release.
                                 </li>
                                 <li class="li liexpand">The new RNN APIs: <samp class="ph codeph">cudnnRNNForward()</samp>,
                                    							<samp class="ph codeph">cudnnRNNBackwardData_v8()</samp>, and
                                    							<samp class="ph codeph">cudnnRNNBackwardWeights_v8()</samp> were available as a
                                    						preview in the cuDNN 8.0.2 release. They no longer hold preview status.
                                 </li>
                                 <li class="li liexpand">When using <samp class="ph codeph">cudnnRNN*Ex()</samp> APIs, if the user planned to use
                                    							<samp class="ph codeph">CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_UNPACKED</samp> or
                                    							<samp class="ph codeph">CUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED</samp> as the
                                    						layout of the RNN data descriptors, the user would have had to call
                                    							<samp class="ph codeph">cudnnSetRNNPaddingMode()</samp> to set the mode to
                                    							<samp class="ph codeph">CUDNN_RNN_PADDED_IO_ENABLED</samp> after initializing an
                                    							<samp class="ph codeph">RNNDescriptor</samp> but before calling
                                    							<samp class="ph codeph">cudnnGetRNNWorkspaceSize()</samp>. Not doing this would result
                                    						in <samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp>. We have added internal
                                    						checks to return <samp class="ph codeph">CUDNN_STATUS_BAD_PARAM</samp> to prevent hitting
                                    							<samp class="ph codeph">EXECUTION_FAILED</samp>.
                                 </li>
                                 <li class="li liexpand">When <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardTrainingEx" target="_blank" shape="rect"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx()</samp></a>
                                    						is called with NHWC tensors with pseudo-half configuration, under rare
                                    						occasions the kernel would produce incorrect results, including possible
                                    						NaNs in the results. This has been fixed in this release. This issue affects
                                    						earlier releases since 7.4.1.
                                 </li>
                                 <li class="li liexpand">Fused convolution-scale-bias-activation with per-channel α1 and α2 scaling gives incorrect
                                    						results when the reorder type in the convolution descriptor is set to
                                    							<samp class="ph codeph">CUDNN_NO_REORDER</samp>. This is an issue in cuDNN version
                                    						8.0.2 This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">On NVIDIA Ampere Architecture GA100, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnConvolutionBackwardData" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardData()</samp></a> for Tensor
                                    						Core enabled problems with half input and output could, in rare cases, could
                                    						produce incorrect results; the same could happen for users of <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBackendExecute" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackendExecute()</samp></a> using engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp> 57 for backward data.
                                    						This has been fixed in this release. <em class="ph i">(not applicable for Jetson
                                       							platforms)</em></li>
                                 <li class="li liexpand">There was a performance regression in MaskRCNN inference with automatic
                                    						mixed precision on V100. This has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Two-dimensional forward convolutions using algo1 may segfault when the filter size is
                                    						large. For example, we have observed this issue when the filter width and
                                    						height are more than or equal to 363. This has been fixed in this
                                    						release.
                                 </li>
                                 <li class="li liexpand">For some 3D spatial non-Tensor-Core convolutions on Maxwell, NVIDIA Pascal, NVIDIA Volta,
                                    						and NVIDIA Turing architectures, <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackwardFilter()</samp></a> can return incorrect
                                    						results when the convolution width padding exceeds the value
                                    							<samp class="ph codeph">(filterWidth - 1)/2</samp>. Likewise, users of <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBackendExecute" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackendExecute()</samp></a> can experience the same
                                    						issue when using the engine with
                                    							<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp> 32 for backward filter.
                                    						The issue affecting <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackwardFilter()</samp></a> has been fixed in this
                                    						release. With <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBackendFinalize" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackendFinalize()</samp></a>, an engine descriptor
                                    						with <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp> 32 and a backward
                                    						filter operation that satisfies the previous condition will return
                                    							<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">Occasionally, inaccurate results were observed in outputs of the
                                    							<samp class="ph codeph">cudnnRNNBackwardWeights()</samp> and
                                    							<samp class="ph codeph">cudnnRNNBackwardWeightsEx()</samp> functions when the RNN cell
                                    						type was GRU and the NVIDIA Ampere Architecture GPU was used with FP32 I/O
                                    						and <samp class="ph codeph">mathType</samp> of <samp class="ph codeph">CUDNN_DEFAULT_MATH</samp> or
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH</samp>. Users may switch to
                                    							<samp class="ph codeph">CUDNN_FMA_MATH</samp> as a temporary workaround. This issue is
                                    						being investigated.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnRNN*()</samp> with LSTM mode may produce inaccurate results on the
                                    							<samp class="ph codeph">cy</samp> outputs when clipping is enabled on all GPUs. This
                                    						issue exists in previous cuDNN releases as well.
                                 </li>
                                 <li class="li liexpand">On NVIDIA Volta and NVIDIA Pascal architectures, performance regressions may be present for
                                    						various <samp class="ph codeph">TRUE_HALF</samp> convolutions.
                                 </li>
                                 <li class="li liexpand">When the user is using <samp class="ph codeph">cudnnRNN*</samp> APIs with the problem
                                    						sizes (input size, hidden size) being not multiples of 16 for FP16 tensors
                                    						or multiples of 8 for FP32 tensors, users may encounter a return status of
                                    							<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp>. This issue also affects
                                    						earlier releases cuDNN 8.0.1 Preview and cuDNN 8.0.2.
                                 </li>
                                 <li class="li liexpand">Some ResNet-50 and SSD mixed precision inference use-cases may have performance regressions
                                    						compared to cuDNN 7.6 on V100. V-Net 3D models might have performance
                                    						regressions on NVIDIA Turing based architectures. 
                                 </li>
                                 <li class="li liexpand">When using <samp class="ph codeph">cudnnRNN*Ex()</samp> APIs, if the user used
                                    							<samp class="ph codeph">CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_UNPACKED</samp> or
                                    							<samp class="ph codeph">CUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED</samp> as the
                                    						layout of the RNN data descriptors, and if the batch size is larger than
                                    						6144 on NVIDIA Volta or NVIDIA Ampere Architecture A100 GPUs, or larger than
                                    						4096 on NVIDIA Turing GPUs, <samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp>
                                    						would be returned. 
                                 </li>
                                 <li class="li liexpand">Documentation of the backend API is not complete. The
                                    							<samp class="ph codeph">CUDNN_BACKEND_OPERATION_GEN_STATS_DESCRIPTOR</samp> and
                                    							<samp class="ph codeph">CUDNN_BACKEND_OPERATION_POINTWISE_DESCRIPTOR</samp> descriptor
                                    						types will be documented in a future release.
                                 </li>
                                 <li class="li liexpand">The <samp class="ph codeph">conv_sample_v8.0</samp> sample is not included in the Debian and RPM
                                    						packages. This will be fixed in a future release.
                                 </li>
                                 <li class="li liexpand">The <samp class="ph codeph">libfreeimage.a</samp> library in the RHEL 8 ppc64le RPM is for the wrong
                                    						architecture. This will be fixed in a future release.
                                 </li>
                                 <li class="li liexpand">When the user is upgrading from cuDNN 8.0.2 to 8.0.3 through the Debian or RPM package,
                                    						before installing <samp class="ph codeph">libcudnn8-samples_*.deb/rpm</samp>, users should
                                    						manually uninstall the old <samp class="ph codeph">libcudnn8-doc</samp> package, otherwise
                                    						a file conflict may happen.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-802"><a name="rel-802" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-802" name="rel-802" shape="rect">1.29.&nbsp;cuDNN Release 8.0.2</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">This is the cuDNN 8.0.2 release notes and first GA release of cuDNN 8.x. This release
                           		includes fixes from the previous cuDNN v8.0.x releases as well as the following additional
                           		changes. These release notes are applicable to both cuDNN and NVIDIA JetPack users of cuDNN
                           		unless appended specifically with <em class="ph i">(not applicable for Jetson platforms)</em>.
                        </p>
                        <div class="section" id="rel-802__section_l2m_s4c_2jb"><a name="rel-802__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">For previous cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-archived/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-802__section_jpr_jww_3mb"><a name="rel-802__section_jpr_jww_3mb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">
                              <dl class="dl">
                                 <dt class="dt dltermexpand">cuDNN 8.0.1 Preview and 8.0.0 Preview</dt>
                                 <dd class="dd">The key features mentioned in cuDNN <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_8.html#rel-801-Preview" target="_blank" shape="rect">8.0.1 Preview</a> and <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_8.html#rel-800-Preview" target="_blank" shape="rect">8.0.0 Preview</a> are now GA
                                    							quality in this release.
                                 </dd>
                                 <dt class="dt dltermexpand">Added new API functions to the documentation</dt>
                                 <dd class="dd"><samp class="ph codeph">cudnnRNNBackwardData_v8()</samp> and
                                    								<samp class="ph codeph">cudnnRNNBackwardWeights_v8()</samp> are now documented in
                                    							the cudnn_adv_train.so Library. For a list of functions and data types
                                    							that were added in this release, see <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#release-800" target="_blank" shape="rect">API changes for cuDNN
                                       							8.0.2</a>.
                                 </dd>
                                 <dt class="dt dltermexpand">TF32 performance</dt>
                                 <dd class="dd">
                                    <div class="p"><a name="rel-802__ul_syv_hx3_lmb" shape="rect">
                                          <!-- --></a><ul class="ul" id="rel-802__ul_syv_hx3_lmb">
                                          <li class="li">TF32 for 3D convolutions and deconvolution performance is
                                             										significantly better, up to 3.9x, compared to cuDNN
                                             										8.0.1.
                                          </li>
                                          <li class="li">TF32 for grouped convolutions on A100 were improved up to
                                             										1.5x performance compared to cuDNN 8.0.1 on ResNext
                                             										convolution layers and up to 3x the performance compared to
                                             										V100 with cuDNN v7.6. <em class="ph i">(not applicable for Jetson
                                                											platforms)</em></li>
                                       </ul>
                                    </div>
                                    <p class="p">The above performance improvements were measured using only cuDNN
                                       								operations. The observed performance improvements will depend on a
                                       								number of factors, such as non-cuDNN operations, kernel run time,
                                       								and model architecture type.
                                    </p>
                                 </dd>
                                 <dt class="dt dltermexpand">Performance improvements</dt>
                                 <dd class="dd">This release includes performance improvements on all architectures for 2D and 3D grouped
                                    							convolutions compared with version 7.6. Additionally, we improved kernel
                                    							selection heuristics on several known <a class="xref" href="https://github.com/NVIDIA/DeepLearningExamples" target="_blank" shape="rect">deep learning GitHub examples (also known as model
                                       								scripts)</a>.
                                 </dd>
                              </dl>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Limitations</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">Samples can crash unless they are installed in a writable location.</li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit non-deterministic behavior when the cuDNN
                                       							8.0.2 library is built with CUDA Toolkit 10.2 or higher. This is the
                                       							result of a new buffer management and heuristics in the cuBLAS library.
                                       							As described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This is
                                       							caused by two buffer sizes (16 KB and 4 MB) used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting for a buffer of
                                       							that size to be released, a smaller buffer may be used with a different
                                       							GPU kernel. The kernel selection may affect numerical results. The user
                                       							can eliminate the non-deterministic behavior of cuDNN RNN and multihead
                                       							attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>.
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16 KB each in GPU
                                       							memory while the second setting creates two buffers of 4 MB each. The
                                       							default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte alignment,
                                    						including INT8 data in the cuDNN library. 
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN 8.0.2 now require increased alignment on tensors in
                                    						order to run efficiently. As always, cuDNN recommends users to align tensors
                                    						to 128-bit boundaries that will be sufficiently aligned for any
                                    						computational option in cuDNN. Doing otherwise may cause performance
                                    						regressions in cuDNN 8.0.2 compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and the output is
                                    						in FP16 (half float), there are cases where the numerical accuracy between
                                    						the different algorithms might differ. cuDNN 8.0.2 users can target the
                                    						backend API to query the numerical notes of the algorithms to get the
                                    						information programmatically. There are cases where algo0 and algo1 will
                                    						have a reduced precision accumulation when users target the legacy API. In
                                    						all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and backward
                                    						filter, grouped convolution with groups larger than 1 and with odd product
                                    						of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D convolution),
                                    							<samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on devices
                                    						older than NVIDIA Volta. To prevent a potential illegal memory access by an
                                    						instruction that only has a 16-bit version in NVIDIA Volta and above, pad at
                                    						least one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">On K80 GPUs, when <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBiasActivationForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionForward()</samp></a> is used with
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp>
                                    						algorithm and half I/O data types a silent error might occur when the output
                                    						width <samp class="ph codeph">Q</samp> is <samp class="ph codeph">1</samp> and both height and width
                                    						padding are zero.
                                 </li>
                                 <li class="li liexpand">Several cuDNN APIs are unable to directly support computations using integer types
                                    							(<samp class="ph codeph">CUDNN_DATA_INT8</samp>, <samp class="ph codeph">CUDNN_DATA_INT8x4</samp>,
                                    							<samp class="ph codeph">CUDNN_DATA_INT8x32</samp> or
                                    						<samp class="ph codeph">CUDNN_DATA_INT32</samp>). Floating types (particularly
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>) are much more widely supported. If an
                                    						API does not support the desired type,
                                    							<samp class="ph codeph">cudnnTransformTensor()</samp> can be used to support the use
                                    						case by converting to/from a supported type and the desired type. Here are
                                    						the steps for doing so:<a name="rel-802__ol_hz5_r31_dmb" shape="rect">
                                       <!-- --></a><ol class="ol" id="rel-802__ol_hz5_r31_dmb">
                                       <li class="li">Convert all input tensors from their native type to a supported type
                                          									(<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> is recommended).
                                       </li>
                                       <li class="li">Run cuDNN API using the converted input tensors and output tensor
                                          								descriptors set as <samp class="ph codeph">CUDNN_DATA_FLOAT</samp>.
                                       </li>
                                       <li class="li">Convert all output tensors from a supported type to your desired
                                          								output type.
                                       </li>
                                    </ol>
                                    <div class="note note"><span class="notetitle">Note:</span> This will require extra memory use for the temporary buffers.
                                       							Further, this will introduce an additional round trip to memory that
                                       							might noticeably impact performance.
                                    </div>
                                 </li>
                                 <li class="li liexpand">In INT8x32 Tensor Core cases, the parameters supported by cuDNN v7.6 are limited to
                                    							<samp class="ph codeph">W &gt;= (R-1) * dilationW &amp;&amp; H &gt;= (S-1) *
                                       							dilationH</samp>, whereas, in cuDNN v8.0.x, <samp class="ph codeph">W == (R-1) *
                                       							dilationW || H == (S-1) * dilationH</samp> cases are no longer
                                    						supported.
                                 </li>
                                 <li class="li liexpand">In prior versions of cuDNN, some convolution algorithms can use texture-based load
                                    						structure for performance improvements particularly in older hardware
                                    						architectures. Users can opt out of using texture using the environmental
                                    						variable <samp class="ph codeph">CUDNN_TEXOFF_DBG</samp>. In cuDNN 8.x, this variable is
                                    						removed. Texture loading is turned off by default. Users who want to
                                    						continue to use texture-based load, can adapt the new backend API, and
                                    						toggle the engine knob <samp class="ph codeph">CUDNN_KNOB_TYPE_USE_TEX</samp> to
                                    							<samp class="ph codeph">1</samp> for engines that support texture-based load
                                    						instructions.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:<a name="rel-802__ul_tsg_jwc_jmb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-802__ul_tsg_jwc_jmb">
                                 <li class="li liexpand">The implementation of <samp class="ph codeph">cuDNNLRNCrossChannelBackward()</samp> for even-sized
                                    						normalization windows was incorrect in all previous releases. This issue has
                                    						been fixed in this release.
                                 </li>
                                 <li class="li liexpand">There is not a dedicated API to query the supported or the most performant algo for
                                    							<samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp> in cuDNN. It is
                                    						not recommended to query <samp class="ph codeph">w</samp> using
                                    							<samp class="ph codeph">cudnnGetConvolutionForwardAlgorithm_v7</samp>. Instead, we
                                    						recommend using the cuDNN version 8 backend API. The number of supported
                                    						engines can be queried using enum
                                    							<samp class="ph codeph">CUDNN_ATTR_OPERATIONGRAPH_ENGINE_GLOBAL_COUNT</samp> from an
                                    						operation graph descriptor using
                                    						<samp class="ph codeph">cudnnBackendGetAttribute()</samp>.
                                 </li>
                                 <li class="li liexpand">A <samp class="ph codeph">memcheck</samp> error may have occurred on cuDNN version 7.x builds when
                                    						calling <samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> on NVIDIA Volta or
                                    						NVIDIA Turing GPUs. This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Various convolutions that exhibited sub-optimal performance on GA100 GPUs are now achieving
                                    						ideal performance. <em class="ph i">(not applicable for Jetson platforms)</em></li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnCnnTrainVersionCheck()</samp> and
                                    							<samp class="ph codeph">cudnnCnnInferVersionCheck()</samp> were missing in past
                                    						releases. This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">Documentation of RNN new APIs and deprecations is not complete. The
                                    							<samp class="ph codeph">cudnnRNNBackwardData_v8()</samp> and
                                    							<samp class="ph codeph">cudnnRNNBackwardWeights_v8()</samp> have been added to this
                                    						release.
                                 </li>
                                 <li class="li liexpand">cuDNN 8.0.1 built with Windows and CUDA 11.0 RC had reduced performance on 2D, 3D, and
                                    						grouped convolutions compared to Linux. This issue has been fixed in this
                                    						release. <em class="ph i">(not applicable for Jetson platforms)</em></li>
                                 <li class="li liexpand">There was a known issue in cuDNN 8.0.1 when linking statically to cuDNN and using the
                                    						library's 3D algo1 backward filter convolutions. Users would see that the
                                    						library emits an internal error or incorrectly state that a shared library
                                    						was missing. This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">When using an RPM file on RedHat for installation, upgrading from cuDNN v7 to cuDNN v8
                                    						directly or indirectly using TensorRT 7.1.3 would cause installation errors.
                                    						This issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">The implementation of <samp class="ph codeph">cuDNNLRNCrossChannelBackward</samp> was inconsistent with
                                    						the implementation of <samp class="ph codeph">cuDNNLRNCrossChannelForward</samp> and
                                    						returned incorrect results when the normalization window was even. This
                                    						issue has been fixed in this release.
                                 </li>
                                 <li class="li liexpand">RNN APIs in cuDNN v8.0.1, compiled with CUDA 11.0, used an incorrect default
                                    						down-conversion on GPUs with CUDA SM version SM80 (NVIDIA Ampere
                                    						Architecture GPU family) when supplied input data and weights have the
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> type and
                                    							<samp class="ph codeph">cudnnMathType_t</samp> set using
                                    							<samp class="ph codeph">cudnnSetRNNMatrixMathType()</samp> is
                                    							<samp class="ph codeph">CUDNN_DEFAULT_MATH</samp> or
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH</samp>. Instead of using the default TF32
                                    						computation when Tensor Cores are used, a down conversion to FP16
                                    						(half-precision) was performed; same as in the
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> mode. This
                                    						introduced a lower dynamic range of intermediate data but possibly faster
                                    						execution. To disable the automatic down conversion of
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> weights and data in RNN APIs, the user
                                    						needed to set the environmental variable
                                    							<samp class="ph codeph">NVIDIA_TF32_OVERRIDE</samp> to <samp class="ph codeph">0</samp> (notice this
                                    						would have disabled the use of TF32 in the entire library, which might have
                                    						a performance impact on CNNs that are not affected by this issue). Another
                                    						workaround was to assign the <samp class="ph codeph">CUDNN_FMA_MATH</samp> mode to the
                                    							<samp class="ph codeph">cudnnMathType_t</samp> argument in
                                    							<samp class="ph codeph">cudnnSetRNNMatrixMathType()</samp>. Due to this, the A100 GPU
                                    						TF32 feature was not accessible for RNNs in cuDNN v8.0.1. This issue has
                                    						been fixed in this release. <em class="ph i">(not applicable for Jetson
                                       						platforms)</em></li>
                                 <li class="li liexpand">cuDNN convolution APIs may return <samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> when the
                                    						number of input or output channels equals to or exceeds 2097152. This issue
                                    						exists for all cuDNN 8.0.x releases. This issue has been fixed in this
                                    						release.
                                 </li>
                                 <li class="li liexpand">Since version 8.0.0 Preview, <samp class="ph codeph">cudnnConvolutionForward()</samp>,
                                    							<samp class="ph codeph">cudnnConvolutionBackwardData()</samp>, and
                                    							<samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> erroneously returned
                                    							<samp class="ph codeph">CUDNN_STATUS_INTERNAL_ERROR</samp> when the workspace size
                                    						argument value was less than the required workspace size as returned by
                                    						their respective <samp class="ph codeph">cudnnGetWorkspace()</samp> API. This issue has
                                    						been fixed and <samp class="ph codeph">CUDNN_STATUS_BAD_PARAMS</samp> is returned as
                                    						documented.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">In this release, the performance of <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBiasActivationForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp></a> for
                                    						true-half use cases on NVIDIA Pascal, INT8x4 use cases on NVIDIA Volta, and
                                    						NVIDIA Turing, compared to version 7.6 is still lower. In addition, FP32 and
                                    						pseudo-FP16 performance on NVIDIA Volta, NVIDIA Turing, and the NVIDIA
                                    						Ampere Architecture GPU is still not fully optimized.
                                 </li>
                                 <li class="li liexpand">The new RNN APIs: <samp class="ph codeph">cudnnRNNForward()</samp>,
                                    							<samp class="ph codeph">cudnnRNNBackwardData_v8()</samp>, and
                                    							<samp class="ph codeph">cudnnRNNBackwardWeights_v8()</samp> are available as a preview
                                    						in the cuDNN 8.0.2 release.
                                 </li>
                                 <li class="li liexpand">Occasionally, inaccurate results were observed in outputs of the
                                    							<samp class="ph codeph">cudnnRNNBackwardWeights()</samp> and
                                    							<samp class="ph codeph">cudnnRNNBackwardWeightsEx()</samp> functions when the RNN cell
                                    						type was GRU and the NVIDIA Ampere Architecture GPU was used with FP32 I/O
                                    						and <samp class="ph codeph">mathType</samp> of <samp class="ph codeph">CUDNN_DEFAULT_MATH</samp> or
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH</samp>. Users may switch to
                                    							<samp class="ph codeph">CUDNN_FMA_MATH</samp> as a temporary workaround. This issue is
                                    						being investigated.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnRNN*()</samp> with LSTM mode may produce inaccurate results on the
                                    							<samp class="ph codeph">cy</samp> outputs when clipping is enabled on all GPUs. This
                                    						issue exists in previous cuDNN releases as well.
                                 </li>
                                 <li class="li liexpand">On NVIDIA Volta and NVIDIA Pascal architectures, performance regressions may be present for
                                    							<samp class="ph codeph">TRUE_HALF</samp> convolution backward filter.
                                 </li>
                                 <li class="li liexpand">When using <samp class="ph codeph">cudnnRNN*Ex()</samp> APIs, if the user uses
                                    							<samp class="ph codeph">CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_UNPACKED</samp> or
                                    							<samp class="ph codeph">CUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED</samp> as the
                                    						layout of the RNN data descriptors, and if the batch size is larger than
                                    						6144 on NVIDIA Volta or NVIDIA Ampere Architecture A100 GPUs, or larger than
                                    						4096 on NVIDIA Turing GPUs, <samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp>
                                    						may be returned.
                                 </li>
                                 <li class="li liexpand">Currently, there are
                                    							<samp class="ph codeph">libcudnn_ops</samp>/<samp class="ph codeph">cnn</samp>/<samp class="ph codeph">adv_infer</samp>/<samp class="ph codeph">train_static.a</samp>
                                    						binaries in the cuDNN Debian and tgz packages. Users are advised not to link
                                    						against those and link against <samp class="ph codeph">libcudnn_static.a</samp> instead.
                                    						Those binaries will be removed from the release packages in the next
                                    						release.
                                 </li>
                                 <li class="li liexpand">When using <samp class="ph codeph">cudnnRNN*Ex()</samp> APIs, if the user plans to use
                                    							<samp class="ph codeph">CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_UNPACKED</samp> or
                                    							<samp class="ph codeph">CUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED</samp> as the
                                    						layout of the RNN data descriptors, the user should call
                                    							<samp class="ph codeph">cudnnSetRNNPaddingMode()</samp> to set the mode to
                                    							<samp class="ph codeph">CUDNN_RNN_PADDED_IO_ENABLED</samp> after initializing an
                                    							<samp class="ph codeph">RNNDescriptor</samp> but before calling
                                    							<samp class="ph codeph">cudnnGetRNNWorkspaceSize()</samp>. Not doing this may result
                                    						in <samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp>.
                                 </li>
                                 <li class="li liexpand"><em class="ph i">Updated: August 24, 2020</em><p class="p">Fused convolution-scale-bias-activation
                                       							with per-channel α1 and α2 scaling gives incorrect results when the
                                       							reorder type in the convolution descriptor is set to
                                       								<samp class="ph codeph">CUDNN_NO_REORDER</samp>. 
                                    </p>
                                 </li>
                                 <li class="li liexpand"><em class="ph i">Updated: August 24, 2020</em><p class="p">When the user is using
                                       								<samp class="ph codeph">cudnnRNN*</samp> APIs with the problem sizes (input size,
                                       							hidden size) being not multiples of 16 for FP16 tensors or multiples of
                                       							8 for FP32 tensors, users may encounter a return status of
                                       								<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp>. 
                                    </p>
                                 </li>
                                 <li class="li liexpand"><em class="ph i">Updated: August 24, 2020</em><p class="p">For some 3D spatial non-Tensor-Core convolutions on
                                       							Maxwell, NVIDIA Pascal, NVIDIA Volta, and NVIDIA Turing architectures,
                                       								<a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackwardFilter()</samp></a> can return
                                       							incorrect results when the convolution width padding exceeds the value
                                       								<samp class="ph codeph">(filterWidth - 1)/2</samp>. Likewise, users of <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBackendExecute" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackendExecute()</samp></a> can experience
                                       							the same issue when using the engine with
                                       								<samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp> 32 for backward
                                       							filter. The issue affecting <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackwardFilter()</samp></a> has been fixed in
                                       							this release. With <a class="xref" href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBackendFinalize" target="_blank" shape="rect"><samp class="ph codeph">cudnnBackendFinalize()</samp></a>, an engine
                                       							descriptor with <samp class="ph codeph">CUDNN_ATTR_ENGINE_GLOBAL_INDEX</samp> 32 and a
                                       							backward filter operation that satisfies the above condition will return
                                       								<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp>.
                                    </p>
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-801-Preview"><a name="rel-801-Preview" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-801-Preview" name="rel-801-Preview" shape="rect">1.30.&nbsp;cuDNN Release 8.0.1 Preview</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"></p>
                        <div class="section" id="rel-801-Preview__section_l2m_s4c_2jb"><a name="rel-801-Preview__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><div class="p">
                              <div class="note attention"><span class="attentiontitle">Attention:</span> This is the cuDNN 8.0.1 Preview release. This Preview release is for
                                 					early testing and feedback, therefore, for production use of cuDNN, continue to
                                 					use <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-archived/index.html#cudnn_7" target="_blank" shape="rect">cuDNN 7.6.5</a>. This release is subject
                                 					to change based on ongoing performance tuning and functional testing. For
                                 					feedback on the new backend API and deprecations, e-mail <a class="xref" href="mailto:cudnn@nvidia.com" target="_blank" shape="rect">cudnn@nvidia.com</a>.
                              </div>
                           </div>
                           <p class="p">These release notes are applicable to NVIDIA JetPack users of cuDNN unless appended
                              				specifically with <em class="ph i">(not applicable for Jetson platforms)</em>.
                           </p>
                           <p class="p">For previous cuDNN documentation, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-archived/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">Added new kernels to improve the performance of fusion.</li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Limitations</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">Samples can crash unless they are installed in a writable location.</li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit non-deterministic behavior when the cuDNN
                                       							8.0.1 library is built with CUDA Toolkit 10.2 or higher. This is the
                                       							result of a new buffer management and heuristics in the cuBLAS library.
                                       							As described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This is
                                       							caused by two buffer sizes (16 KB and 4 MB) used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting for a buffer of
                                       							that size to be released, a smaller buffer may be used with a different
                                       							GPU kernel. The kernel selection may affect numerical results. The user
                                       							can eliminate the non-deterministic behavior of cuDNN RNN and multihead
                                       							attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>.
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16 KB each in GPU
                                       							memory while the second setting creates two buffers of 4 MB each. The
                                       							default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">Some data types are not widely supported by all cuDNN API. For example,
                                    							<samp class="ph codeph">CUDNN_DATA_INT8x4</samp> is not supported by many functions.
                                    						In such cases, support is available by using <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnTransformTensor" target="_blank" shape="rect"><samp class="ph codeph">cudnnTransformTensor()</samp></a> to transform the tensors
                                    						from the desired type to a type supported by the API. For example, a user is
                                    						able to transform input tensors from <samp class="ph codeph">CUDNN_DATA_INT8x4</samp> to
                                    							<samp class="ph codeph">CUDNN_DATA_INT8</samp>, run the desired API and then transform
                                    						output tensors from <samp class="ph codeph">CUDNN_DATA_INT8</samp> to
                                    							<samp class="ph codeph">CUDNN_DATA_INT8x4</samp>. Note that this transformation will
                                    						incur an extra round trip to memory.
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte alignment,
                                    						including INT8 data in the cuDNN library.
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN 8.0.1 now require increased alignment on tensors in
                                    						order to run efficiently. As always, cuDNN recommends users to align tensors
                                    						to 128-bit boundaries that will be sufficiently aligned for any
                                    						computational option in cuDNN. Doing otherwise may cause performance
                                    						regressions in cuDNN 8.0.1 compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and the output is
                                    						in FP16 (half float), there are cases where the numerical accuracy between
                                    						the different algorithms might differ. cuDNN 8.0.1 users can target the
                                    						backend API to query the numerical notes of the algorithms to get the
                                    						information programmatically. There are cases where algo0 and algo1 will
                                    						have a reduced precision accumulation when users target the legacy API. In
                                    						all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                                 <li class="li liexpand">For the <samp class="ph codeph">_ALGO_0</samp> algorithm of convolution backward data and backward
                                    						filter, grouped convolution with groups larger than 1 and with odd product
                                    						of dimensions <samp class="ph codeph">C</samp>, <samp class="ph codeph">D</samp> (if 3D convolution),
                                    							<samp class="ph codeph">H</samp>, and <samp class="ph codeph">W</samp> is not supported on devices
                                    						older than NVIDIA Volta. To prevent a potential illegal memory access by an
                                    						instruction that only has a 16-bit version in Volta and later, pad at least
                                    						one of the dimensions to an even value.
                                 </li>
                                 <li class="li liexpand">On K80 GPUs, when <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBiasActivationForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionForward()</samp></a> is used with
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp>
                                    						algorithm and half I/O data types a silent error might occur when the output
                                    						width <samp class="ph codeph">Q</samp> is <samp class="ph codeph">1</samp> and both height and width
                                    						padding are zero.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-801-Preview__fixed-issues"><a name="rel-801-Preview__fixed-issues" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been fixed in this release:</p>
                           <ul class="ul">
                              <li class="li liexpand">The <samp class="ph codeph">dimA</samp> and <samp class="ph codeph">strideA</samp> parameters in <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnSetTensorNdDescriptor" target="_blank" shape="rect"><samp class="ph codeph">cudnnSetTensorNdDescriptor()</samp></a> do not document the
                                 					tensor layout. The documentation has been updated to include this
                                 					information.
                              </li>
                              <li class="li liexpand">cuDNN 8.0.0 Preview will not work with GA10x NVIDIA Ampere Architecture GPUs. This has been
                                 					fixed in 8.0.1 Preview.
                              </li>
                              <li class="li liexpand">cuDNN 8.0.0 Preview removed a restriction on convolution backward filter for output filter
                                 					with odd products of dimensions (<samp class="ph codeph">N*C*D*H*W</samp>) for a kernel in
                                 						<samp class="ph codeph">algo0</samp> for pre-Volta GPUs. This can potentially lead to an
                                 					illegal memory access error. This restriction is restored in cuDNN 8.0.1
                                 					Preview. cuDNN will use a kernel that does not have this restriction for this
                                 					computation case.
                              </li>
                              <li class="li liexpand">Fixed performance issues for pre-Vola architectures for convolutions (except when the
                                 					compute type is half).
                              </li>
                              <li class="li liexpand">Mitigated the performance regression to less than 10% end to end.</li>
                           </ul>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p">
                              <ul class="ul">
                                 <li class="li liexpand">On pre-Volta, there are significant performance issues on convolution layers when the
                                    						compute type is half.
                                 </li>
                                 <li class="li liexpand">Sub-optimal performance is present in this release for all INT8 convolutions for all
                                    						GPUs.
                                 </li>
                                 <li class="li liexpand">The performance of <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBiasActivationForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp></a> is
                                    						slower than v7.6 in most cases. This is being actively worked on and
                                    						performance optimizations will be available in the upcoming releases.
                                 </li>
                                 <li class="li liexpand">There are some peer-to-peer documentation links that are broken within the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html" target="_blank" shape="rect">NVIDIA cuDNN API Reference</a>. These
                                    						links will be fixed in the next release.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnCnnTrainVersionCheck()</samp> and
                                    							<samp class="ph codeph">cudnnCnnInferVersionCheck()</samp> are missing in this release
                                    						and will be added in the GA release.
                                 </li>
                                 <li class="li liexpand">Documentation of RNN new APIs and deprecations is not complete. The
                                    							<samp class="ph codeph">cudnnRNNBackwardData_v8()</samp> and
                                    							<samp class="ph codeph">cudnnRNNBackwardWeights_v8()</samp> functions will be
                                    						implemented in the next release.
                                 </li>
                                 <li class="li liexpand">cuDNN 8.0.1 Preview build with Windows and CUDA 11.0 RC has reduced performance on 2D, 3D,
                                    						and grouped convolutions compared to Linux.
                                 </li>
                                 <li class="li liexpand">There is a known issue in cuDNN 8.0.1 when linking statically to cuDNN and using the
                                    						library's 3D algo1 backward filter convolutions. Users will see that the
                                    						libraries emit an internal error or incorrectly state that a shared library
                                    						is missing. This is a bug that will be fixed in a future release.
                                 </li>
                                 <li class="li liexpand">
                                    <div class="p">When using an RPM file on RedHat for installation, installing cuDNN v8 directly or using
                                       							TensorRT 7.1.3 will enable users to build their application with cuDNN
                                       							v8. However, in order for the user to compile an application with cuDNN
                                       							v7 after cuDNN v8 is installed, the user must perform the following
                                       								steps:<a name="rel-801-Preview__ol_hmn_fvf_dmb" shape="rect">
                                          <!-- --></a><ol class="ol" id="rel-801-Preview__ol_hmn_fvf_dmb">
                                          <li class="li">Issue <samp class="ph codeph">sudo mv /usr/include/cudnn.h
                                                										/usr/include/cudnn_v8.h</samp>.
                                          </li>
                                          <li class="li">Issue <samp class="ph codeph">sudo ln -s /etc/alternatives/libcudnn
                                                										/usr/include/cudnn.h</samp>.
                                          </li>
                                          <li class="li">Switch to cuDNN v7 by issuing <samp class="ph codeph">sudo update-alternatives
                                                										--config libcudnn</samp> and choose cuDNN v7 from the
                                             									list.
                                          </li>
                                       </ol>
                                    </div>
                                    <p class="p">Steps 1 and 2 are required for the user to be able to switch between v7 and v8
                                       							installations. After steps 1 and 2 are performed once, step 3 can be
                                       							used repeatedly and the user can choose the appropriate cuDNN version to
                                       							work with. For more information, refer to <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installlinux-rpm" target="_blank" shape="rect">Installing From An RPM File</a>
                                       							and <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#upgrade" target="_blank" shape="rect">Upgrading From v7 To v8</a>.
                                    </p>
                                 </li>
                                 <li class="li liexpand">When FFT Tiled aglo (that is, <samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING</samp> in
                                    						forward convolution or
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING</samp> for backward
                                    						data) is used for 3D convolution, an intermittent silent failure might
                                    						happen due to an incorrect stream used for kernel execution. In some cases,
                                    						this might be manifested as undefined values seen in the output.
                                 </li>
                                 <li class="li liexpand">The implementation of <samp class="ph codeph">cuDNNLRNCrossChannelBackward</samp> is inconsistent with
                                    						the implementation of <samp class="ph codeph">cuDNNLRNCrossChannelForward</samp> and
                                    						returns incorrect results when the normalization window is even. This will
                                    						be fixed in a future release.
                                 </li>
                                 <li class="li liexpand">RNN APIs in cuDNN v8.0.1, compiled with CUDA 11.0, use an incorrect default down-conversion
                                    						on GPUs with CUDA SM version SM80 (NVIDIA Ampere Architecture GPU family)
                                    						when supplied input data and weights have the
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> type and
                                    							<samp class="ph codeph">cudnnMathType_t</samp> set using
                                    							<samp class="ph codeph">cudnnSetRNNMatrixMathType()</samp> is
                                    							<samp class="ph codeph">CUDNN_DEFAULT_MATH</samp> or
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH</samp>. Instead of using the default TF32
                                    						computation when Tensor Cores are used, a down conversion to FP16
                                    						(half-precision) is performed; same as in the
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> mode. This
                                    						introduces a lower dynamic range of intermediate data but possibly faster
                                    						execution. To disable the automatic down conversion of
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> weights and data in RNN APIs, set the
                                    						environmental variable <samp class="ph codeph">NVIDIA_TF32_OVERRIDE</samp> to
                                    							<samp class="ph codeph">0</samp> (notice this will disable the use of TF32 in the
                                    						entire library, which might have a performance impact on CNNs that are not
                                    						affected by this issue). Another workaround is to assign the
                                    							<samp class="ph codeph">CUDNN_FMA_MATH</samp> mode to the
                                    							<samp class="ph codeph">cudnnMathType_t</samp> argument in
                                    							<samp class="ph codeph">cudnnSetRNNMatrixMathType()</samp>. Due to this, the A100 TF32
                                    						feature is not accessible for RNNs in cuDNN v8.0.1.
                                 </li>
                                 <li class="li liexpand">Several cuDNN APIs are unable to directly support computations using integer types
                                    							(<samp class="ph codeph">CUDNN_DATA_INT8</samp>, <samp class="ph codeph">CUDNN_DATA_INT8x4</samp>,
                                    							<samp class="ph codeph">CUDNN_DATA_INT8x32</samp> or
                                    						<samp class="ph codeph">CUDNN_DATA_INT32</samp>). Floating types (particularly
                                    							<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>) are much more widely supported. If an
                                    						API does not support the desired type,
                                    							<samp class="ph codeph">cudnnTransformTensor()</samp> can be used to support the use
                                    						case by converting to/from a supported type and the desired type. Here are
                                    						the steps for doing so:<a name="rel-801-Preview__ol_hz5_r31_dmb" shape="rect">
                                       <!-- --></a><ol class="ol" id="rel-801-Preview__ol_hz5_r31_dmb">
                                       <li class="li">Convert all input tensors from their native type to a supported type
                                          									(<samp class="ph codeph">CUDNN_DATA_FLOAT</samp> is recommended).
                                       </li>
                                       <li class="li">Run cuDNN API using the converted input tensors and output tensor
                                          								descriptors set as <samp class="ph codeph">CUDNN_DATA_FLOAT</samp>.
                                       </li>
                                       <li class="li">Convert all output tensors from a supported type to your desired
                                          								output type.
                                       </li>
                                    </ol>
                                    <div class="note note"><span class="notetitle">Note:</span> This will require extra memory use for the temporary buffers.
                                       							Further, this will introduce an additional round trip to memory that
                                       							might noticeably impact performance.
                                    </div>
                                 </li>
                                 <li class="li liexpand"><em class="ph i">Updated: August 24, 2020</em><p class="p">cuDNN convolution APIs may return
                                       								<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> when the number of
                                       							input or output channels equals to or exceeds 2097152.
                                    </p>
                                 </li>
                                 <li class="li liexpand"><em class="ph i">Updated: August 24, 2020</em><p class="p">When the user is using
                                       								<samp class="ph codeph">cudnnRNN*</samp> APIs with the problem sizes (input size,
                                       							hidden size) being not multiples of 16 for FP16 tensors or multiples of
                                       							8 for FP32 tensors, users may encounter a return status of
                                       								<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp>. 
                                    </p>
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel-800-Preview"><a name="rel-800-Preview" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel-800-Preview" name="rel-800-Preview" shape="rect">1.31.&nbsp;cuDNN Release 8.0.0 Preview</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"></p>
                        <div class="section" id="rel-800-Preview__section_l2m_s4c_2jb"><a name="rel-800-Preview__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><div class="p">
                              <div class="note attention"><span class="attentiontitle">Attention:</span> This is the cuDNN 8.0.0 Preview release. This Preview release is for
                                 					early testing and feedback, therefore, for production use of cuDNN, continue to
                                 					use <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-archived/index.html#cudnn_7" target="_blank" shape="rect">cuDNN 7.6.5</a>. This release is subject
                                 					to change based on ongoing performance tuning and functional testing. For
                                 					feedback on the new backend API and deprecations, e-mail <a class="xref" href="mailto:cudnn@nvidia.com" target="_blank" shape="rect">cudnn@nvidia.com</a>.
                              </div>
                           </div>
                           <div class="p">These release notes are applicable to NVIDIA JetPack users of cuDNN unless appended
                              				specifically with <em class="ph i">(not applicable for Jetson platforms)</em>.
                              <div class="note note"><span class="notetitle">Note:</span> cuDNN 8.0.0
                                 					passed GA quality testing and validation for TensorRT and JetPack
                                 				users.
                              </div>
                           </div>
                           <p class="p">For previous cuDNN documentation, see the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-archived/index.html" target="_blank" shape="rect"><u class="ph u">cuDNN Archived Documentation</u></a>.
                           </p>
                        </div>
                        <div class="section" id="rel-800-Preview__features-enhancements"><a name="rel-800-Preview__features-enhancements" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">The following features and enhancements have been added to this release:
                              <dl class="dl">
                                 <dt class="dt dlterm">cuDNN library</dt>
                                 <dd class="dd">
                                    <div class="p"><a name="rel-800-Preview__ul_e5s_fgh_dmb" shape="rect">
                                          <!-- --></a><ul class="ul" id="rel-800-Preview__ul_e5s_fgh_dmb">
                                          <li class="li liexpand">The cuDNN library has been split into the following
                                             											libraries:<a name="rel-800-Preview__ul_hsw_1b1_5kb" shape="rect">
                                                <!-- --></a><ul class="ul" id="rel-800-Preview__ul_hsw_1b1_5kb">
                                                <li class="li liexpand"><samp class="ph codeph">cudnn_ops_infer</samp> - This entity
                                                   												contains the routines related to cuDNN context
                                                   												creation and destruction, tensor descriptor
                                                   												management, tensor utility routines, and the
                                                   												inference portion of common machine learning
                                                   												algorithms such as batch normalization, softmax,
                                                   												dropout, and so on.
                                                </li>
                                                <li class="li liexpand"><samp class="ph codeph">cudnn_ops_train</samp> - This entity
                                                   												contains common training routines and algorithms,
                                                   												such as batch normalization, softmax, dropout, and
                                                   												so on. The <samp class="ph codeph">cudnn_ops_train</samp> library
                                                   												depends on <samp class="ph codeph">cudnn_ops_infer</samp>.
                                                </li>
                                                <li class="li liexpand"><samp class="ph codeph">cudnn_cnn_infer</samp> - This entity
                                                   												contains all routines related to convolutional
                                                   												neural networks needed at inference time. The
                                                   												<samp class="ph codeph">cudnn_cnn_infer</samp> library depends
                                                   												on <samp class="ph codeph">cudnn_ops_infer</samp>.
                                                </li>
                                                <li class="li liexpand"><samp class="ph codeph">cudnn_cnn_train</samp> - This entity
                                                   												contains all routines related to convolutional
                                                   												neural networks needed during training time. The
                                                   												<samp class="ph codeph">cudnn_cnn_train</samp> library depends
                                                   												on <samp class="ph codeph">cudnn_ops_infer</samp>,
                                                   												<samp class="ph codeph">cudnn_ops_train</samp>, and
                                                   												<samp class="ph codeph">cudnn_cnn_infer</samp>.
                                                </li>
                                                <li class="li liexpand"><samp class="ph codeph">cudnn_adv_infer</samp> - This entity
                                                   												contains all other features and algorithms. This
                                                   												includes RNNs, CTC loss, and multihead attention.
                                                   												The <samp class="ph codeph">cudnn_adv_infer</samp> library depends
                                                   												on <samp class="ph codeph">cudnn_ops_infer</samp>.
                                                </li>
                                                <li class="li liexpand"><samp class="ph codeph">cudnn_adv_train</samp> - This entity
                                                   												contains all the training counterparts of
                                                   												<samp class="ph codeph">cudnn_adv_infer</samp>. The
                                                   												<samp class="ph codeph">cudnn_adv_train</samp> library depends
                                                   												on <samp class="ph codeph">cudnn_ops_infer</samp>,
                                                   												<samp class="ph codeph">cudnn_ops_train</samp>, and
                                                   												<samp class="ph codeph">cudnn_adv_infer</samp>.
                                                </li>
                                                <li class="li liexpand"><samp class="ph codeph">cudnn</samp> - This is an optional shim
                                                   												layer between the application layer and the cuDNN
                                                   												code. This layer opportunistically opens the correct
                                                   												library for the API at runtime.
                                                </li>
                                             </ul>
                                          </li>
                                          <li class="li liexpand">cuDNN does not support mixing sub library versions. If there
                                             										is a mismatch in the cuDNN version numbers in the cuDNN sub
                                             										library header files, the build will crash. The versions
                                             										must match on the major number and minor number, as well as
                                             										the patch level.
                                          </li>
                                          <li class="li liexpand">The cuDNN sub libraries must be installed under a single
                                             										directory.
                                          </li>
                                       </ul>
                                    </div>
                                 </dd>
                                 <dt class="dt dlterm">Multiple dynamic libraries</dt>
                                 <dd class="dd">In order to link against a subset of cuDNN, you must know which subset
                                    							of the API you are using and then link against the appropriate cuDNN sub
                                    							components. The cuDNN sub components are as follows:<a name="rel-800-Preview__ul_clq_fb1_5kb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-800-Preview__ul_clq_fb1_5kb">
                                       <li class="li"><samp class="ph codeph">cudnn_ops_infer.so</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnn_ops_train.so</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnn_cnn_infer.so</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnn_cnn_train.so</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnn_adv_infer.so</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnn_adv_train.so</samp></li>
                                    </ul>
                                 </dd>
                                 <dt class="dt dlterm">cuDNN linking options</dt>
                                 <dd class="dd">There are two different linking options:<a name="rel-800-Preview__ul_f5s_fgh_dmb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-800-Preview__ul_f5s_fgh_dmb">
                                       <li class="li liexpand">Linking against individual sub libraries: Users who link against
                                          									individual sub libraries must be able to identify the API
                                          									exposed by each cuDNN sub library. Users also must know the
                                          									hierarchy of the different cuDNN sub libraries. Each
                                          										<samp class="ph codeph">.so</samp> or <samp class="ph codeph">.a</samp> needs to be
                                          									specified explicitly in the user’s linking command, as well as
                                          									any external dependencies cuDNN require. For more information,
                                          									refer to the <em class="ph i">Limitations</em> section below.
                                       </li>
                                       <li class="li liexpand">Linking against the full cuDNN (compatibility option): This
                                          									would allow users to use <samp class="ph codeph">-lcudnn</samp>.
                                          										<samp class="ph codeph">libcudnn.so</samp> is provided as a shim layer
                                          									that would open the appropriate cuDNN sub-library for any
                                          									particular cuDNN API call. While <samp class="ph codeph">libcudnn.a</samp> is
                                          									largely unchanged, it is a statically linked file for all of
                                          									cuDNN.
                                       </li>
                                    </ul>
                                 </dd>
                                 <dt class="dt dlterm">cuDNN loading options</dt>
                                 <dd class="dd">For users who want a smaller memory footprint, there are two ways of
                                    							loading the library.<a name="rel-800-Preview__ul_g5s_fgh_dmb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-800-Preview__ul_g5s_fgh_dmb">
                                       <li class="li liexpand">Cherry-pick loading: Each sub library is loaded only when
                                          									accessed. This will cause the first reference to that sub
                                          									library to take a long time but will ensure the user isn’t
                                          									loading more libraries than they need.
                                       </li>
                                       <li class="li liexpand">All access loading: All available cuDNN sub libraries are loaded
                                          									early during runtime.
                                       </li>
                                    </ul>
                                 </dd>
                                 <dt class="dt dlterm">New API functions</dt>
                                 <dd class="dd">For a list of functions and data types that were added in this release,
                                    							refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#release-800" target="_blank" shape="rect">API changes for cuDNN
                                       							8.0.0</a>.
                                 </dd>
                                 <dt class="dt dlterm">General Support of CUDA Graph Capture</dt>
                                 <dd class="dd">
                                    <div class="p">CUDA Graphs are now supported for all functions in this release; with
                                       								the following restrictions.<a name="rel-800-Preview__ul_h5s_fgh_dmb" shape="rect">
                                          <!-- --></a><ul class="ul" id="rel-800-Preview__ul_h5s_fgh_dmb">
                                          <li class="li liexpand">CUDA Toolkit 10.2 or higher is required.</li>
                                          <li class="li liexpand">cuDNN 8.0.0 graphs are captured using the CUDA graph-capture
                                             										APIs.
                                          </li>
                                          <li class="li liexpand">any non-default use of textures by users of cuDNN must be
                                             										disabled prior to capture
                                          </li>
                                       </ul>
                                    </div>
                                    <p class="p">cuDNN 8.0.0 does not at this time offer API support to add operations
                                       								to an existing CUDA graph directly; however, the captured graph may
                                       								be added to an existing graph through the existing CUDA Graphs
                                       								API.
                                    </p>
                                    <p class="p">Regarding texture usage, cuDNN 8.0.0 by default will not enable
                                       								texture usage; expert users may enable texture usage where allowed,
                                       								but that usage will prevent a successful CUDA Graph capture until
                                       								disabled. In order for cuDNN 8.0.0 to be graph-capture compatible
                                       								library-wide, the cuDNN 8.0.0 CTC API was updated as described
                                       								elsewhere.
                                    </p>
                                    <p class="p">The usual restrictions for CUDA Graphs apply in addition to these
                                       								restrictions here.
                                    </p>
                                 </dd>
                                 <dt class="dt dlterm">New APIs for convolution</dt>
                                 <dd class="dd">A new set of API functions to provide a brand new approach to cuDNN that
                                    							offers more fine-grain control of performance, numerical properties, and
                                    							so on for convolution. Using this API, users directly access various
                                    							engines that compute convolution forward propagation, backward data,
                                    							backward filter, and generic support for fusion starting with a limited
                                    							support in this cuDNN 8.0.0 release and expanding support in follow-up
                                    							releases. Each engine has performance-tuning knobs such as GEMM tiling
                                    							and split-K. Users can use this API to fine-tune their network by
                                    							querying cuDNN’s heuristics, or doing their own, to find the most
                                    							optimal engine configuration with which cuDNN computes each network
                                    							layer.
                                 </dd>
                                 <dt class="dt dlterm">NVIDIA Ampere architecture GPU support <em class="ph i">(not applicable for Jetson
                                       								platforms)</em></dt>
                                 <dd class="dd"><a name="rel-800-Preview__ul_i5s_fgh_dmb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-800-Preview__ul_i5s_fgh_dmb">
                                       <li class="li liexpand">Added support for A100 GPU based on NVIDIA Ampere
                                          									Architecture.
                                       </li>
                                       <li class="li liexpand">cuDNN 8.0.0 has seen significant improvements when using A100
                                          									GPUs compared to NVIDIA Volta V100 with cuDNN 7.6.
                                       </li>
                                       <li class="li liexpand">Added support for Tensor Float 32 (TF32) for 1D and 2D
                                          									convolutions. Full support for TF32 will come in future releases
                                          									such as grouped convolutions and 3D convolutions in addition to
                                          									further performance tuning.
                                       </li>
                                       <li class="li liexpand">Increased performance for the legacy Tensor Cores (mixed
                                          									precision for 1D, 2D, 3D, and grouped convolutions.
                                       </li>
                                    </ul>
                                 </dd>
                                 <dt class="dt dlterm">NVIDIA Turing and NVIDIA Volta architecture improvements</dt>
                                 <dd class="dd"><a name="rel-800-Preview__ul_j5s_fgh_dmb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel-800-Preview__ul_j5s_fgh_dmb">
                                       <li class="li liexpand">New kernels for Tensor Cores and heuristics update for 1D
                                          									convolution resulting in performance improvements for speech
                                          									networks such as <a class="xref" href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechRecognition/Jasper" target="_blank" shape="rect">Jasper</a> and <a class="xref" href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/Tacotron2" target="_blank" shape="rect">Tacotron2 and
                                             									WaveGlow</a>, in addition to support for grouped 1D
                                          									convolution (<a class="xref" href="https://github.com/NVIDIA/NeMo/tree/main/examples/asr/conf/quartznet" target="_blank" shape="rect">QuartzNet</a>).
                                       </li>
                                       <li class="li liexpand">Added 3D convolutions support of NHWC and improved heuristics
                                          									and kernels for Tensor Cores in NCHW resulting in performance
                                          									improvements for <a class="xref" href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Segmentation/VNet" target="_blank" shape="rect">VNet</a>, <a class="xref" href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Segmentation/UNet_Medical" target="_blank" shape="rect">UNet-Medical</a>, and
                                          										<a class="xref" href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Segmentation/UNet_Industrial" target="_blank" shape="rect">UNet-Industrial</a>.
                                          									Additionally, FP16 3D convolutions are supported as well.
                                       </li>
                                       <li class="li liexpand">Better utilization of Tensor Cores and heuristics for grouped
                                          									convolutions result in improvements for <a class="xref" href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets" target="_blank" shape="rect">ResNext</a>.
                                       </li>
                                       <li class="li liexpand">More tuning for vision networks like ResNet-50 ([<a class="xref" href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/MxNet/Classification/RN50v1.5" target="_blank" shape="rect">MXNet</a>] [<a class="xref" href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets" target="_blank" shape="rect">PyTorch</a>] [<a class="xref" href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Classification/RN50v1.5" target="_blank" shape="rect">TensorFlow</a>]) and SSD
                                          										([<a class="xref" href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD" target="_blank" shape="rect">PyTorch</a>] [<a class="xref" href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Detection/SSD" target="_blank" shape="rect">TensorFlow</a>]) with new
                                          									updated heuristics.
                                       </li>
                                    </ul>
                                 </dd>
                                 <dt class="dt dlterm">Operation fusion</dt>
                                 <dd class="dd">Operation fusion can be achieved using the backend API. The general
                                    							workflow is similar to running unfused operations, except that instead
                                    							of creating a single operation Operation Graph, the user may specify a
                                    							multi-operation Operation Graph. For more information, refer to <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#op-fusion" target="_blank" shape="rect">Operation Fusion using the Backend
                                       								API</a>.
                                 </dd>
                                 <dt class="dt dlterm">Depthwise convolution extension</dt>
                                 <dd class="dd">We have extended the <samp class="ph codeph">fprop</samp> and <samp class="ph codeph">dgrad</samp>
                                    							NHWC depthwise kernels to support more combinations (filter
                                    							sizes/strides) such as 5x5/1x1, 5x5/2x2, 7x7/1x1, 7x7/2x2 (in addition
                                    							to what we already have, 1x1/1x1, 3x3/1x1, 3x3/2x2), which provides good
                                    							performance.
                                 </dd>
                              </dl>
                           </div>
                        </div>
                        <div class="section" id="rel-800-Preview__section_gzk_54c_2jb"><a name="rel-800-Preview__section_gzk_54c_2jb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel-800-Preview__section_qhc_jc1_5kb"><a name="rel-800-Preview__section_qhc_jc1_5kb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel-800-Preview__ul_k5s_fgh_dmb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-800-Preview__ul_k5s_fgh_dmb">
                                 <li class="li liexpand">Samples must be installed in a writable location, otherwise the samples can crash.</li>
                                 <li class="li liexpand">
                                    <p class="p">RNN and multihead attention API calls may exhibit non-deterministic
                                       							behavior when the cuDNN 8.0.0 library is built with CUDA Toolkit 10.2 or
                                       							higher. This is the result of a new buffer management and heuristics in
                                       							the cuBLAS library. As described in <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect">Results Reproducibility</a>,
                                       							numerical results may not be deterministic when cuBLAS APIs are launched
                                       							in more than one CUDA stream using the same cuBLAS handle. This is
                                       							caused by two buffer sizes (16 KB and 4 MB) used in the default
                                       							configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting
                                       							for a buffer of that size to be released, a smaller buffer may be used
                                       							with a different GPU kernel. The kernel selection may affect numerical
                                       							results. The user can eliminate the non-deterministic behavior of cuDNN
                                       							RNN and multihead attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>.
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16
                                       							KB each in GPU memory while the second setting creates two buffers of 4
                                       							MB each. The default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                                 <li class="li liexpand">Some data types are not widely supported by all cuDNN API. For example,
                                    							<samp class="ph codeph">CUDNN_DATA_INT8x4</samp> is not supported by many functions.
                                    						In such cases, support is available by using <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnTransformTensor" target="_blank" shape="rect"><samp class="ph codeph">cudnnTransformTensor()</samp></a> to transform the tensors
                                    						from the desired type to a type supported by the API. For example, a user is
                                    						able to transform input tensors from <samp class="ph codeph">CUDNN_DATA_INT8x4</samp> to
                                    							<samp class="ph codeph">CUDNN_DATA_INT8</samp>, run the desired API and then transform
                                    						output tensors from <samp class="ph codeph">CUDNN_DATA_INT8</samp> to
                                    							<samp class="ph codeph">CUDNN_DATA_INT8x4</samp>. Note that this transformation will
                                    						incur an extra round trip to memory.
                                 </li>
                                 <li class="li liexpand">The tensor pointers and the filter pointers require at a minimum 4-byte alignment,
                                    						including INT8 data in the cuDNN library. 
                                 </li>
                                 <li class="li liexpand">Some computational options in cuDNN 8.0.0 now require increased alignment on tensors in
                                    						order to run efficiently. As always, cuDNN recommends users to align tensors
                                    						to 128-bit boundaries that will be sufficiently aligned for any
                                    						computational option in cuDNN. Doing otherwise may cause performance
                                    						regressions in cuDNN 8.0.0 compared to cuDNN v7.6.
                                 </li>
                                 <li class="li liexpand">For certain algorithms, when the computation is in float (32-bit float) and the output is
                                    						in FP16 (half float), there are cases where the numerical accuracy between
                                    						the different algorithms might differ. cuDNN 8.0.0 users can target the
                                    						backend API to query the numerical notes of the algorithms to get the
                                    						information programmatically. There are cases where algo0 and algo1 will
                                    						have a reduced precision accumulation when users target the legacy API. In
                                    						all cases, these numerical differences are not known to affect training
                                    						accuracy even though they might show up in unit tests.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-800-Preview__section_l5s_fgh_dmb"><a name="rel-800-Preview__section_l5s_fgh_dmb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Deprecated Features</h3>
                           <div class="p">The following features are deprecated in cuDNN 8.0.0:<a name="rel-800-Preview__ul_m5s_fgh_dmb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-800-Preview__ul_m5s_fgh_dmb">
                                 <li class="li liexpand">Support for Ubuntu 14.04 has been deprecated in this release. Upgrade to
                                    						16.04 or 18.04 for continued support.
                                 </li>
                                 <li class="li liexpand">Support for Mac OS X has been deprecated in this release. Linux and Windows
                                    						OS are currently supported.
                                 </li>
                                 <li class="li liexpand">cuDNN version 8 introduces a new API deprecation policy to enable a faster
                                    						pace of innovation. A streamlined, two-step, deprecation policy will be used
                                    						for all API changes starting with cuDNN version 8. For details about this
                                    						new deprecation policy, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#backward-compatibility" target="_blank" shape="rect">Backward Compatibility and Deprecation
                                       							Policy</a>.
                                 </li>
                                 <li class="li liexpand">Removed and deprecated API changes. For a list of removed and deprecated
                                    						APIs, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#release-800" target="_blank" shape="rect">API changes for cuDNN 8.0.0</a>.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel-800-Preview__fixed-issues"><a name="rel-800-Preview__fixed-issues" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been fixed in this release:</p><a name="rel-800-Preview__ul_n5s_fgh_dmb" shape="rect">
                              <!-- --></a><ul class="ul" id="rel-800-Preview__ul_n5s_fgh_dmb">
                              <li class="li liexpand">There is a known issue in that <samp class="ph codeph">cudnnDestroy()</samp> does not destroy all that
                                 						<samp class="ph codeph">cudnnCreate()</samp> created. Calling
                                 						<samp class="ph codeph">cudnnDestroy()</samp> after <samp class="ph codeph">cudnnCreate()</samp> has a
                                 					memory leak in some tests of about 1.6 MB on host memory. This issue has been
                                 					fixed in cuDNN 8.0.0.
                              </li>
                              <li class="li liexpand">Starting in cuDNN 7.6.1, when using the experimental multihead attention API, it is possible
                                 					that the forward and backward paths produce different results for the BERT
                                 					model, when the batch size is greater than one and the number of heads is
                                 					greater than one. This issue has been fixed in cuDNN 8.0.0.
                              </li>
                              <li class="li liexpand">The description of <samp class="ph codeph">cudnnSetCTCLossDescriptorEx()</samp> is not clear. This issue
                                 					has been fixed in cuDNN 8.0.0.
                              </li>
                              <li class="li liexpand">Documentation affecting 1x1 convolution functions are not clear, for example
                                 						<samp class="ph codeph">cudnnFindConvolutionBackwardDataAlgorithm()</samp>. This issue has
                                 					been fixed in cuDNN 8.0.0.
                              </li>
                              <li class="li liexpand">cuDNN forward convolution with
                                 						<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp> does not
                                 					propagate NANs in weights. This issue has been fixed in cuDNN 8.0.0.
                              </li>
                              <li class="li liexpand">Document mathematical definitions of all operations in cuDNN. We include full mathematical
                                 					descriptions for the convolution functions.
                              </li>
                              <li class="li liexpand">The functions <samp class="ph codeph">cudnnGetConvolutionForwardAlgorithm_v7()</samp> and
                                 						<samp class="ph codeph">cudnnGetConvolutionForwardWorkspaceSize()</samp> may return
                                 						<samp class="ph codeph">CUDNN_STATUS_SUCCESS</samp> while the execution of the same
                                 					convolution returns <samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp>. Similar issues
                                 					may also happen for <samp class="ph codeph">convolutionBackwardData()</samp> and
                                 						<samp class="ph codeph">convolutionBackwardFilter()</samp>. This issue is present in cuDNN
                                 					7.2.2 library and later versions. This has been fixed in cuDNN 8.0.0.
                              </li>
                              <li class="li liexpand">Algorithms returned by <samp class="ph codeph">cudnnGetConvolution*Algorithm()</samp> may, in some limited
                                 					use cases, fail to execute when they are actually run. This is a cuDNN
                                 					library-wide issue and applies for convolution forward, convolution backward
                                 					data, and convolution backward filter operations. This issue is also present in
                                 					versions before cuDNN 8.0.0 EA.
                              </li>
                              <li class="li liexpand">cuDNN does not support CUDA graphs. When launching a CUDA graph constructed using a stream
                                 					capture that includes a <samp class="ph codeph">cudnnConvolutionForward()</samp> operation,
                                 					you may see <samp class="ph codeph">cudaErrorLaunchFailure</samp> error. This is because CUDA
                                 					graphs were not supported. The user can proceed.
                              </li>
                              <li class="li liexpand">There was a known performance drop in 3D convolutions for some cases on NVIDIA Turing GPUs
                                 					since cuDNN 7.4.2. This has been fixed on T4. <em class="ph i">(not applicable for Jetson
                                    						platforms)</em></li>
                              <li class="li liexpand">There are rare cases where <samp class="ph codeph">cudnnConvolution*</samp> will return
                                 						<samp class="ph codeph">STATUS_NOT_SUPPORTED</samp> when
                                 						<samp class="ph codeph">cudnn*GetWorkspaceSize</samp> might return success for a given
                                 					algorithm. This has been fixed in cuDNN 8.0.0.
                              </li>
                              <li class="li liexpand">In previous versions of cuDNN,
                                 						<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp> did not
                                 					propagate NaN values in some cases. This is fixed in the current release. Users
                                 					desiring the old behavior can configure ReLU activation and set the floor to be
                                 						<samp class="ph codeph">-Inf</samp>.
                              </li>
                              <li class="li liexpand">The <samp class="ph codeph">multiHeadAttention</samp> sample code was added to the cuDNN 7.6.3 release.
                                 					The sample code includes a simple NumPy/Autograd reference model of the
                                 					multihead attention block that computes the forward response and all
                                 					derivatives. The test code demonstrates how to use the multihead attention API,
                                 					access attention weights, and sequence data.
                              </li>
                              <li class="li liexpand"><em class="ph i">Updated: July 22, 2020</em><p class="p">In version 7.6.x,
                                    							<samp class="ph codeph">cudnnConvolutionBackwardData()</samp> with
                                    							<samp class="ph codeph">PSEUDO_HALF_CONFIG</samp> with
                                    							<samp class="ph codeph">CUDNN_TENSOR_OP_MATH</samp> or <samp class="ph codeph">FLOAT_CONFIG</samp>
                                    						with <samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> returns
                                    						incorrect results in 3D convolution when the filter size of the
                                    							<samp class="ph codeph">w</samp> dimension is <samp class="ph codeph">1</samp> and padding of the
                                    							<samp class="ph codeph">w</samp> dimension is <samp class="ph codeph">0</samp>. This issue has been
                                    						fixed in this release.
                                 </p>
                              </li>
                           </ul>
                        </div>
                        <div class="section" id="rel-800-Preview__section_f3r_df1_5kb"><a name="rel-800-Preview__section_f3r_df1_5kb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel-800-Preview__ul_o5s_fgh_dmb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel-800-Preview__ul_o5s_fgh_dmb">
                                 <li class="li liexpand">Performance regressions on V100 are observed in this release on SSD inference use cases if
                                    						not using TensorRT.
                                 </li>
                                 <li class="li liexpand">There are significant performance regressions on pre-Volta GPUs and some NVIDIA Turing GPUs
                                    						based on the TU102 architecture. This performance regression is not
                                    						applicable to T4, NVIDIA JetPack, and NVIDIA Tegra.
                                 </li>
                                 <li class="li liexpand">Sub-optimal performance is present in this release for all INT8 convolutions for all
                                    						GPUs.
                                 </li>
                                 <li class="li liexpand">The performance of <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBiasActivationForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp></a> is
                                    						slower than v7.6 in most cases. This is being actively worked on and
                                    						performance optimizations will be available in the upcoming releases.
                                 </li>
                                 <li class="li liexpand">On K80 GPUs, when <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBiasActivationForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionForward()</samp></a> is used with
                                    							<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp>
                                    						algorithm and half I/O data types a silent error might occur.
                                 </li>
                                 <li class="li liexpand">There are some peer-to-peer documentation links that are broken within the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html" target="_blank" shape="rect">NVIDIA cuDNN API Reference</a>. These
                                    						links will be fixed in the next release.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnCnnTrainVersionCheck()</samp> and
                                    							<samp class="ph codeph">cudnnCnnInferVersionCheck()</samp> are missing in this release
                                    						and will be added in the GA release.
                                 </li>
                                 <li class="li liexpand">Documentation of RNN new APIs and deprecations is not complete. The
                                    							<samp class="ph codeph">cudnnRNNBackwardData_v8()</samp> and
                                    							<samp class="ph codeph">cudnnRNNBackwardWeights_v8()</samp> functions will be
                                    						implemented in the next release.
                                 </li>
                                 <li class="li liexpand">cuDNN 8.0.0 Preview will not work with GA10x NVIDIA Ampere Architecture GPUs. This will be
                                    						fixed in the next release.
                                 </li>
                                 <li class="li liexpand">cuDNN 8.0.0 Preview build with Windows and CUDA 11.0 RC has reduced performance on 2D, 3D,
                                    						and grouped convolutions compared to Linux.
                                 </li>
                                 <li class="li liexpand"><em class="ph i">Updated: June 12, 2020</em><p class="p">There is a known issue in cuDNN 8.0.0 when linking
                                       							statically to cuDNN and using the library's 3D algo1 backward filter
                                       							convolutions. Users will see that the libraries emit an internal error
                                       							or incorrectly state that a shared library is missing. This is a bug
                                       							that will be fixed in a future release.
                                    </p>
                                 </li>
                                 <li class="li liexpand"><em class="ph i">Updated: June 25, 2019</em><p class="p">There is a known issue in cuDNN 8.0.0 when linking
                                       							statically to cuDNN and using the library's 3D algo1 backward filter
                                       							convolutions. Users will see that the library emit an internal error or
                                       							incorrectly state that a shared library is missing. This is a bug that
                                       							will be fixed in a future release.
                                    </p>
                                 </li>
                                 <li class="li liexpand"><em class="ph i">Updated: June 25, 2019</em><div class="p">When using an RPM file on RedHat for installation,
                                       							installing cuDNN v8 directly or using TensorRT 7.1.3 will enable users
                                       							to build their application with cuDNN v8. However, in order for the user
                                       							to compile an application with cuDNN v7 after cuDNN v8 is installed, the
                                       							user must perform the following steps:<a name="rel-800-Preview__ol_hmn_fvf_dmb" shape="rect">
                                          <!-- --></a><ol class="ol" id="rel-800-Preview__ol_hmn_fvf_dmb">
                                          <li class="li">Issue <samp class="ph codeph">sudo mv /usr/include/cudnn.h
                                                										/usr/include/cudnn_v8.h</samp>.
                                          </li>
                                          <li class="li">Issue <samp class="ph codeph">sudo ln -s /etc/alternatives/libcudnn
                                                										/usr/include/cudnn.h</samp>.
                                          </li>
                                          <li class="li">Switch to cuDNN v7 by issuing <samp class="ph codeph">sudo update-alternatives
                                                										--config libcudnn</samp> and choose cuDNN v7 from the
                                             									list.
                                          </li>
                                       </ol>
                                    </div>
                                    <p class="p">Steps 1 and 2 are required for the user to be able to switch
                                       							between v7 and v8 installations. After steps 1 and 2 are performed one
                                       							time, step 3 can be used repeatedly and the user can choose the
                                       							appropriate cuDNN version to work with. For more information, refer to
                                       								<a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installlinux-rpm" target="_blank" shape="rect">Installing from an RPM File</a>
                                       							and <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#upgrade" target="_blank" shape="rect">Upgrading from v7 to
                                          						v8</a>.
                                    </p>
                                 </li>
                                 <li class="li liexpand"><em class="ph i">Updated: July 22, 2020</em><p class="p"><samp class="ph codeph">cudnnConvolutionForward()</samp>,
                                       								<samp class="ph codeph">cudnnConvolutionBackwardData()</samp>, and
                                       								<samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> erroneously
                                       							returns <samp class="ph codeph">CUDNN_STATUS_INTERNAL_ERROR</samp> when the workspace
                                       							size argument value is less than the required workspace size as returned
                                       							by their respective <samp class="ph codeph">cudnnGetWorkspace()</samp> API.
                                    </p>
                                 </li>
                                 <li class="li liexpand"><em class="ph i">Updated: August 24, 2020</em><p class="p">cuDNN convolution APIs may return
                                       								<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> when the number of
                                       							input or output channels equals to or exceeds 2097152.
                                    </p>
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
               </div>
               <div class="topic reference nested0" id="rel_7xx"><a name="rel_7xx" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#rel_7xx" name="rel_7xx" shape="rect">2.&nbsp;cuDNN Release 7.x.x</a></h2>
                  <div class="body refbody">
                     <p class="shortdesc"></p>
                     <div class="section"></div>
                  </div>
                  <div class="topic reference nested1" id="rel_765"><a name="rel_765" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_765" name="rel_765" shape="rect">2.1.&nbsp;cuDNN Release 7.6.5</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"> This is the cuDNN 7.6.5 release notes. This release includes fixes from the previous
                           		cuDNN v7.x.x releases as well as the following additional changes. These release notes are
                           		applicable to both cuDNN and NVIDIA JetPack users unless appended specifically with <em class="ph i">(not
                              			applicable for Jetson platforms)</em>.
                        </p>
                        <div class="section" id="rel_765__section_l2m_s4c_2jb"><a name="rel_765__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">For previous cuDNN release notes, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-archived/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel_765__features-enhancements"><a name="rel_765__features-enhancements" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <p class="p">The following features and enhancements have been added to this release:</p><a name="rel_765__ul_cht_2pb_5gb" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_765__ul_cht_2pb_5gb">
                              <li class="li liexpand">Made performance improvements to several APIs including <samp class="ph codeph">cudnnAddTensor</samp>,
                                 						<samp class="ph codeph">cudnnOpTensor</samp>, <samp class="ph codeph">cudnnActivationForward</samp>, and
                                 						<samp class="ph codeph">cudnnActivationBackward</samp>.
                              </li>
                              <li class="li liexpand">
                                 <p class="p">Separated the cuDNN datatype references and APIs from the NVIDIA cuDNN Developer Guide into
                                    						a new <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html" target="_blank" shape="rect">NVIDIA cuDNN API Reference</a>.
                                 </p>
                              </li>
                              <li class="li liexpand">
                                 <p class="p">Published <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-best-practices/index.html" target="_blank" shape="rect"><u class="ph u">Best Practices For Using cuDNN 3D
                                          								Convolutions</u></a>.
                                 </p>
                              </li>
                           </ul>
                        </div>
                        <div class="section" id="rel_765__section_gzk_54c_2jb"><a name="rel_765__section_gzk_54c_2jb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, see the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel_765__section_flm_5pq_xlb"><a name="rel_765__section_flm_5pq_xlb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><em class="ph i">Updated: June 5, 2020</em><a name="rel_765__ul_apv_5pq_xlb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel_765__ul_apv_5pq_xlb">
                                 <li class="li">
                                    <p class="p">RNN and multihead attention API calls may exhibit non-deterministic behavior when the cuDNN
                                       							7.6.5 library is built with CUDA Toolkit 10.2 or higher. This is the
                                       							result of a new buffer management and heuristics in the cuBLAS library.
                                       							As described in the <a class="xref" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasXtApi_reproducibility" target="_blank" shape="rect"><u class="ph u">Results Reproducibility</u></a>
                                       							section in the <em class="ph i">cuBLAS Library User's Guide</em>, numerical results may
                                       							not be deterministic when cuBLAS APIs are launched in more than one CUDA
                                       							stream using the same cuBLAS handle. This is caused by two buffer sizes
                                       							(16 KB and 4 MB) used in the default configuration.
                                    </p>
                                    <p class="p">When a larger buffer size is not available at runtime, instead of waiting for a buffer of
                                       							that size to be released, a smaller buffer may be used with a different
                                       							GPU kernel. The kernel selection may affect numerical results. The user
                                       							can eliminate the non-deterministic behavior of cuDNN RNN and multihead
                                       							attention APIs, by setting a single buffer size in the
                                       								<samp class="ph codeph">CUBLAS_WORKSPACE_CONFIG</samp> environmental variable, for
                                       							example, <samp class="ph codeph">:16:8</samp> or <samp class="ph codeph">:4096:2</samp>.
                                    </p>
                                    <p class="p">The first configuration instructs cuBLAS to allocate eight buffers of 16 KB each in GPU
                                       							memory while the second setting creates two buffers of 4 MB each. The
                                       							default buffer configuration in cuBLAS 10.2 and 11.0 is
                                       								<samp class="ph codeph">:16:8:4096:2</samp>, that is, we have two buffer sizes. In
                                       							earlier cuBLAS libraries, such as cuBLAS 10.0, it used the
                                       								<samp class="ph codeph">:16:8</samp> non-adjustable configuration. When buffers of
                                       							only one size are available, the behavior of cuBLAS calls is
                                       							deterministic in multi-stream setups.
                                    </p>
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel_765__fixed-issues"><a name="rel_765__fixed-issues" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been fixed in this release:</p><a name="rel_765__ul_akt_jsr_cfb3" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_765__ul_akt_jsr_cfb3">
                              <li class="li liexpand">Corrected the documentation for <samp class="ph codeph">cudnnBatchNormalization*</samp> API functions,
                                 					clarifying which are optional arguments and when the user must pass them to the
                                 					API.
                              </li>
                              <li class="li liexpand">Fixed a lack-of-synchronization issue when <samp class="ph codeph">cudnnRNNBackwardData()</samp> and
                                 						<samp class="ph codeph">cudnnRNNBackwardDataEx()</samp> calls a kernel that is not
                                 					synchronized back to the application's stream. This issue only appears when
                                 					users are using bidirectional RNN using algo of
                                 						<samp class="ph codeph">CUDNN_RNN_ALGO_STANDARD</samp>. This issue affects cuDNN versions
                                 					5 through 7.6.4.
                              </li>
                              <li class="li liexpand">Corrected-supported tensor format tables for
                                 					<samp class="ph codeph">cudnnConvolutionForward()</samp>.
                              </li>
                              <li class="li liexpand"><samp class="ph codeph">cudnnConvolutionBackwardData</samp> used to give wrong answers when the kernel
                                 					size was &gt;=30 in any dimension and the stride is 2 in that dimension; with the
                                 					algorithm set to <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING</samp>.
                                 					This has been fixed.
                              </li>
                              <li class="li liexpand">Fixed an issue where if the user uses
                                 						<samp class="ph codeph">cudnnBatchNormalizationForwardInference</samp> with the mode of
                                 						<samp class="ph codeph">CUDNN_BATCHNORM_SPATIAL_PERSISTENT</samp>, the API will return
                                 						<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> and not fall back to
                                 						<samp class="ph codeph">CUDNN_BATCHNORM_SPATIAL</samp> mode. Now, it falls back correctly
                                 					similar to the behavior of the other batch normalization APIs including
                                 						<samp class="ph codeph">cudnnBatchNormalizationForwardTraining</samp>,
                                 						<samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx</samp>,
                                 						<samp class="ph codeph">cudnnBatchNormalizationBackward</samp>, and
                                 						<samp class="ph codeph">cudnnBatchNormalizationBackwardEx</samp>.
                              </li>
                              <li class="li liexpand">Previously, when cuDNN invoked <samp class="ph codeph">convolve_common_engine_int8_NHWC kernel</samp> for
                                 					NHWC format, irrespective of the output data precision, the output values were
                                 					clipped to be in the range from -128 to 127. In this release, we have fixed the
                                 					issue. As a result, output values are clipped only for INT8 precision. Whereas
                                 					if the output data is float precision, the values are not clipped.
                              </li>
                           </ul>
                        </div>
                        <div class="section" id="rel_765__section_rqy_mzx_smb"><a name="rel_765__section_rqy_mzx_smb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="rel_765__ul_wsh_nzx_smb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel_765__ul_wsh_nzx_smb">
                                 <li class="li"><em class="ph i">Updated: August 24, 2020</em><p class="p">Two-dimensional forward convolutions using algo1 may
                                       							segfault when the filter size is large. For example, we have observed
                                       							this issue when the filter width and height are more than or equal to
                                       							363.
                                    </p>
                                 </li>
                                 <li class="li"><em class="ph i">Updated: September 28, 2020</em><p class="p"><samp class="ph codeph">cudnnConvolutionForward()</samp>,
                                       								<samp class="ph codeph">cudnnConvolutionBackwardData()</samp>, and
                                       								<samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> calls with
                                       								<samp class="ph codeph">algo0</samp> or <samp class="ph codeph">algo1</samp> can result in an
                                       							illegal memory access for <samp class="ph codeph">PSEUDO_HALF_CONFIG</samp> data
                                       							configuration when the number of elements in the output tensor is odd.
                                       							This can be mitigated by allocating one extra element in the output
                                       							buffer.
                                    </p>
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_764"><a name="rel_764" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_764" name="rel_764" shape="rect">2.2.&nbsp;cuDNN Release 7.6.4</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"> This is the cuDNN 7.6.4 release notes. This release includes fixes from the previous
                           		cuDNN v7.x.x releases as well as the following additional changes. 
                        </p>
                        <div class="section" id="rel_764__section_l2m_s4c_2jb"><a name="rel_764__section_l2m_s4c_2jb" shape="rect">
                              <!-- --></a><p class="p">For previous cuDNN release notes, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-archived/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel_764__features-enhancements"><a name="rel_764__features-enhancements" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <p class="p">The following features and enhancements have been added to this release:</p><a name="rel_764__ul_cht_2pb_5gb" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_764__ul_cht_2pb_5gb">
                              <li class="li liexpand">Gained significant speed-up in multihead-attention forward training and
                                 					inference.  
                              </li>
                           </ul>
                        </div>
                        <div class="section" id="rel_764__section_gzk_54c_2jb"><a name="rel_764__section_gzk_54c_2jb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Compatibility</h3>
                           <p class="p">For the latest compatibility software versions of the OS, CUDA, the CUDA driver, and the
                              				NVIDIA hardware, see the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-support-matrix/index.html#cudnn-cuda-hardware-versions" target="_blank" shape="rect">NVIDIA cuDNN Support Matrix</a>.
                           </p>
                        </div>
                        <div class="section" id="rel_764__section_yz4_v4c_2jb"><a name="rel_764__section_yz4_v4c_2jb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Limitations</h3>
                           <div class="p"><a name="rel_764__ul_gpc_w4c_2jb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel_764__ul_gpc_w4c_2jb">
                                 <li class="li">When launching a CUDA graph constructed using a stream capture that includes a
                                    							<samp class="ph codeph">cudnnConvolutionForward</samp> operation, the subsequent
                                    						synchronization point reports a <samp class="ph codeph">cudaErrorLaunchFailure</samp>
                                    						error. This error appears when cuDNN is set to use a non-default
                                    						stream.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section" id="rel_764__fixed-issues"><a name="rel_764__fixed-issues" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been fixed in this release:</p><a name="rel_764__ul_akt_jsr_cfb3" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_764__ul_akt_jsr_cfb3">
                              <li class="li liexpand">Earlier versions of cuDNN v7.6 contained symbols that would conflict with those of in
                                 					TensorRT 5.1 and later. In some cases, these conflicts could lead to application
                                 					crashes when applications linked against cuDNN and TensorRT. This issue is fixed
                                 					in cuDNN 7.6.4.
                              </li>
                              <li class="li liexpand">Addressed the regressions that were introduced in the
                                 						<samp class="ph codeph">cudnnConvolutionBiasActivationForward</samp> function in cuDNN
                                 					7.6.3. Previously, if this API had different values in destination data buffer
                                 					and zData buffer, then incorrect results were computed. This issue has been
                                 					resolved and now the API will compute correct results even if users provide an
                                 					arbitrary set of values to the destination data and zData.
                              </li>
                              <li class="li liexpand">Multihead attention will now return <samp class="ph codeph">CUDNN_STATUS_ARCH_MISMATCH</samp> for
                                 					true-half configuration on devices with compute capability less than 5.3 (for
                                 					example, most of Maxwell and all of NVIDIA Kepler, and so on), which do not have
                                 					native hardware support for true half computation. Previously, an error like
                                 						<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> may be triggered or
                                 					inaccurate results may be produced.
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_763"><a name="rel_763" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_763" name="rel_763" shape="rect">2.3.&nbsp;cuDNN Release 7.6.3</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"> This is the cuDNN 7.6.3 release notes. This release includes fixes from the previous
                           		cuDNN v7.x.x releases as well as the following additional changes. These release notes are
                           		applicable to both cuDNN and NVIDIA JetPack users unless appended specifically with <em class="ph i">(not
                              			applicable for Jetson platforms)</em>. 
                        </p>
                        <div class="section" id="rel_763__section_yyd_pgx_3jb"><a name="rel_763__section_yyd_pgx_3jb" shape="rect">
                              <!-- --></a><p class="p">For previous cuDNN release notes, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-archived/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel_763__features-enhancements"><a name="rel_763__features-enhancements" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <p class="p">The following features and enhancements have been added to this release:</p><a name="rel_763__ul_cht_2pb_5gb" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_763__ul_cht_2pb_5gb">
                              <li class="li liexpand">The cuDNN 7.6.3 library now supports auto-padding for NHWC layout. The functional behavior,
                                 					and the benefits of auto-padding as follows: <em class="ph i">(not applicable for Jetson
                                    						platforms)</em><a name="rel_763__ul_bbj_vnf_t3b" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_763__ul_bbj_vnf_t3b">
                                    <li class="li liexpand">For use cases where C and K dimensions of input and filter Tensors are
                                       							not multiples of 8, the auto-padding feature increases the Tensor size
                                       							so that the Tensor dimensions are multiples of eight. 
                                    </li>
                                    <li class="li liexpand">With auto-padding, the cuDNN library invokes faster kernels, improving
                                       							the performance. 
                                    </li>
                                    <li class="li liexpand">With auto-padding, the performance with NHWC data layout is now
                                       							comparable to that of the NCHW layout. 
                                    </li>
                                 </ul>
                              </li>
                              <li class="li liexpand">Added support for <samp class="ph codeph">dataType=CUDNN_DATA_HALF</samp> and
                                 						<samp class="ph codeph">computePrec=CUDNN_DATA_HALF</samp> in multihead attention forward
                                 						(<a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnMultiHeadAttnForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnMultiHeadAttnForward()</samp></a>) and backward
                                 					(gradient) (<a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnMultiHeadAttnBackwardData" target="_blank" shape="rect"><samp class="ph codeph">cudnnMultiHeadAttnBackwardData()</samp></a> and <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnMultiHeadAttnBackwardWeights" target="_blank" shape="rect"><samp class="ph codeph">cudnnMultiHeadAttnBackwardWeights()</samp></a>) API
                                 					functions. <em class="ph i">(not applicable for Jetson platforms)</em></li>
                              <li class="li liexpand">
                                 <p class="p">Multihead attention API now supports bias after the projections on <strong class="ph b">Q</strong>, <strong class="ph b">K</strong>,
                                    							<strong class="ph b">V</strong>, and <strong class="ph b">O</strong> in the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnMultiHeadAttnForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnMultiHeadAttnForward()</samp></a> call (backward
                                    						bias gradient is not yet supported). <em class="ph i">(not applicable for Jetson
                                       							platforms)</em></p>
                                 <p class="p">The new feature required a small API change in <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnSetAttnDescriptor" target="_blank" shape="rect"><samp class="ph codeph">cudnnSetAttnDescriptor()</samp></a>: the
                                    							<samp class="ph codeph">cudnnAttnQueryMap_t queryMap</samp> argument is replaced with
                                    							<samp class="ph codeph">unsigned attnMode</samp> to pass various on and off options.
                                    						This change is backward compatible with earlier API versions. <em class="ph i">(not
                                       							applicable for Jetson platforms)</em></p>
                              </li>
                              <li class="li liexpand">Significantly improved the performance in typical multihead attention use cases in forward
                                 					inference and training, especially when the vector length of each head is a
                                 					multiple of 32 up to 128. <em class="ph i">(not applicable for Jetson platforms)</em></li>
                              <li class="li liexpand">Tensor Core support is added for true half and single-precision use cases in multihead
                                 					attention. Users may use it by setting the <samp class="ph codeph">mathType</samp> argument in
                                 						<a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnSetAttnDescriptor" target="_blank" shape="rect"><samp class="ph codeph">cudnnSetAttnDescriptor()</samp></a> to
                                 						<samp class="ph codeph">CUDNN_TENSOR_OP_MATH</samp> or
                                 						<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp>. <em class="ph i">(not applicable
                                    						for Jetson platforms)</em></li>
                              <li class="li liexpand">The <samp class="ph codeph">multiHeadAttention</samp> sample code is added. The sample code includes a
                                 					compact NumPy/Autograd reference model of the multihead attention block that
                                 					computes the forward response and all first-order derivatives. The test code
                                 					demonstrates how to use the multihead attention API, access attention weights,
                                 					and sequence data. <em class="ph i">(not applicable for Jetson platforms)</em></li>
                              <li class="li liexpand">Improved depth-wise convolution for forward, <samp class="ph codeph">dgrad</samp>, and
                                 						<samp class="ph codeph">wgrad</samp> under the following conditions: <a name="rel_763__ul_kzs_cvl_t3b" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_763__ul_kzs_cvl_t3b">
                                    <li class="li liexpand">Algorithm is algo1.</li>
                                    <li class="li liexpand">Tensor format for filter is NCHW (<samp class="ph codeph">wgrad</samp> supports NHWC
                                       							also).
                                    </li>
                                    <li class="li liexpand">Input and outputs are in FP16 and computation is in FP32.</li>
                                    <li class="li liexpand">Filter size: 1x1, 3x3, 5x5, 7x7 (<samp class="ph codeph">dgrad</samp> only supports
                                       							stride 1).
                                    </li>
                                    <li class="li liexpand">Math type is CUDNN_DEFAULT_MATH.</li>
                                 </ul>
                              </li>
                              <li class="li liexpand">Improved-grouped convolution for <samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> in the
                                 					configuration under the following conditions: <a name="rel_763__ul_ijd_gvl_t3b" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_763__ul_ijd_gvl_t3b">
                                    <li class="li liexpand">Algorithm is <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1</samp>.
                                    </li>
                                    <li class="li liexpand">Math type is <samp class="ph codeph">CUDNN_DEFAULT_MATH</samp>.
                                    </li>
                                    <li class="li liexpand">Tensor format for filter is NCHW.</li>
                                    <li class="li liexpand">Input and outputs are in FP16 and computation is in FP32.</li>
                                    <li class="li liexpand">Filter size: 1x1, 3x3, 5x5, 7x7 </li>
                                 </ul>
                              </li>
                              <li class="li liexpand">Improved the performance of grouped convolution, for
                                 						<samp class="ph codeph">cudnnConvolutionForward()</samp> in the configuration under the
                                 					following conditions: <a name="rel_763__ul_ywf_jvl_t3b" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_763__ul_ywf_jvl_t3b">
                                    <li class="li liexpand">Algorithm is
                                       								<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp></li>
                                    <li class="li liexpand">Math type is <samp class="ph codeph">CUDNN_TENSOR_OP_MATH</samp> or
                                       								<samp class="ph codeph">CUDNN_TENSOROP_MATH_ALLOW_CONVERSION</samp></li>
                                    <li class="li liexpand">Tensor format for filter is NHWC.</li>
                                    <li class="li liexpand">Input and outputs are in FP16 and computation is in FP16/ FP32.</li>
                                    <li class="li liexpand">Per group C and K == 4/8/16/32</li>
                                    <li class="li liexpand">Filter size: 3x3</li>
                                 </ul>
                              </li>
                              <li class="li liexpand">Improved the performance of grouped convolution, for
                                 						<samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> in the configuration under
                                 					the following conditions: <a name="rel_763__ul_rhz_mvl_t3b" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_763__ul_rhz_mvl_t3b">
                                    <li class="li liexpand">Algorithm is <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1</samp></li>
                                    <li class="li liexpand">Math type is <samp class="ph codeph">CUDNN_TENSOR_OP_MATH</samp> or
                                       								<samp class="ph codeph">CUDNN_TENSOROP_MATH_ALLOW_CONVERSION</samp></li>
                                    <li class="li liexpand">Tensor format for filter is NHWC.</li>
                                    <li class="li liexpand">Input and outputs are in FP16 and computation is in FP32.</li>
                                    <li class="li liexpand">On NVIDIA Volta (compute capability 7.0)</li>
                                    <li class="li liexpand">Per group C and K == 4/8/16/32</li>
                                    <li class="li liexpand">Filter size: 1x1, 3x3</li>
                                 </ul>
                              </li>
                           </ul>
                        </div>
                        <div class="section" id="rel_763__fixed-issues"><a name="rel_763__fixed-issues" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been
                              				fixed in this release:
                           </p><a name="rel_763__ul_akt_jsr_cfb3" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_763__ul_akt_jsr_cfb3">
                              <li class="li liexpand">Fixed an issue where <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnMultiHeadAttnBackwardData" target="_blank" shape="rect"><samp class="ph codeph">cudnnMultiHeadAttnBackwardData()</samp></a> was producing
                                 					incorrect results when K sequence length is longer than 32.
                              </li>
                              <li class="li liexpand">Fixed a race condition in <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnMultiHeadAttnBackwardData" target="_blank" shape="rect"><samp class="ph codeph">cudnnMultiHeadAttnBackwardData()</samp></a> that was
                                 					producing intermittent incorrect results.
                              </li>
                              <li class="li liexpand">The function <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnCTCLoss" target="_blank" shape="rect"><samp class="ph codeph">cudnnCTCLoss()</samp></a>
                                 					produced incorrect gradient result for label whose length is smaller than the
                                 					maximal sequence length in the batch. This is fixed in cuDNN 7.6.3.
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_762"><a name="rel_762" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_762" name="rel_762" shape="rect">2.4.&nbsp;cuDNN Release 7.6.2</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"> This is the cuDNN 7.6.2 release notes. This release includes fixes from the previous
                           		cuDNN v7.x.x releases as well as the following additional changes. 
                        </p>
                        <div class="section" id="rel_762__section_yyd_pgx_3jb"><a name="rel_762__section_yyd_pgx_3jb" shape="rect">
                              <!-- --></a><p class="p">For previous cuDNN release notes, refer to the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-archived/index.html" target="_blank" shape="rect">NVIDIA cuDNN Archives</a>.
                           </p>
                        </div>
                        <div class="section" id="rel_762__features-enhancements"><a name="rel_762__features-enhancements" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <p class="p">The following features and enhancements have been added to this release:</p><a name="rel_762__ul_cht_2pb_5gb" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_762__ul_cht_2pb_5gb">
                              <li class="li liexpand">Enhanced the performance of 3D deconvolution using <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBackwardData" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardData()</samp></a>, for the
                                 					following configuration: <a name="rel_762__ul_ul4_bqs_m3b" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_762__ul_ul4_bqs_m3b">
                                    <li class="li liexpand">2x2x2 filter and 2x2x2 convolution stride. </li>
                                    <li class="li liexpand">For FP16 for data input and output, and for accumulation. </li>
                                    <li class="li liexpand">For FP32 for data input and output, and for accumulation. </li>
                                 </ul>
                              </li>
                              <li class="li liexpand">Enhanced the performance of 3D convolution using <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionForward()</samp></a>, for the following
                                 					configuration: <a name="rel_762__ul_tv4_fqs_m3b" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_762__ul_tv4_fqs_m3b">
                                    <li class="li liexpand">Tensor Core for FP16 for data input and output and FP32 accumulation
                                       							when <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnMathType_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnMathType_t</samp></a> is set. 
                                    </li>
                                    <li class="li liexpand">Tensor Core for FP32 for data input and output and FP32 accumulation
                                       							when <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnMathType_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnMathType_t</samp></a> is set.
                                    </li>
                                 </ul>
                              </li>
                              <li class="li liexpand">Enhanced the functionality of the data type <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnFusedOps_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnFusedOps_t</samp></a> by
                                 					adding the following three enums: <a name="rel_762__ul_xpl_zdr_l3b" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_762__ul_xpl_zdr_l3b">
                                    <li class="li liexpand"><samp class="ph codeph">CUDNN_FUSED_CONV_SCALE_BIAS_ADD_ACTIVATION</samp></li>
                                    <li class="li liexpand"><samp class="ph codeph">CUDNN_FUSED_SCALE_BIAS_ADD_ACTIVATION_GEN_BITMASK</samp>,
                                       							and
                                    </li>
                                    <li class="li liexpand"><samp class="ph codeph">CUDNN_FUSED_DACTIVATION_FORK_DBATCHNORM</samp></li>
                                 </ul>
                              </li>
                           </ul>
                        </div>
                        <div class="section" id="rel_762__fixed-issues"><a name="rel_762__fixed-issues" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been
                              				fixed in this release:
                           </p><a name="rel_762__ul_akt_jsr_cfb3" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_762__ul_akt_jsr_cfb3">
                              <li class="li liexpand">In cuDNN 7.6.1, on NVIDIA Volta architecture only, there may be a performance degradation
                                 					when the function <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> is used for 3D
                                 					convolutions with <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBwdFilterAlgo_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBwdFilterAlgo_t</samp></a>. This is fixed
                                 					in cuDNN 7.6.2. 
                              </li>
                              <li class="li liexpand">In cuDNN 7.6.1, on NVIDIA Turing and NVIDIA Pascal architectures, performance may be
                                 					degraded for <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBackwardData" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardData()</samp></a>, when used with
                                 					the following conditions: <a name="rel_762__ul_aqg_v2r_l3b" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_762__ul_aqg_v2r_l3b">
                                    <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBwdDataAlgo_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBwdDataAlgo_t</samp></a> for 3D
                                       							convolutions 
                                    </li>
                                    <li class="li liexpand"><samp class="ph codeph">wDesc</samp>, <samp class="ph codeph">dyDesc</samp>, and
                                       								<samp class="ph codeph">dxDesc</samp> are all in NCDHW.
                                    </li>
                                    <li class="li liexpand">Data type configuration is <samp class="ph codeph">FLOAT_CONFIG</samp> (that is,
                                       							single-precision data and compute).
                                    </li>
                                 </ul>
                                 <p class="lines"> 
                                    This is fixed in cuDNN 7.6.2. 
                                 </p>
                              </li>
                              <li class="li liexpand">In cuDNN 7.6.1, in some cases the function <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBackwardData" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardData()</samp></a> may fail with
                                 					“disallowed mismatches” error on NVIDIA Turing (T4) and NVIDIA Volta (V100)
                                 					architectures, when used with the following configuration: <a name="rel_762__ul_mvf_rfr_l3b" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_762__ul_mvf_rfr_l3b">
                                    <li class="li liexpand"> Algorithm is <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBwdDataAlgo_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBwdDataAlgo_t</samp></a></li>
                                    <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnMathType_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnMathType_t</samp></a> is
                                       								<samp class="ph codeph">CUDNN_TENSOR_OP_MATH</samp> or
                                       								<samp class="ph codeph">CUDNN_TENSOROP_MATH_ALLOW_CONVERSION</samp></li>
                                    <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnTensorFormat_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnTensorFormat_t</samp></a> for filter is
                                       							NCHW.
                                    </li>
                                    <li class="li liexpand">Input and outputs are in FP16 and computation is in FP32.</li>
                                 </ul>
                                 <p class="lines"> 
                                    This is fixed in cuDNN 7.6.2. 
                                 </p>
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_761"><a name="rel_761" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_761" name="rel_761" shape="rect">2.5.&nbsp;cuDNN Release 7.6.1</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"> This is the cuDNN 7.6.1 release notes. This release includes fixes from the previous
                           		cuDNN v7.x.x releases as well as the following additional changes. 
                        </p>
                        <div class="section" id="rel_761__features-enhancements"><a name="rel_761__features-enhancements" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <p class="p">The following features and enhancements have been added to this release:</p><a name="rel_761__ul_cht_2pb_5gb" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_761__ul_cht_2pb_5gb">
                              <li class="li liexpand">Performance is enhanced for 3D convolutions using Tensor Core for FP16 input and output data
                                 					types, whenever they are supported. Moreover, for single-precision (FP32) I/O,
                                 					cuDNN 7.6.1 will use these enhanced kernels whenever possible, and only when
                                 						<a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnMathType_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnMathType_t</samp></a> is
                                 					set to <samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp>. See <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionForward()</samp></a> and <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBackwardData" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardData()</samp></a> and <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a>. 
                              </li>
                              <li class="li liexpand">On Maxwell and NVIDIA Pascal architectures only, the performance of 3D convolutions with the
                                 					kernel size of 128^3, when used with
                                 						<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1</samp>, is enhanced.
                              </li>
                              <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#api-logging" target="_blank" shape="rect">API logging</a> is fully implemented for
                                 					the experimental multihead attention API, namely, for the following
                                 						functions:<a name="rel_761__ul_nt1_nxb_5gb" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_761__ul_nt1_nxb_5gb">
                                    <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnCreateAttnDescriptor" target="_blank" shape="rect"><samp class="ph codeph">cudnnCreateAttnDescriptor()</samp></a></li>
                                    <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnDestroyAttnDescriptor" target="_blank" shape="rect"><samp class="ph codeph">cudnnDestroyAttnDescriptor()</samp></a></li>
                                    <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnSetAttnDescriptor" target="_blank" shape="rect"><samp class="ph codeph">cudnnSetAttnDescriptor()</samp></a></li>
                                    <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnGetAttnDescriptor" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetAttnDescriptor()</samp></a></li>
                                    <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnGetMultiHeadAttnBuffers" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetMultiHeadAttnBuffers()</samp></a></li>
                                    <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnGetMultiHeadAttnWeights" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetMultiHeadAttnWeights()</samp></a></li>
                                    <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnMultiHeadAttnForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnMultiHeadAttnForward()</samp></a></li>
                                    <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnMultiHeadAttnBackwardData" target="_blank" shape="rect"><samp class="ph codeph">cudnnMultiHeadAttnBackwardData()</samp></a></li>
                                    <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnMultiHeadAttnBackwardWeights" target="_blank" shape="rect"><samp class="ph codeph">cudnnMultiHeadAttnBackwardWeights()</samp></a></li>
                                    <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnSetSeqDataDescriptor" target="_blank" shape="rect"><samp class="ph codeph">cudnnSetSeqDataDescriptor()</samp></a></li>
                                    <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnGetSeqDataDescriptor" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetSeqDataDescriptor()</samp></a></li>
                                    <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnCreateSeqDataDescriptor" target="_blank" shape="rect"><samp class="ph codeph">cudnnCreateSeqDataDescriptor()</samp></a></li>
                                    <li class="li liexpand"><a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnDestroySeqDataDescriptor" target="_blank" shape="rect"><samp class="ph codeph">cudnnDestroySeqDataDescriptor()</samp></a></li>
                                 </ul>
                              </li>
                              <li class="li liexpand">Performance of the experimental multihead attention forward API is enhanced. For more
                                 					information, refer to  <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnMultiHeadAttnForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnMultiHeadAttnForward()</samp></a>.
                              </li>
                              <li class="li liexpand">Performance is enhanced for the fused convolution and fused <samp class="ph codeph">wgrad</samp> fallback
                                 					path. For more information, refer to  <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnFusedOps_t" target="_blank" shape="rect"><samp class="ph codeph">cudnnFusedOps_t</samp></a>. 
                              </li>
                           </ul>
                        </div>
                        <div class="section" id="rel_761__fixed-issues"><a name="rel_761__fixed-issues" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been
                              				fixed in this release:
                           </p><a name="rel_761__ul_akt_jsr_cfb3" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_761__ul_akt_jsr_cfb3">
                              <li class="li liexpand">In cuDNN 7.6.0, the function <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnGetConvolutionBackwardDataWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetConvolutionBackwardDataWorkspaceSize()</samp></a>
                                 					returns a value for which <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBackwardData" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardData()</samp></a>, when used with
                                 						<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp>, returns
                                 						<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp>. This is fixed in cuDNN 7.6.1 so
                                 					that now <samp class="ph codeph">cudnnGetConvolutionBackwardDataWorkspaceSize()</samp> returns
                                 					a proper value for <samp class="ph codeph">cudnnConvolutionBackwardData()</samp>. 
                              </li>
                              <li class="li liexpand">
                                 <div class="p">In cuDNN 7.6.0 and earlier versions, when all the following conditions are
                                    						true, <a name="rel_761__ul_kqs_jf1_d3b" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel_761__ul_kqs_jf1_d3b">
                                       <li class="li liexpand">RNN model is bi-directional,</li>
                                       <li class="li liexpand">Cell type is LSTM,</li>
                                       <li class="li liexpand"><samp class="ph codeph">cudnnRNNAlgo_t= CUDNN_RNN_ALGO_STANDARD</samp>, and
                                       </li>
                                       <li class="li liexpand">Dropout probability was greater than zero,</li>
                                    </ul>
                                    
                                    then the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnRNNBackwardWeights" target="_blank" shape="rect"><samp class="ph codeph">cudnnRNNBackwardWeights()</samp></a> function produces
                                    						inaccurate and occasionally non-deterministic results. This is fixed in
                                    						cuDNN 7.6.1.
                                 </div>
                                 <p class="p">An underlying issue, where the same buffer was used for left to right and
                                    						right-to-left directions when re-computing forward dropout results passed
                                    						from one RNN layer to the next, was the cause of the bug. 
                                 </p>
                              </li>
                              <li class="li liexpand">
                                 <p class="p">A bug in cuDNN 7.6.0 and earlier versions, in the <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnRNNForwardTraining" target="_blank" shape="rect"><samp class="ph codeph">cudnnRNNForwardTraining()</samp></a> function, related
                                    						to dropout, is fixed in cuDNN 7.6.1.
                                 </p>
                                 <div class="p">When all the following conditions are true:<a name="rel_761__ul_t5s_hg1_d3b" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel_761__ul_t5s_hg1_d3b">
                                       <li class="li liexpand"><samp class="ph codeph">cudnnRNNAlgo_t=CUDNN_RNN_ALGO_PERSIST_STATIC</samp>,
                                       </li>
                                       <li class="li liexpand"><samp class="ph codeph">cudnnMathType_t</samp> is
                                          									<samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp>, and
                                       </li>
                                       <li class="li liexpand">input data type is <samp class="ph codeph">CUDNN_DATA_FLOAT</samp>,
                                       </li>
                                    </ul>
                                    
                                    then the FP32-to-FP16 conversion might be applied as a performance
                                    						optimization.
                                 </div>
                                 <p class="p">When this down conversion is scheduled, a GPU kernel invoked by <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnDropoutForward" target="_blank" shape="rect"><samp class="ph codeph">cudnnDropoutForward()</samp></a> would crash due to
                                    						incorrect parameters being passed. In this case CUDA runtime reports the
                                    						"misaligned address" error when reading the data from global memory.
                                 </p>
                              </li>
                              <li class="li liexpand">In cuDNN 7.6.0, on RHEL7 only, the
                                 						<samp class="ph codeph">/usr/src/cudnn_samples_v7/samples_common.mk</samp> file is
                                 					missing. This requires a workaround to compile the cuDNN samples. This is fixed
                                 					in cuDNN 7.6.1 and the workaround is not needed for cuDNN 7.6.1. 
                              </li>
                              <li class="li liexpand">In cuDNN 7.6.0, on pre-Volta hardware only, the function <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnGetConvolutionBackwardFilterWorkspaceSize" target="_blank" shape="rect"><samp class="ph codeph">cudnnGetConvolutionBackwardFilterWorkspaceSize()</samp></a>
                                 					can erroneously return <samp class="ph codeph">CUDNN_STATUS_SUCCESS</samp> for <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> for 3D
                                 					convolutions, using <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1</samp> with
                                 					NDHWC layout. When this occurs, the
                                 						<samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> function will process the
                                 					data using a kernel that expects the data in NCDHW layout (the only format
                                 					supported by <samp class="ph codeph">wDesc</samp> in this case), leading to incorrect results.
                                 					In cuDNN 7.6.1, this is fixed so that
                                 						<samp class="ph codeph">cudnnGetConvolutionBackwardFilterWorkspaceSize()</samp> will now
                                 					return <samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp>.
                              </li>
                              <li class="li liexpand">In cuDNN 7.5.x and 7.6.0 for Jetson platform, in some cases the function <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBackwardData" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardData()</samp></a> , when used with
                                 						<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD</samp>, might return
                                 					incorrect results. This is fixed in cuDNN 7.6.1.
                              </li>
                              <li class="li liexpand">When the data type configuration is <samp class="ph codeph">FLOAT_CONFIG</samp>, then
                                 						<samp class="ph codeph">cudnnGetConvolution*Algorithm()</samp>, for a few convolution
                                 					sizes, incorrectly returns a slow algorithm for the NVIDIA Pascal architecture.
                                 					This is fixed in cuDNN 7.5.0 and later versions. 
                              </li>
                              <li class="li liexpand">When using the <samp class="ph codeph">fusedOps</samp> API with the enum
                                 						<samp class="ph codeph">CUDNN_FUSED_SCALE_BIAS_ACTIVATION_CONV_BNSTATS</samp> or
                                 						<samp class="ph codeph">CUDNN_FUSED_SCALE_BIAS_ACTIVATION_WGRAD</samp>, and when input
                                 					tensor is in NCHW format or is not fully packed, then incorrect results may be
                                 					produced. This is now fixed in cuDNN 7.6.1.
                              </li>
                           </ul>
                        </div>
                        <div class="section" id="rel_761__known-issues"><a name="rel_761__known-issues" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <p class="p">The following issues and limitations exist in this release:</p><a name="rel_761__ul_lg3_tkx_gfb" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_761__ul_lg3_tkx_gfb">
                              <li class="li liexpand">Algorithms returned by <samp class="ph codeph">cudnnGetConvolution*Algorithm()</samp> may, in some limited
                                 					use cases, fail to execute when they are actually run. This is a cuDNN
                                 					library-wide issue and applies for convolution forward, convolution backward
                                 					data, and convolution backward filter operations. This issue is also present in
                                 					versions before cuDNN 7.6.1.
                              </li>
                              <li class="li liexpand">When the input and output tensors are in NHWC and the filter is 1x1 and NCHW, the
                                 					performance of the function <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBackwardData" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardData()</samp></a> might be
                                 					degraded. 
                              </li>
                              <li class="li liexpand">In cuDNN 7.6.1, when using the experimental multihead attention API, it is possible that the
                                 					forward and backward paths produce different results for the BERT model, when
                                 					the batch size is greater than one and the number of heads is greater than one. 
                              </li>
                              <li class="li liexpand">In cuDNN 7.6.1, on NVIDIA Volta architecture only, there may be a performance degradation
                                 					when the function <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBackwardFilter" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp></a> is used for 3D
                                 					convolutions with <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1</samp>. 
                              </li>
                              <li class="li liexpand">In cuDNN 7.6.1, on NVIDIA Turing and NVIDIA Pascal architectures, performance may be
                                 					degraded for <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-api/index.html#cudnnConvolutionBackwardData" target="_blank" shape="rect"><samp class="ph codeph">cudnnConvolutionBackwardData()</samp></a>, when used with
                                 					the following conditions: <a name="rel_761__ul_w1t_4ww_23b" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_761__ul_w1t_4ww_23b">
                                    <li class="li liexpand"><samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp> for 3D
                                       							convolutions.
                                    </li>
                                    <li class="li liexpand"><samp class="ph codeph">wDesc</samp>, <samp class="ph codeph">dyDesc</samp>, and
                                       								<samp class="ph codeph">dxDesc</samp> are all in NCDHW.
                                    </li>
                                    <li class="li liexpand">Data type configuration is <samp class="ph codeph">FLOAT_CONFIG</samp> (that is,
                                       							single-precision data and compute).
                                    </li>
                                 </ul>
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_760"><a name="rel_760" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_760" name="rel_760" shape="rect">2.6.&nbsp;cuDNN Release 7.6.0</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"> This is the cuDNN 7.6.0 release notes. This release includes fixes from the previous
                           		cuDNN v7.x.x releases as well as the following additional changes. 
                        </p>
                        <div class="section" id="rel_760__features-enhancements"><a name="rel_760__features-enhancements" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <p class="p">The following features and enhancements have been added to this release:</p><a name="rel_760__ul_cht_2pb_5gb" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_760__ul_cht_2pb_5gb">
                              <li class="li liexpand">
                                 <p class="p">A new API is introduced for fused ops, which can accelerate many use cases in
                                    						ResNet-like networks. With this new API, it is now possible to execute
                                    						various fused operations such as apply per channel scale and bias, perform
                                    						activation, compute convolution, and generate batchnorm statistics. Below is
                                    						a list of supported datatype and functions in this API:
                                 </p>
                                 <div class="p"><strong class="ph b">Datatypes</strong>:<a name="rel_760__ul_b12_dj5_qjb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel_760__ul_b12_dj5_qjb">
                                       <li class="li"><samp class="ph codeph">cudnnFusedOpsVariantParamPack_t</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnFusedOpsConstParamPack_t</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnFusedOpsPlan_t</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnFusedOps_t</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnFusedOpsConstParamLabel_t</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnFusedOpsPointerPlaceHolder_t</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnFusedOpsVariantParamLabel_t</samp></li>
                                    </ul>
                                 </div>
                                 <div class="p"><strong class="ph b">Functions</strong>:<a name="rel_760__ul_kxl_y35_qjb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel_760__ul_kxl_y35_qjb">
                                       <li class="li"><samp class="ph codeph">cudnnCreateFusedOpsConstParamPack</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnDestroyFusedOpsConstParamPack</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnSetFusedOpsConstParamPackAttribute</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnGetFusedOpsConstParamPackAttribute</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnCreateFusedOpsVariantParamPack</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnDestroyFusedOpsVariantParamPack</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnSetFusedOpsVariantParamPackAttribute</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnGetFusedOpsVariantParamPackAttribute</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnCreateFusedOpsPlan</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnDestroyFusedOpsPlan</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnMakeFusedOpsPlan</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnFusedOpsExecute</samp></li>
                                    </ul>
                                 </div>
                              </li>
                              <li class="li liexpand">Improved the performance of grouped convolution layers in ResNeXt-50, for
                                 						<samp class="ph codeph">cudnnConvolutionBackwardData()</samp> in the configuration below:
                                 						<a name="rel_760__ul_tfh_5hn_thb" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_760__ul_tfh_5hn_thb">
                                    <li class="li liexpand">On NVIDIA Volta (compute capability 7.0)</li>
                                    <li class="li liexpand">Algorithm is <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp></li>
                                    <li class="li liexpand">Stride of 1</li>
                                    <li class="li liexpand">Math type is <samp class="ph codeph">CUDNN_TENSOR_OP_MATH</samp> or
                                       								<samp class="ph codeph">CUDNN_TENSOROP_MATH_ALLOW_CONVERSION</samp></li>
                                    <li class="li liexpand">Tensor format for filter is NHWC.</li>
                                    <li class="li liexpand">Input and outputs are in FP16 and computation is in FP32.</li>
                                 </ul>
                              </li>
                              <li class="li liexpand">A new API is introduced to enhance the inference time. With this new API, it is now possible
                                 					to separate the filter layout transformation that was applied on every call,
                                 					which in turn leads to inference time enhancement. Below is a list of supported
                                 					datatype and functions in this API. <a name="rel_760__ul_eqn_jj5_qjb" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_760__ul_eqn_jj5_qjb">
                                    <li class="li"><samp class="ph codeph">cudnnReorderType_t</samp></li>
                                    <li class="li"><samp class="ph codeph">cudnnReorderFilterAndBias</samp></li>
                                    <li class="li"><samp class="ph codeph">cudnnSetConvolutionReorderType</samp></li>
                                    <li class="li"><samp class="ph codeph">cudnnGetConvolutionReorderType</samp></li>
                                 </ul>
                              </li>
                              <li class="li liexpand"> Performance is enhanced (by selecting a faster kernel) on NVIDIA T4 cards for
                                 					INT8x4 and INT8x32.    
                              </li>
                           </ul>
                        </div>
                        <div class="section" id="rel_760__fixed-issues"><a name="rel_760__fixed-issues" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been
                              				fixed in this release:
                           </p><a name="rel_760__ul_akt_jsr_cfb3" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_760__ul_akt_jsr_cfb3">
                              <li class="li liexpand">In cuDNN 7.5.0 and cuDNN 7.5.1, a bug in the <samp class="ph codeph">cudnnRNNBackwardData()</samp>
                                 					function affected the thread synchronization. This effect is limited to only the
                                 					first iteration of the loop, and only in some paths. This occurs when using the
                                 					function with the CUDNN_RNN_ALGO_PERSIST_STATIC method. This is fixed in cuDNN
                                 					7.6.0.
                              </li>
                           </ul>
                        </div>
                        <div class="section" id="rel_760__known-issues"><a name="rel_760__known-issues" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <p class="p">The following issues and limitations exist in this release:</p><a name="rel_760__ul_lg3_tkx_gfb" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_760__ul_lg3_tkx_gfb">
                              <li class="li liexpand">The <samp class="ph codeph">cudnnConvolutionBackwardData()</samp> function for
                                 						<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_0</samp> fails with
                                 						<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when the input size is large. 
                              </li>
                              <li class="li liexpand">A general known issue for cuDNN library: the Tensor pointers and the filter pointers
                                 					require at a minimum 4-byte alignment, including for FP16 or INT8 data. 
                              </li>
                              <li class="li liexpand">On RHEL7 only, the <samp class="ph codeph">/usr/src/cudnn_samples_v7/samples_common.mk</samp> file is missing.
                                 					This will prevent compiling the cuDNN samples. The workaround is to copy the
                                 					below contents into <samp class="ph codeph">samples_common.mk</samp> text file and place this
                                 					file in the <samp class="ph codeph">/usr/src/cudnn_samples_v7/</samp> directory, so that the
                                 						<samp class="ph codeph">/usr/src/cudnn_samples_v7/samples_common.mk</samp> file exists. 
                                 <div class="p"><pre xml:space="preserve"># Setting SMS for all samples
# architecture

ifneq ($(TARGET_ARCH), ppc64le)
CUDA_VERSION := $(shell cat $(CUDA_PATH)/include/cuda.h |grep "define CUDA_VERSION" |awk '{print $$3}')
else
CUDA_VERSION := $(shell cat $(CUDA_PATH)/targets/ppc64le-linux/include/cuda.h |grep "define CUDA_VERSION" |awk '{print $$3}')
endif

#Link against cublasLt for CUDA 10.1 and up.
CUBLASLT:=false
ifeq ($(shell test $(CUDA_VERSION) -ge 10010; echo $$?),0)
CUBLASLT:=true
endif
$(info Linking agains cublasLt = $(CUBLASLT))

ifeq ($(CUDA_VERSION),8000 )
SMS_VOLTA =
else
ifneq ($(TARGET_ARCH), ppc64le)
ifeq ($(CUDA_VERSION), $(filter $(CUDA_VERSION), 9000 9010 9020))
SMS_VOLTA ?= 70
else
ifeq ($(TARGET_OS), darwin)
SMS_VOLTA ?= 70
else
SMS_VOLTA ?= 70 72 75
endif #ifneq ($(TARGET_OS), darwin)
endif #ifeq ($(CUDA_VERSION), $(filter $(CUDA_VERSION), 9000 9010 9020))
else
SMS_VOLTA ?= 70
endif #ifneq ($(TARGET_ARCH), ppc64le)
endif #ifeq ($(CUDA_VERSION),8000 )
SMS ?= 30 35 50 53 60 61 62 $(SMS_VOLTA)
</pre></div>
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_751"><a name="rel_751" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_751" name="rel_751" shape="rect">2.7.&nbsp;cuDNN Release 7.5.1</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"> This is the cuDNN 7.5.1 release notes. This release includes fixes from the previous
                           		cuDNN v7.x.x releases as well as the following additional changes. 
                        </p>
                        <div class="section" id="rel_751__features-enhancements"><a name="rel_751__features-enhancements" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <p class="p">The following features and enhancements have been added to this release:</p><a name="rel_751__ul_cht_2pb_5gb" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_751__ul_cht_2pb_5gb">
                              <li class="li liexpand">The function <samp class="ph codeph">cudnnMultiHeadAttnForward()</samp> is now enabled to sweep through
                                 					all the time steps in a single API call. This is indicated by a negative value
                                 					of the <samp class="ph codeph">currIdx</samp> argument in the inference mode, that is, when
                                 						<samp class="ph codeph">reserveSpace=NULL</samp> so that either
                                 						<samp class="ph codeph">cudnnMultiHeadAttnBackwardData()</samp> or
                                 						<samp class="ph codeph">cudnnMultiHeadAttnBackwardWeights()</samp> will not be invoked.
                                 					This sweep mode can be used to implement self-attention on the encoder side of
                                 					the transformer model. 
                              </li>
                           </ul>
                        </div>
                        <div class="section" id="rel_751__fixed-issues"><a name="rel_751__fixed-issues" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been
                              				fixed in this release:
                           </p><a name="rel_751__ul_akt_jsr_cfb3" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_751__ul_akt_jsr_cfb3">
                              <li class="li liexpand">In cuDNN 7.5.0, using the static link for
                                 						<samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp> function may result
                                 					in <samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> error message. The workaround is
                                 					to perform a whole-archive link. This issue is fixed in cuDNN 7.5.1.
                              </li>
                              <li class="li liexpand">In cuDNN 7.5.0 and 7.4.x, in some cases of input images with large dimensions, the 3D
                                 					forward convolution operations with
                                 						<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp> will cause
                                 					a crash with <samp class="ph codeph">illegal memory access</samp> error. This is fixed in
                                 					cuDNN 7.5.1.
                              </li>
                              <li class="li liexpand">In cuDNN 7.5.0, setting <samp class="ph codeph">attnDropoutDesc=NULL</samp> in
                                 						<samp class="ph codeph">cudnnSetAttnDescriptor()</samp> triggered a segmentation fault in
                                 						<samp class="ph codeph">cudnnMultiHeadAttnForward()</samp>, even though the user is
                                 					required to set it to NULL in the inference mode. This is fixed in cuDNN 7.5.1. 
                              </li>
                           </ul>
                        </div>
                        <div class="section" id="rel_751__known-issues"><a name="rel_751__known-issues" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <p class="p">The following issues and limitations exist in this release:</p><a name="rel_751__ul_lg3_tkx_gfb" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_751__ul_lg3_tkx_gfb">
                              <li class="li liexpand">In cuDNN7.5 and cudnn7.5.1, image size smaller than filter size is unsupported, even with
                                 					sufficient padding.
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_750"><a name="rel_750" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_750" name="rel_750" shape="rect">2.8.&nbsp;cuDNN Release 7.5.0</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"> This is the cuDNN 7.5.0 release notes. This release includes fixes from the previous
                           		cuDNN v7.x.x releases as well as the following additional changes. 
                        </p>
                        <div class="section" id="rel_750__features-enhancements"><a name="rel_750__features-enhancements" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <p class="p">The following features and enhancements have been added to this release:</p><a name="rel_750__ul_cht_2pb_5gb" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_750__ul_cht_2pb_5gb">
                              <li class="li liexpand">In <samp class="ph codeph">cudnnConvolutionForward()</samp> for 2D convolutions, for
                                 						<samp class="ph codeph">wDesc</samp> NCHW, the <samp class="ph codeph">IMPLICIT_GEMM</samp> algorithm
                                 					(algo 0) now supports the data type configuration of
                                 						<samp class="ph codeph">INT8x4_CONFIG</samp> and <samp class="ph codeph">INT8x4_EXT_CONFIG</samp>.
                              </li>
                              <li class="li liexpand">
                                 <p dir="ltr" class="p" id="rel_750__docs-internal-guid-b0cc6091-7fff-57ef-3ee5-c6a38c269e84"><a name="rel_750__docs-internal-guid-b0cc6091-7fff-57ef-3ee5-c6a38c269e84" shape="rect">
                                       <!-- --></a>A new set of APIs is
                                    						added to provide support for multihead attention computation. The following
                                    						is a list of the new functions and data types:
                                 </p>
                                 <div class="p"><strong class="ph b">Datatypes</strong>:<a name="rel_750__ul_xb3_sj5_qjb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel_750__ul_xb3_sj5_qjb">
                                       <li class="li"><samp class="ph codeph">cudnnSeqDataAxis_t</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnMultiHeadAttnWeightKind_t</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnSeqDataDescriptor_t</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnWgradMode_t</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnAttnQueryMap_t</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnAttnDescriptor_t</samp></li>
                                    </ul>
                                 </div>
                                 <div class="p"><strong class="ph b">Functions</strong>:<a name="rel_750__ul_grw_yj5_qjb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel_750__ul_grw_yj5_qjb">
                                       <li class="li"><samp class="ph codeph">cudnnCreateAttnDescriptor</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnDestroyAttnDescriptor</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnSetAttnDescriptor</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnGetAttnDescriptor</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnGetMultiHeadAttnBuffers</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnGetMultiHeadAttnWeights</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnMultiHeadAttnForward</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnMultiHeadAttnBackwardData</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnMultiHeadAttnBackwardWeights</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnSetSeqDataDescriptor</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnGetSeqDataDescriptor</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnCreateSeqDataDescriptor</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnDestroySeqDataDescriptor</samp></li>
                                    </ul>
                                 </div>
                              </li>
                              <li class="li liexpand">
                                 <p dir="ltr" class="p" id="rel_750__docs-internal-guid-92b7bcf9-7fff-5062-811c-70fec680cc87"><a name="rel_750__docs-internal-guid-92b7bcf9-7fff-5062-811c-70fec680cc87" shape="rect">
                                       <!-- --></a>A new set
                                    						of APIs for general tensor folding is introduced. The following is a list of the
                                    						new functions and data types:
                                 </p>
                                 <div class="p"><strong class="ph b">Datatypes</strong>:<a name="rel_750__ul_smb_kk5_qjb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel_750__ul_smb_kk5_qjb">
                                       <li class="li"><samp class="ph codeph">cudnnTensorTransformDescriptor_t</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnFoldingDirection_t</samp></li>
                                    </ul>
                                 </div>
                                 <div class="p"><strong class="ph b">Functions</strong>:<a name="rel_750__ul_t3d_mk5_qjb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel_750__ul_t3d_mk5_qjb">
                                       <li class="li"><samp class="ph codeph">cudnnTransformTensorEx</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnCreateTensorTransformDescriptor</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnDestroyTensorTransformDescriptor</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnInitTransformDest</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnSetTensorTransformDescriptor</samp></li>
                                       <li class="li">cudnnGetTensorTransformDescriptor</li>
                                    </ul>
                                 </div>
                              </li>
                              <li class="li liexpand">
                                 <p class="p">A new set of APIs, and enhancements for the existing APIs, are introduced for RNNs. The
                                    						following is the list of the new and enhanced functions and data types:
                                 </p>
                                 <div class="p"><strong class="ph b">Datatypes</strong>:<a name="rel_750__ul_onk_rk5_qjb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel_750__ul_onk_rk5_qjb">
                                       <li class="li"><samp class="ph codeph">cudnnRNNBiasMode_t</samp> (new)
                                       </li>
                                       <li class="li"><samp class="ph codeph">cudnnRNNMode_t</samp> (enhanced)
                                       </li>
                                    </ul>
                                 </div>
                                 <div class="p"><strong class="ph b">Functions</strong>:<a name="rel_750__ul_jnw_fl5_qjb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel_750__ul_jnw_fl5_qjb">
                                       <li class="li"><samp class="ph codeph">cudnnSetRNNBiasMode</samp> (new)
                                       </li>
                                       <li class="li"><samp class="ph codeph">cudnnGetRNNBiasMode</samp>  (new)
                                       </li>
                                       <li class="li"><samp class="ph codeph">cudnnGetRNNLinLayerBiasParams</samp>  (enhanced)
                                       </li>
                                    </ul>
                                 </div>
                              </li>
                              <li class="li liexpand">All <samp class="ph codeph">cudnnRNNForward/Backward*</samp> functions are enhanced to support FP16 math
                                 					precision mode when both input and output are in FP16. To switch to FP16 math
                                 					precision, set the <samp class="ph codeph">mathPrec</samp> parameter in
                                 						<samp class="ph codeph">cudnnSetRNNDescriptor</samp> to <samp class="ph codeph">CUDNN_DATA_HALF</samp>.
                                 					To switch to FP32 math precision, set the <samp class="ph codeph">mathPrec</samp> parameter in
                                 						<samp class="ph codeph">cudnnSetRNNDescriptor</samp> to <samp class="ph codeph">CUDNN_DATA_FLOAT</samp>.
                                 					This feature is only available for <samp class="ph codeph">CUDNN_ALGO_STANDARD</samp> and for
                                 					the compute capability 5.3 or higher.
                              </li>
                              <li class="li liexpand">Added support for INT8x4 and INT8x32 data type for <samp class="ph codeph">cudnnPoolingForward</samp>.
                                 					Using these will provide improved performance over scalar data type. 
                              </li>
                           </ul>
                        </div>
                        <div class="section" id="rel_750__fixed-issues"><a name="rel_750__fixed-issues" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been fixed in this release:</p><a name="rel_750__ul_akt_jsr_cfb3" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_750__ul_akt_jsr_cfb3">
                              <li class="li liexpand">When the following is true for the <samp class="ph codeph">cudnnConvolutionBackwardData()</samp> function:
                                 						<a name="rel_750__ul_fjw_vgj_zgb" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_750__ul_fjw_vgj_zgb">
                                    <li class="li liexpand">used with <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING</samp>,
                                       							and 
                                    </li>
                                    <li class="li liexpand"><samp class="ph codeph">convDesc</samp>'s vertical stride is exactly 2, and 
                                    </li>
                                    <li class="li liexpand">the vertical padding is a multiple of 2, and </li>
                                    <li class="li liexpand">the filter height is a multiple of 2 </li>
                                 </ul>
                                 
                                 or<a name="rel_750__ul_gnh_zgj_zgb" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_750__ul_gnh_zgj_zgb">
                                    <li class="li liexpand">used with <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING</samp>,
                                       							and 
                                    </li>
                                    <li class="li liexpand"><samp class="ph codeph">convDesc</samp>'s horizontal stride is exactly 2, and 
                                    </li>
                                    <li class="li liexpand">the horizontal padding is a multiple of 2, and </li>
                                    <li class="li liexpand">the filter width is a multiple of 2 </li>
                                 </ul>
                                 <p class="p">then the resulting output is incorrect. This issue was present in cuDNN 7.3.1
                                    						and later. This is fixed in cuDNN 7.5.0.
                                 </p>
                              </li>
                              <li class="li liexpand">The <samp class="ph codeph">mathPrec</samp> parameter in <samp class="ph codeph">cudnnSetRNNDescriptor</samp> is
                                 					reserved for controlling math precision in RNN, but was not checked or enforced. This
                                 					parameter is now strictly enforced. As a result, the following applies: <a name="rel_750__ul_ixm_xzp_5gb" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_750__ul_ixm_xzp_5gb">
                                    <li class="li liexpand">For the I/O in FP16, the parameter <samp class="ph codeph">mathPrec</samp> can be
                                       								<samp class="ph codeph">CUDNN_DATA_HALF</samp> or
                                       								<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>. 
                                    </li>
                                    <li class="li liexpand">For the I/O in FP32, the parameter <samp class="ph codeph">mathPrec</samp> can only be
                                       								<samp class="ph codeph">CUDNN_DATA_FLOAT</samp>.
                                    </li>
                                    <li class="li liexpand">For the I/O in FP64, double type, the parameter <samp class="ph codeph">mathPrec</samp> can only be
                                       								<samp class="ph codeph">CUDNN_DATA_DOUBLE</samp>.
                                    </li>
                                 </ul>
                              </li>
                              <li class="li liexpand">Users upgrading to cuDNN 7.4 may see insufficiently small values returned from the
                                 					function <samp class="ph codeph">cudnnGetConvolutionBackwardFilterWorkspaceSize ()</samp> for
                                 					dimensions 5 and greater, resulting in a
                                 						<samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp> error message. In cuDNN 7.4,
                                 					the workaround for this issue is to calculate the workspace by using the formula
                                 					below:<pre xml:space="preserve">Let M be the product of output tensor (gradDesc) dimensions starting at 1.
Let N be the output tensor dimension 0.
Let Mp = (M+31)/32
Let Np = (N+31)/32
W = 2 * M * N * sizeof(int) is the workspace that should be used.</pre>This
                                 					is fixed. </li>
                              <li class="li liexpand">In earlier cuDNN versions, when all the conditions below are true: <a name="rel_750__ul_tym_p5s_vgb" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_750__ul_tym_p5s_vgb">
                                    <li class="li liexpand">3D convolution </li>
                                    <li class="li liexpand">Batch size &gt; 1 </li>
                                    <li class="li liexpand">Algorithm is <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1</samp></li>
                                    <li class="li liexpand"><samp class="ph codeph">convDesc</samp>'s <samp class="ph codeph">dataType</samp> is <samp class="ph codeph">CUDNN_DATA_HALF</samp>,
                                       							then, calls to ​​<samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> may
                                       							produce incorrect (and non-deterministic) results. This is fixed in
                                       							cuDNN 7.5.0.
                                    </li>
                                 </ul>
                              </li>
                              <li class="li liexpand">In cuDNN 7.4.2, for some cases the 3D convolution resulted in a reduced performance on
                                 					NVIDIA Turing GPUs, compared to the previous cuDNN releases. This is fixed.
                              </li>
                              <li class="li liexpand">For <samp class="ph codeph">int8x32</samp> datatype, the function
                                 						<samp class="ph codeph">cudnnSetTensor4dDescriptorEx</samp> erroneously returns
                                 						<samp class="ph codeph">CUDNN_STATUS_BAD_PARAM</samp>. Now it is fixed in cuDNN 7.5 so it
                                 					no longer returns bad param.
                              </li>
                              <li class="li liexpand">
                                 <p class="p">In cuDNN 7.4.1 and 7.4.2, when <samp class="ph codeph">cudnnBatchNormMode_t</samp> is set to
                                    							<samp class="ph codeph">CUDNN_BATCHNORM_SPATIAL_PERSISTENT</samp> and the I/O tensors
                                    						are in NHWC format and of <samp class="ph codeph">CUDNN_DATA_HALF</samp> datatype, then,
                                    						on Windows only, the <samp class="ph codeph">cudnnBatchNormalization*Ex</samp> functions
                                    						are supported only with the device in TCC mode. Refer to <a class="xref" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#tesla-compute-cluster-mode-for-windows" target="_blank" shape="rect">Tesla Compute Cluster Mode for
                                       							Windows</a> for more information.
                                 </p>
                                 <p dir="ltr" class="p" id="rel_750__docs-internal-guid-7f0ff5a1-7fff-b8ab-9a21-0c07d2c9b276"><a name="rel_750__docs-internal-guid-7f0ff5a1-7fff-b8ab-9a21-0c07d2c9b276" shape="rect">
                                       <!-- --></a>Starting
                                    						with cuDNN 7.5.0, the following checks are added for the driver mode on Windows.
                                    						If on Windows and not in TCC mode:
                                 </p><a name="rel_750__ul_b45_gbq_5gb" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_750__ul_b45_gbq_5gb">
                                    <li class="li liexpand">The functions fallback to a slower implementation if <samp class="ph codeph">bnOps</samp> in the
                                       								<samp class="ph codeph">cudnnBatchNormalization*Ex</samp> function is set to
                                       								<samp class="ph codeph">CUDNN_BATCHNORM_OPS_BN</samp>.
                                    </li>
                                    <li class="li liexpand">If <samp class="ph codeph">bnOps</samp> is set to <samp class="ph codeph">CUDNN_BATCHNORM_OPS_BN_ACTIVATION</samp>, or
                                       								<samp class="ph codeph">CUDNN_BATCHNORM_OPS_BN_ADD_ACTIVATION</samp>, the
                                       								<samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> is returned. 
                                    </li>
                                 </ul>
                              </li>
                              <li class="li liexpand">In cuDNN 7.4.2, in some cases the <samp class="ph codeph">cudnnConvolutionBackwardData()</samp> function,
                                 					when used with NHWC tensor format, resulted in the <samp class="ph codeph">disallowed
                                    						mismatches</samp> error. This is fixed.
                              </li>
                              <li class="li liexpand">In some cases, using <samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp> with
                                 						<samp class="ph codeph">GroupCount() &gt; 1</samp> and <samp class="ph codeph">xDesc</samp>'s data type is
                                 						<samp class="ph codeph">CUDNN_DATA_HALF</samp> will produce incorrect results for all
                                 					groups except the first. This is fixed.
                              </li>
                              <li class="li liexpand">When using cuDNN 7.3.1 on Quadro P4000, when calling the
                                 						<samp class="ph codeph">cudnnConvolutionForward()</samp> function with
                                 						<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED</samp> algorithm,
                                 					there was a small chance of seeing intermittent inaccurate results. This is
                                 					fixed.
                              </li>
                              <li class="li liexpand">When <samp class="ph codeph">cudnnConvolutionForward()</samp> is called with these settings: <a name="rel_750__ul_vyw_1cw_4tb" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_750__ul_vyw_1cw_4tb">
                                    <li class="li">The datatype is <samp class="ph codeph">CUDNN_DATA_INT8x4</samp>. 
                                    </li>
                                    <li class="li">The convolution is 2D.</li>
                                    <li class="li">The architecture is <samp class="ph codeph">sm_61</samp>.
                                    </li>
                                    <li class="li">The filter size is larger than 8x8.</li>
                                 </ul>
                                 
                                 Then, incorrect results and potential illegal memory access errors occur.
                                 					This is fixed. 
                              </li>
                              <li class="li liexpand">For <samp class="ph codeph">sm_72</samp> and <samp class="ph codeph">sm_75</samp>, the function
                                 						<samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp>, when used with
                                 					INT8x32, failed to run. This is fixed.
                              </li>
                              <li class="li liexpand">In the function <samp class="ph codeph">cudnnSetRNNDataDescriptor</samp> , if API logging is turned
                                 					on, the <samp class="ph codeph">seqLengthArray</samp> field in the log may not display the correct
                                 					number of array elements. This is fixed. 
                              </li>
                              <li class="li liexpand">For the batchNorm functions
                                 						<samp class="ph codeph">cudnnBatchNormalization{Backward|BackwardEx|ForwardInference|ForwardTraining|ForwardTrainingEx}</samp>,
                                 					the value of <samp class="ph codeph">epsilon</samp> is required to be greater or equal to
                                 						<samp class="ph codeph">CUDNN_BN_MIN_EPSILON</samp> that was defined in the
                                 						<samp class="ph codeph">cudnn.h</samp> file to the value 1e-5. This threshold value is now
                                 					lowered to 0.0 to allow a wider range of <samp class="ph codeph">epsilon</samp> value.
                                 					However, users should still choose the <samp class="ph codeph">epsilon</samp> value carefully,
                                 					since a too small a value of <samp class="ph codeph">epsilon</samp> may cause
                                 					batchNormalization to overflow when the input data's standard deviation is close
                                 					to 0. 
                              </li>
                              <li class="li liexpand">Some grouped convolutions (particularly those used in depthwise-separable convolutions) may
                                 					return <pre xml:space="preserve">INTERNAL_ERROR</pre> if they have all inputs/outputs as
                                 					NHWC-packed and do not match one of the following criteria: <a name="rel_750__ul_lsv_pgq_5gb" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_750__ul_lsv_pgq_5gb">
                                    <li class="li liexpand"><samp class="ph codeph">filter_height = 1, filter_width = 1, vertical_conv_stride = 1,
                                          								horizontal_conv_stride = 1</samp></li>
                                    <li class="li liexpand"><samp class="ph codeph">filter_height = 3, filter_width = 3, vertical_conv_stride = 1,
                                          								horizontal_conv_stride = 1</samp></li>
                                    <li class="li liexpand"><samp class="ph codeph">filter_height = 3, filter_width = 3, vertical_conv_stride = 2,
                                          								horizontal_conv_stride = 2</samp></li>
                                 </ul>
                              </li>
                           </ul>
                        </div>
                        <div class="section" id="rel_750__known-issues"><a name="rel_750__known-issues" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues</h3>
                           <p class="p">The following issues and limitations exist in this release:</p><a name="rel_750__ul_lg3_tkx_gfb" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_750__ul_lg3_tkx_gfb">
                              <li class="li liexpand">The RNN persist-static algorithm returns incorrect results for GRU problems in backwards
                                 					mode, when the hidden size is greater than 1024. Due to this, RNN persist-static
                                 					algorithm is disabled in cuDNN 7.5.0. Users with such GRU problems are advised
                                 					to use the standard or persist-dynamic RNN algorithms. See
                                 						<samp class="ph codeph">cudnnRNNAlgo_t</samp>. This note applies to all previous cuDNN 7
                                 					releases.
                              </li>
                              <li class="li liexpand">The function <samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp>, when used with
                                 						<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1</samp>, returns the error
                                 						<samp class="ph codeph">"Uninitialized __global__ memory read of size 4".</samp></li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_742"><a name="rel_742" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_742" name="rel_742" shape="rect">2.9.&nbsp;cuDNN Release 7.4.2</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"> This is the cuDNN 7.4.2 release notes. This release includes fixes from the previous
                           		cuDNN v7.x.x releases as well as the following additional changes. 
                        </p>
                        <div class="section">
                           <h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been fixed in this release:</p><a name="rel_742__ul_akt_jsr_cfb3" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_742__ul_akt_jsr_cfb3">
                              <li class="li">In some cases when the data is in <samp class="ph codeph">CUDNN_DATA_HALF</samp> and NHWC,
                                 					illegal memory access may occur for <samp class="ph codeph">cudnnBatchNormalization*</samp>
                                 					functions in the cuDNN 7.4.1 library. This is now fixed. 
                              </li>
                              <li class="li">When the data is in <samp class="ph codeph">CUDNN_DATA_HALF</samp> and NHWC, for
                                 						<samp class="ph codeph">cudnnBatchNormalization*</samp> functions when (N*H*W) is large
                                 					and odd number, the output may contain wrong results. This is fixed.
                              </li>
                              <li class="li">When calling the <samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp> function with the
                                 						<samp class="ph codeph">algo</samp> parameter set to
                                 						<samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_FFT</samp> and the
                                 						<samp class="ph codeph">activationDesc</samp> parameter set to
                                 						<samp class="ph codeph">CUDNN_ACTIVATION_RELU</samp> and sufficiently large inputs, the
                                 					ReLU operation is not applied and negative values are passed through to the
                                 					output. This issue is now fixed. This issue was present in all previous cuDNN
                                 					versions. 
                              </li>
                              <li class="li">Performance regression was introduced in cuDNN 7.4.1 for
                                 						<samp class="ph codeph">cudnnConvolutionBwdFilterAlgo_t()</samp> function with
                                 						<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1</samp> algorithm. This is
                                 					fixed.
                              </li>
                           </ul>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Known Issues</h3>
                           <p class="p">The following issues and limitations exist in this release:</p><a name="rel_742__ul_lg3_tkx_gfb" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_742__ul_lg3_tkx_gfb">
                              <li class="li">When <samp class="ph codeph">cudnnBatchNormMode_t</samp> is set to
                                 						<samp class="ph codeph">CUDNN_BATCHNORM_SPATIAL_PERSISTENT</samp> and the I/O tensors are
                                 					in NHWC format and of <samp class="ph codeph">CUDNN_DATA_HALF</samp> datatype, then, <strong class="ph b">on
                                    						Windows only</strong>, the <samp class="ph codeph">cudnnBatchNormalization*Ex</samp> functions
                                 					are supported only with the device in TCC mode. See <a class="xref" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#tesla-compute-cluster-mode-for-windows" target="_blank" shape="rect">Tesla Compute Cluster Mode for
                                    					Windows</a>. This issue is not present on Linux systems. This issue is
                                 					present in cuDNN 7.4.1 and this current version. 
                              </li>
                              <li class="li">
                                 <p class="p">In some cases, the 3D convolution will have a reduced performance on NVIDIA Turing GPUs,
                                    						compared to the previous cuDNN releases.
                                 </p>
                              </li>
                              <li class="li">
                                 <p class="p">The functions <samp class="ph codeph">cudnnGetConvolutionForwardAlgorithm_v7()</samp> and
                                    							<samp class="ph codeph">cudnnGetConvolutionForwardWorkspaceSize()</samp> will return
                                    							<samp class="ph codeph">CUDNN_STATUS_SUCCESS</samp>, but the execution of the
                                    						convolution returns <samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp>. This issue
                                    						is present in cuDNN 7.2.2 library and later versions.
                                 </p>
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_741"><a name="rel_741" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_741" name="rel_741" shape="rect">2.10.&nbsp;cuDNN Release 7.4.1</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">This is the cuDNN 7.4.1 release notes. This release includes fixes from the previous
                           cuDNN v7.x.x releases as well as the following additional changes.  
                        </p>
                        <div class="section" id="rel_741__section_ppp_blx_gfb"><a name="rel_741__section_ppp_blx_gfb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <p class="p">The following enhancements have been added to this release:</p><a name="rel_741__ul_qpp_blx_gfb" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_741__ul_qpp_blx_gfb">
                              <li class="li">
                                 <p dir="ltr" class="p" id="rel_741__docs-internal-guid-65c83366-7fff-4ea4-a796-96450cf9d247"><a name="rel_741__docs-internal-guid-65c83366-7fff-4ea4-a796-96450cf9d247" shape="rect">
                                       <!-- --></a>Added
                                    a new family of fast NHWC batch normalization functions. Refer to the
                                    following five new functions and one new type descriptor:
                                 </p><a name="rel_741__ul_ols_dsr_qfb" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_741__ul_ols_dsr_qfb">
                                    <li class="li">
                                       <p class="p"><samp class="ph codeph">cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize()</samp>
                                          function
                                       </p>
                                    </li>
                                    <li class="li">
                                       <p class="p"><samp class="ph codeph">cudnnBatchNormalizationForwardTrainingEx</samp>
                                          function
                                       </p>
                                    </li>
                                    <li class="li">
                                       <p class="p"><samp class="ph codeph">cudnnGetBatchNormalizationBackwardExWorkspaceSize()</samp>
                                          function
                                       </p>
                                    </li>
                                    <li class="li">
                                       <p class="p"><samp class="ph codeph">cudnnBatchNormalizationBackwardEx()</samp>  function
                                       </p>
                                    </li>
                                    <li class="li">
                                       <p class="p"><samp class="ph codeph">cudnnGetBatchNormalizationTrainingExReserveSpaceSize()</samp>
                                          function
                                       </p>
                                    </li>
                                    <li class="li">
                                       <p class="p"><samp class="ph codeph">cudnnBatchNormOps_t</samp> type descriptor
                                       </p>
                                    </li>
                                 </ul>
                              </li>
                              <li class="li">For API Logging, a conversion specifier for the process id is added. With this,
                                 the process id can be included in the log file name. Refer to  <a class="xref" href="https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#api-logging" target="_blank" shape="rect">API Logging</a> for more information.
                              </li>
                              <li class="li">Performance of <samp class="ph codeph">cudnnPoolingBackward()</samp> is enhanced for the
                                 average pooling when using NHWC data format-for both the
                                 <samp class="ph codeph">CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING</samp> and
                                 <samp class="ph codeph">CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING</samp> cases of
                                 <samp class="ph codeph">cudnnPoolingMode_t</samp>.
                              </li>
                              <li class="li">Performance of the strided convolution in
                                 <samp class="ph codeph">cudnnConvolutionBackwardData()</samp> is enhanced when the filter
                                 is in NHWC format and the data type is <samp class="ph codeph">TRUE_HALF_CONFIG</samp>,
                                 <samp class="ph codeph">PSEUDO_HALF_CONFIG</samp>, or <samp class="ph codeph">FLOAT_CONFIG</samp>. For
                                 strides <samp class="ph codeph">u,v &lt; r,s</samp> the performance is further enhanced. 
                              </li>
                              <li class="li">Significantly improved the performance of
                                 <samp class="ph codeph">cudnnConvolutionForward()</samp>,
                                 <samp class="ph codeph">cudnnConvolutionBackwardData()</samp>, and
                                 <samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> functions on RCNN models
                                 such as Fast RCNN, Faster RCNN, and Mask RCNN. 
                              </li>
                           </ul>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been fixed in this release:</p><a name="rel_741__ul_akt_jsr_cfb3" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_741__ul_akt_jsr_cfb3">
                              <li class="li">The following set-up was giving <samp class="ph codeph">Misaligned Address</samp> error in
                                 cuDNN 7.3.x. This is fixed in cuDNN 7.4.1: For the
                                 <samp class="ph codeph">cudnnConvolutionForward()</samp> function with the
                                 <samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp> algorithm,
                                 in the data type configuration of <samp class="ph codeph">PSEUDO_HALF_CONFIG</samp>, when the
                                 input and output tensors are in NHWC and the filter is 1x1 and NCHW, and Tensor
                                 Op is enabled. 
                              </li>
                              <li class="li">For a few convolution sizes for <samp class="ph codeph">ALGO_0</samp> and
                                 <samp class="ph codeph">ALGO_1</samp>, the performance of the function
                                 <samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> was degraded in cuDNN
                                 7.3.1. This is now fixed. 
                              </li>
                              <li class="li">Fixed. In cuDNN 7.3.1, the function <samp class="ph codeph">cudnnAddTensor</samp> was
                                 computing incorrect results when run on GPUs with the compute capability &lt;
                                 6.0 (before NVIDIA Pascal). 
                              </li>
                           </ul>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Known Issues</h3>
                           <p class="p">The following issues and limitations exist in this release:</p><a name="rel_741__ul_lg3_tkx_gfb" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_741__ul_lg3_tkx_gfb">
                              <li class="li">When calling the <samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp>
                                 function with the <samp class="ph codeph">algo</samp> parameter set to
                                 <samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_FFT</samp> and the
                                 <samp class="ph codeph">activationDesc</samp> parameter set to
                                 <samp class="ph codeph">CUDNN_ACTIVATION_RELU</samp> and sufficiently large inputs, the
                                 ReLU operation is not applied and negative values are passed through to the
                                 output. This issue is present in all previous cuDNN versions. 
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_731"><a name="rel_731" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_731" name="rel_731" shape="rect">2.11.&nbsp;cuDNN Release 7.3.1</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">This is the cuDNN 7.3.1 release notes. This release includes fixes from the previous
                           cuDNN v7.x.x releases as well as the following additional changes.  
                        </p>
                        <div class="section" id="rel_731__section_ppp_blx_gfb"><a name="rel_731__section_ppp_blx_gfb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <p class="p">The following enhancements have been added to this release:</p><a name="rel_731__ul_qpp_blx_gfb" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_731__ul_qpp_blx_gfb">
                              <li class="li">The FFT tiling algorithms for convolution have been enhanced to support strided
                                 convolution. In specific, for the algorithms
                                 <samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING</samp> and
                                 <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING</samp>, the
                                 <samp class="ph codeph">convDesc</samp>'s vertical and horizontal filter stride can be 2
                                 when neither the filter width nor the filter height is 1.
                              </li>
                              <li class="li">The <samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD</samp> algorithm for
                                 <samp class="ph codeph">cudnnConvolutionForward()</samp> and
                                 <samp class="ph codeph">cudnnConvolutionBackwardData()</samp> now give superior
                                 performance for NVIDIA Volta architecture. In addition, the mobile version of
                                 this algorithm in the same functions gives superior performance for Maxwell and
                                 NVIDIA Pascal architectures. 
                              </li>
                              <li class="li">Dilated convolutions now give superior performance for
                                 <samp class="ph codeph">cudnnConvolutionForward()</samp>,
                                 <samp class="ph codeph">cudnnConvolutionBackwardData()</samp>, and
                                 <samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> on NVIDIA Volta
                                 architecture, in some cases.
                              </li>
                           </ul>
                        </div>
                        <div class="section" id="rel_731__section_kg3_tkx_gfb"><a name="rel_731__section_kg3_tkx_gfb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle">Known Issues and Limitations</h3>
                           <p class="p">The following issues and limitations exist in this release:</p><a name="rel_731__ul_lg3_tkx_gfb" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_731__ul_lg3_tkx_gfb">
                              <li class="li">For the <samp class="ph codeph">cudnnConvolutionForward()</samp>, when using a 1x1 filter with
                                 input and output tensors of <samp class="ph codeph">NHWC</samp> format and of
                                 <samp class="ph codeph">CUDNN_DATA_HALF</samp> (half precision) type, and the filter
                                 format is <samp class="ph codeph">NCHW</samp>, with compute type of float, cuDNN will generate
                                 incorrect results.
                              </li>
                              <li class="li">On Quadro P4000, when calling <samp class="ph codeph">cudnnConvolutionForward()</samp>
                                 function with <samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED</samp>
                                 algorithm, there may be a small chance of seeing intermittent inaccurate
                                 results.
                              </li>
                              <li class="li">When using <samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> with
                                 <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0</samp> in mixed precision
                                 computation, with I/O in <samp class="ph codeph">CUDNN_DATA_HALF</samp> (half precision) and
                                 compute type of float, when the number of batches (N) is larger than 1 the
                                 results might include INF due to an intermediate down convert to half float. In
                                 other words, with an accumulation of float for all intermediate values (such as
                                 in <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1</samp>) the result will be a
                                 finite half precision float. This limitation also exists in all previous cuDNN
                                 versions.
                              </li>
                           </ul>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been fixed in this release:</p><a name="rel_731__ul_akt_jsr_cfb3" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_731__ul_akt_jsr_cfb3">
                              <li class="li">Fixed a pointer arithmetic integer overflow issue in RNN forward and backward
                                 functions, when sequence length and mini-batch size are sufficiently large.
                              </li>
                              <li class="li">When tensor cores are enabled in cuDNN 7.3.0, the
                                 <samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> calculations were
                                 performing an illegal memory access when K and C values are both non-integral
                                 multiples of 8. This issue is fixed.
                              </li>
                              <li class="li">For the <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1</samp> algorithm in
                                 <samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp>, on NVIDIA Volta, the
                                 tensor operations were occasionally failing when the filter-spatial size (filter
                                 <samp class="ph codeph">h</samp> * filter <samp class="ph codeph">w</samp>) was greater than 64. This
                                 issue is fixed. 
                              </li>
                              <li class="li">While running cuDNN 7.3.0 on NVIDIA Turing with CUDA 10.0, r400 driver, the
                                 functions <samp class="ph codeph">cudnnRNNForwardTraining(Ex)</samp> and
                                 <samp class="ph codeph">cudnnRNNForwardInference(Ex)</samp> errored out returning
                                 <samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp>. This issue is fixed. 
                              </li>
                              <li class="li">In cuDNN 7.3.0, when using <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1</samp>
                                 with tensor data or filter data in <samp class="ph codeph">NHWC</samp> format, the function
                                 might have resulted in a silent failure. This is now fixed. 
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_730"><a name="rel_730" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_730" name="rel_730" shape="rect">2.12.&nbsp;cuDNN Release 7.3.0</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"> This is the cuDNN 7.3.0release notes. This release includes fixes from the previous
                           cuDNN v7.x.x releases as well as the following additional changes. 
                        </p>
                        <div class="section">
                           <h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <p class="p">The following enhancements have been added to this release:</p>
                           <div class="p"><a name="rel_730__ul_q5j_44r_cfb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel_730__ul_q5j_44r_cfb">
                                 <li class="li">Support is added to the following for the dilated convolution, for
                                    <samp class="ph codeph">NCHW</samp> and <samp class="ph codeph">NHWC</samp> filter formats:<a name="rel_730__ul_rtg_r4r_cfb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel_730__ul_rtg_r4r_cfb">
                                       <li class="li"><samp class="ph codeph">cudnnConvolutionForward()</samp> for 2D
                                       </li>
                                       <li class="li"><samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnConvolutionBackwardData()</samp> for 2D
                                       </li>
                                       <li class="li"><samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp></li>
                                       <li class="li"><samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> for 2D
                                       </li>
                                       <li class="li"><samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1 </samp></li>
                                    </ul>
                                    <p class="p">For these supported cases, the dilated convolution is expected to
                                       offer superior speed, compared to the existing dilated convolution with
                                       algo 0.
                                    </p>
                                 </li>
                                 <li class="li">Grouped convolutions for depth-wise separable convolutions are optimized for
                                    the following NHWC formats: HHH (input: Half, compute: Half, output: Half),
                                    HSH, and SSS. 
                                 </li>
                                 <li class="li">While using <samp class="ph codeph">CUDNN_TENSOR_OP_MATH</samp> or
                                    <samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp>, with the tensor
                                    cores, the <samp class="ph codeph">c</samp>, and <samp class="ph codeph">k</samp> dimensions of the
                                    tensors are now padded to multiples of 8 (as needed), to allow a tensor core
                                    kernel to run.
                                 </li>
                                 <li class="li">The <samp class="ph codeph">CUDNN_BATCHNORM_SPATIAL_PERSISTENT</samp> algo is enhanced in
                                    <samp class="ph codeph">cudnnBatchNormalizationForwardTraining()</samp> and
                                    <samp class="ph codeph">cudnnBatchNormalizationBackward()</samp> to propagate NaN-s or
                                    Inf-s as in a pure floating point implementation (the "persistent" flavor of
                                    the batch normalization is optimized for speed and it uses integer atomics
                                    for inter thread-block reductions). In earlier versions of cuDNN, we
                                    recommended invoking <samp class="ph codeph">cudnnQueryRuntimeError()</samp> to ensure
                                    that no overflow was encountered. When it happened, the best practice was to
                                    discard the results, and use <samp class="ph codeph">CUDNN_BATCHNORM_SPATIAL</samp>
                                    instead, as some results generated by
                                    <samp class="ph codeph">CUDNN_BATCHNORM_SPATIAL_PERSISTENT</samp> could be finite but
                                    invalid. This behavior is now corrected: NaN-s and Inf-s are consistently
                                    output when intermediate results are out of range. The refined
                                    implementation simulates math operations on special floating point values,
                                    for example, <samp class="ph codeph">+Inf-Inf=NaN</samp>. 
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Known Issues and Limitations</h3>
                           <p class="p">Following issues and limitations exist in this release:</p>
                           <ul class="ul">
                              <li class="li">When tensor cores are enabled in cuDNN 7.3.0, the wgrad calculations will
                                 perform an illegal memory access when K and C values are both non-integral
                                 multiples of 8. This will not likely produce incorrect results, but may corrupt
                                 other memory depending on the user buffer locations. This issue is present on
                                 NVIDIA Volta and NVIDIA Turing architectures.
                              </li>
                              <li class="li">Using <samp class="ph codeph">cudnnGetConvolution*_v7</samp> routines with
                                 <samp class="ph codeph">cudnnConvolutionDescriptor_t</samp> set to
                                 <samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> leads to incorrect
                                 outputs. These incorrect outputs will consist only of
                                 <samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> cases, instead of
                                 also returning the performance results for both <samp class="ph codeph">DEFAULT_MATH</samp>
                                 and <samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> cases.
                              </li>
                           </ul>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been fixed in this release:</p><a name="rel_730__ul_akt_jsr_cfb3" shape="rect">
                              <!-- --></a><ul class="ul" id="rel_730__ul_akt_jsr_cfb3">
                              <li class="li"> Using <samp class="ph codeph">cudnnConvolutionBackwardData()</samp> with
                                 <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD</samp> algorithm produced
                                 incorrect results due to an incorrect filter transform. This issue was present
                                 in cuDNN 7.2.1.
                              </li>
                              <li class="li">For INT8 type, with <samp class="ph codeph">xDesc</samp> and <samp class="ph codeph">yDesc</samp> of NHWC
                                 format, the <samp class="ph codeph">cudnnGetConvolutionForwardAlgorithm_v7</samp> function was
                                 incorrectly returning <samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM</samp>
                                 as a valid algorithm. This is fixed. 
                              </li>
                              <li class="li"><samp class="ph codeph">cudnnConvolutionForward()</samp> using
                                 <samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD</samp> intermittently produced
                                 incorrect results in cuDNN 7.2, due to a race condition. This issue is
                                 fixed.
                              </li>
                              <li class="li">When running <samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> with NHWC filter
                                 format, when <samp class="ph codeph">n</samp>, <samp class="ph codeph">c</samp>, and <samp class="ph codeph">k</samp> are
                                 all multiple of 8, and when the <samp class="ph codeph">workSpace</samp> input is exactly as
                                 indicated by <samp class="ph codeph">cudnnGetConvolutionBackwardFilterWorkspaceSize()</samp>,
                                 leads to error in cuDNN 7.2. This is fixed. 
                              </li>
                              <li class="li">When the user runs <samp class="ph codeph">cudnnRNNForward</samp>* or
                                 <samp class="ph codeph">cudnnRNNBackward</samp>* with FP32 I/O on sm_70 or sm_72, with RNN
                                 descriptor's <samp class="ph codeph">algo</samp> field set to
                                 <samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_STATIC</samp>, and
                                 <samp class="ph codeph">cudnnMathType_t</samp> type set to
                                 <samp class="ph codeph">CUDNN_TENSOR_OP_MATH</samp> using
                                 <samp class="ph codeph">cudnnSetRNNMatrixMathType</samp>, then the results were incorrect.
                                 This is fixed.
                              </li>
                              <li class="li">When the user runs <samp class="ph codeph">cudnnRNNForward</samp>* or
                                 <samp class="ph codeph">cudnnRNNBackward</samp>* with FP32 I/O on sm_70 or sm_72, with RNN
                                 descriptor's <samp class="ph codeph">algo</samp> field set to
                                 <samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_STATIC</samp>, and
                                 <samp class="ph codeph">cudnnMathType_t</samp> type set to
                                 <samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> using
                                 <samp class="ph codeph">cudnnSetRNNMatrixMathType</samp>, then the resulting performance
                                 was suboptimal. This is fixed.
                              </li>
                              <li class="li">Convolution routines with filter format as NHWC require both input and output
                                 formats to be NHWC. However, in cuDNN 7.2 and earlier, this condition was not
                                 being checked for, as a result of which silent failures may have occurred. This
                                 is fixed in 7.3.0 to correctly return
                                 <samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp>.
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_721"><a name="rel_721" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_721" name="rel_721" shape="rect">2.13.&nbsp;cuDNN Release 7.2.1</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"> This is the cuDNN 7.2.1 release notes. This release includes fixes from the previous
                           cuDNN v7.x.x releases as well as the following additional changes. 
                        </p>
                        <div class="section">
                           <h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <p class="p">The following enhancements have been added to this release:</p>
                           <div class="p"><a name="rel_721__ul_d5f_1gt_m2b" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel_721__ul_d5f_1gt_m2b">
                                 <li class="li">The following new functions are added to provide support for the padding
                                    mask for the <samp class="ph codeph">cudnnRNN*</samp> family of functions: <a name="rel_721__ul_dxy_ggt_m2b" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel_721__ul_dxy_ggt_m2b">
                                       <li class="li"><samp class="ph codeph">cudnnSetRNNPaddingMode()</samp>: Enables/disables the
                                          padded RNN I/O. 
                                       </li>
                                       <li class="li"><samp class="ph codeph">cudnnGetRNNPaddingMode()</samp>: Reads the padding mode
                                          status. 
                                       </li>
                                       <li class="li"><samp class="ph codeph">cudnnCreateRNNDataDescriptor()</samp> and
                                          <samp class="ph codeph">cudnnDestroyRNNDataDescriptor()</samp>: Creates and
                                          destroys, respectively, <samp class="ph codeph">cudnnRNNDataDescriptor_t</samp>,
                                          an RNN data descriptor. 
                                       </li>
                                       <li class="li"><samp class="ph codeph">cudnnSetRNNDataDescriptor()</samp> and
                                          <samp class="ph codeph">cudnnGetRNNDataDescriptor()</samp>: Initializes and
                                          reads, respectively, the RNN data descriptor. 
                                       </li>
                                       <li class="li"><samp class="ph codeph">cudnnRNNForwardTrainingEx()</samp>: An extended version of
                                          the <samp class="ph codeph">cudnnRNNForwardTraining()</samp> to allow for the
                                          padded (unpacked) layout for the I/O. 
                                       </li>
                                       <li class="li"><samp class="ph codeph">cudnnRNNForwardInferenceEx()</samp>: An extended version
                                          of the <samp class="ph codeph">cudnnRNNForwardInference()</samp> to allow for the
                                          padded (unpacked) layout for the I/O. 
                                       </li>
                                       <li class="li"><samp class="ph codeph">cudnnRNNBackwardDataEx()</samp>: An extended version of
                                          the <samp class="ph codeph">cudnnRNNBackwardData()</samp> to allow for the padded
                                          (unpacked) layout for the I/O. 
                                       </li>
                                       <li class="li"><samp class="ph codeph">cudnnRNNBackwardWeightsEx()</samp>: An extended version of
                                          the <samp class="ph codeph">cudnnRNNBackwardWeights()</samp> to allow for the
                                          padded (unpacked) layout for the I/O. 
                                       </li>
                                    </ul>
                                 </li>
                                 <li class="li">
                                    <p class="p">Added support for cell clipping in cuDNN LSTM. The following new
                                       functions are added: 
                                    </p><a name="rel_721__ul_nwr_ngt_m2b" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel_721__ul_nwr_ngt_m2b">
                                       <li class="li"><samp class="ph codeph">cudnnRNNSetClip()</samp> and
                                          <samp class="ph codeph">cudnnRNNGetClip()</samp>: Sets and retrieves,
                                          respectively, the LSTM cell clipping mode. 
                                       </li>
                                    </ul>
                                 </li>
                                 <li class="li">Accelerate your convolution computation with this new feature: When the
                                    input channel size <samp class="ph codeph">c</samp> is a multiple of 32, you can use the
                                    new data type <samp class="ph codeph">CUDNN_DATA_INT8x32</samp> to accelerate your
                                    convolution computation. 
                                    <div class="note note"><span class="notetitle">Note:</span> This new data type
                                       <samp class="ph codeph">CUDNN_DATA_INT8x32</samp> is only supported by
                                       sm_72.
                                    </div>
                                 </li>
                                 <li class="li">Enhanced the family of <samp class="ph codeph">cudnnFindRNN*</samp> functions. The
                                    <samp class="ph codeph">findIntensity</samp> input to these functions now enables the
                                    user to control the overall runtime of the RNN find algorithms, by selecting
                                    a percentage of a large Cartesian product space to be searched. 
                                 </li>
                                 <li class="li">A new mode <samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> is added
                                    to <samp class="ph codeph">cudnnMathType_t</samp>. The computation time for FP32 tensors
                                    can be reduced by selecting this mode. 
                                 </li>
                                 <li class="li">The functions <samp class="ph codeph">cudnnRNNForwardInference()</samp>,
                                    <samp class="ph codeph">cudnnRNNForwardTraining()</samp>,
                                    <samp class="ph codeph">cudnnRNNBackwardData()</samp>, and
                                    <samp class="ph codeph">cudnnRNNBackwardWeights()</samp> will now perform down
                                    conversion of FP32 I/O only when
                                    <samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> is set. 
                                 </li>
                                 <li class="li">Improved the heuristics for <samp class="ph codeph">cudnnGet*Algorithm()</samp> functions.
                                    
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Known Issues and Limitations</h3>
                           <p class="p">Following issues and limitations exist in this release:</p>
                           <ul class="ul">
                              <li class="li">For FP16 inputs, the functions
                                 <samp class="ph codeph">cudnnGetConvolutionForwardAlgorithm()</samp>,
                                 <samp class="ph codeph">cudnnGetConvolutionBackwardDataAlgorithm()</samp>, and
                                 <samp class="ph codeph">cudnnGetConvolutionBackwardFilterAlgorithm()</samp> will obtain a
                                 slower algorithm. 
                              </li>
                              <li class="li">For cases where <samp class="ph codeph">beta</samp> is not equal to zero, and when the input
                                 channel size is greater than 65535, then the below
                                 <samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> algorithms may return
                                 <samp class="ph codeph">EXECUTION_FAILED</samp> error: <a name="rel_721__ul_p1h_1ht_m2b" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_721__ul_p1h_1ht_m2b">
                                    <li class="li"><samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0</samp></li>
                                    <li class="li"><samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1</samp></li>
                                    <li class="li"><samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_3</samp></li>
                                 </ul>
                              </li>
                              <li class="li"><strong class="ph b">This is a rare occurrence:</strong> When <samp class="ph codeph">beta</samp> is not equal to
                                 zero, the function
                                 <samp class="ph codeph">cudnnFindConvolutionBackwardFilterAlgorithm()</samp> may not
                                 return the fastest algorithm available for
                                 <samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp>. 
                              </li>
                              <li class="li">Grouped convolutions are not supported in the <samp class="ph codeph">TRUE_HALF_CONFIG</samp>
                                 (<samp class="ph codeph">convDesc</samp> is <samp class="ph codeph">CUDNN_DATA_HALF</samp>) data type
                                 configuration. As a workaround, the <samp class="ph codeph">PSEUDO_HALF_CONFIG</samp>
                                 (<samp class="ph codeph">convDesc</samp> is <samp class="ph codeph">CUDNN_DATA_FLOAT</samp>) data type
                                 configuration can be used without losing any precision. 
                              </li>
                              <li class="li">For the <samp class="ph codeph">cudnnConvolutionBiasActivationForward()</samp> function, if
                                 the input <samp class="ph codeph">cudnnActivationMode_t</samp> is set to enum value
                                 <samp class="ph codeph">CUDNN_ACTIVATION_IDENTITY</samp>, then the input
                                 <samp class="ph codeph">cudnnConvolutionFwdAlgo_t</samp> must be set to the enum value
                                 <samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_​PRECOMP_GEMM</samp>. 
                              </li>
                              <li class="li">When the user runs <samp class="ph codeph">cudnnRNNForward</samp>* or <samp class="ph codeph">cudnnRNNBackward</samp>*
                                 with FP32 I/O, on sm_70 or sm_72, with RNN descriptor's <samp class="ph codeph">algo</samp>
                                 field set to <samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_STATIC</samp>, and math type set
                                 to <samp class="ph codeph">CUDNN_TENSOR_OP_MATH</samp> using
                                 <samp class="ph codeph">cudnnSetRNNMatrixMathType()</samp>, then the results are
                                 incorrect. 
                              </li>
                              <li class="li">When the user runs <samp class="ph codeph">cudnnRNNForward</samp>* or <samp class="ph codeph">cudnnRNNBackward</samp>*
                                 with FP32 I/O, on sm_70 or sm_72, with RNN descriptor's <samp class="ph codeph">algo</samp>
                                 field set to <samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_STATIC</samp>, and math type set
                                 to <samp class="ph codeph">CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</samp> using
                                 <samp class="ph codeph">cudnnSetRNNMatrixMathType()</samp>, then the resulting performance
                                 is suboptimal. 
                              </li>
                           </ul>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been fixed in this release:</p>
                           <ul class="ul">
                              <li class="li">The <samp class="ph codeph">cudnnConvolutionBackwardData()</samp> function produced incorrect
                                 					result under these conditions: <a name="rel_721__ul_bms_zkm_r2b" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_721__ul_bms_zkm_r2b">
                                    <li class="li">The <samp class="ph codeph">algo</samp> input is set to
                                       								<samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp> in
                                       								<samp class="ph codeph">cudnnConvolutionBwdDataAlgo_t</samp>, and 
                                    </li>
                                    <li class="li"><samp class="ph codeph">CUDNN_TENSOR_OP_MATH</samp> is selected. 
                                       <p class="p">Under these conditions, the dgrad
                                          computation was giving incorrect results when the data is not packed
                                          and the data format is NCHW. This is fixed. 
                                       </p>
                                    </li>
                                 </ul>
                              </li>
                              <li class="li">
                                 <p class="p">When the <samp class="ph codeph">cudnnConvolutionFwdAlgo_t()</samp> was set to
                                    							<samp class="ph codeph">CONVOLUTION_FWD_ALGO_FFT_TILING</samp> then the function
                                    							<samp class="ph codeph">cudnnConvolutionForward()</samp> was leading to illegal memory
                                    						access. This is now fixed. 
                                 </p>
                              </li>
                              <li class="li"><samp class="ph codeph">cudnnPoolingBackward()</samp> was failing when using a large kernel
                                 size used for 'global_pooling' with NHWC I/O layout. This is fixed. 
                              </li>
                              <li class="li">The below two items are fixed: If you set RNN mathtype to
                                 <samp class="ph codeph">CUDNN_TENSOR_OP_MATH</samp>, and run RNN on <strong class="ph b">sm6x</strong> or earlier
                                 hardware: <a name="rel_721__ul_z4c_3ht_m2b" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_721__ul_z4c_3ht_m2b">
                                    <li class="li">You may have received <samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> when
                                       algo selected is <samp class="ph codeph">CUDNN_RNN_ALGO_STANDARD</samp> or
                                       <samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_STATIC</samp>. 
                                    </li>
                                    <li class="li">You may have received incorrect results when algo selected is
                                       <samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_DYNAMIC</samp>. 
                                    </li>
                                 </ul>
                              </li>
                              <li class="li">If you passed in variable sequence length input tensor to
                                 <samp class="ph codeph">cudnnRNNForwardInference()</samp>,
                                 <samp class="ph codeph">cudnnRNNForwardTraining()</samp>,
                                 <samp class="ph codeph">cudnnRNNBackwardData()</samp>, and used
                                 <samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_STATIC</samp> or
                                 <samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_DYNAMIC</samp>, then you may have received
                                 incorrect results. Now this is being checked, and
                                 <samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> will be returned. 
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_714"><a name="rel_714" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_714" name="rel_714" shape="rect">2.14.&nbsp;cuDNN Release 7.1.4</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"> This is the cuDNN 7.1.4 release notes. This release includes fixes from the previous
                           cuDNN v7.x.x releases as well as the following additional changes. 
                        </p>
                        <div class="section">
                           <h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <p class="p">The following enhancements have been added to this release:</p>
                           <ul class="ul">
                              <li class="li liexpand">Improved performance for some cases of data-gradient convolutions and
                                 maxpooling. This is expected to improve performance of ResNet-50 like
                                 networks.
                              </li>
                              <li class="li liexpand">The runtime of the RNN Find algorithm suite is improved in v7.1.4 resulting in
                                 slightly improved runtime of <samp class="ph codeph">cudnnFindRNN***AlgorithmEx</samp>.
                              </li>
                           </ul>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Known Issues</h3>
                           <p class="p">Following are known issues in this release:</p>
                           <ul class="ul">
                              <li class="li liexpand"><samp class="ph codeph">cudnnGet</samp> picks a slow algorithm that does not use Tensor Cores
                                 on NVIDIA Volta when inputs are FP16 and it is possible to do so.
                              </li>
                              <li class="li liexpand">The <samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> function may output
                                 incorrect results for
                                 <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT_TILING</samp> when the
                                 convolution mode is <samp class="ph codeph">CUDNN_CONVOLUTION</samp>. This function should not
                                 be used in this mode.
                              </li>
                           </ul>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been fixed in this release:</p>
                           <ul class="ul">
                              <li class="li liexpand"><samp class="ph codeph">cudnnAddTensorNd</samp> might cause a segmentation fault if called
                                 with bad arguments (for example, null pointer). This issue is in 7.1.3 only and
                                 fixed in 7.1.4.
                              </li>
                              <li class="li liexpand"><samp class="ph codeph">cudnnRNNBackwardData</samp> LSTM cell with FP16 (half) inputs might
                                 generate wrong values (silently). This issue exists in cuDNN 7.1.3 binaries
                                 compiled with CUDA Toolkit 9.0 and 9.2. This issue does not exist in cuDNN 7.1.3
                                 binaries compiled with CUDA Toolkit 9.1.
                              </li>
                              <li class="li liexpand"><samp class="ph codeph">cudnnGetRNNLinLayerMatrixParams</samp> wrongly returns
                                 <samp class="ph codeph">CUDNN_STATUS_BAD_PARAM</samp> when
                                 <samp class="ph codeph">cudnnSetRNNDescriptor</samp> is called with <samp class="ph codeph">dataType ==
                                    CUDNN_DATA_FLOAT</samp>. This is an issue in 7.1.3 only and will be fixed
                                 in 7.1.4. The <samp class="ph codeph">dataType</samp> argument as of today supports only
                                 <samp class="ph codeph">CUDNN_DATA_FLOAT</samp>. We plan to support additional compute
                                 types in the future.
                              </li>
                              <li class="li liexpand">There is a small memory leak issue when calling
                                 <samp class="ph codeph">cudnnRNNBackwardData</samp> with
                                 <samp class="ph codeph">CUDNN_RNN_ALGO_STANDARD</samp>. This issue also affects previous
                                 cuDNN v7 releases. This is fixed in 7.1.4.
                              </li>
                              <li class="li liexpand">RNN with half-precision returns <samp class="ph codeph">CUDNN_EXECUTION_FAILED</samp> on
                                 NVIDIA Kepler GPU in 7.1.3. This is fixed in 7.1.4.
                              </li>
                              <li class="li liexpand">The RNN Find algorithm suite mistakenly did not test
                                 <samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_STATIC</samp> and
                                 <samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_DYNAMIC</samp> kernels with tensor
                                 operations enabled when it was possible to do so. This is fixed in v7.1.4.
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_713"><a name="rel_713" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_713" name="rel_713" shape="rect">2.15.&nbsp;cuDNN Release 7.1.3</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"> This is the cuDNN 7.1.3 release notes. This release includes fixes from the previous
                           cuDNN v7.x.x releases as well as the following additional changes. 
                        </p>
                        <div class="section">
                           <h3 class="title sectiontitle">Known Issues</h3>
                           <p class="p">Following are known issues in this release:</p>
                           <ul class="ul">
                              <li class="li liexpand"><samp class="ph codeph">cudnnGet</samp> picks a slow algorithm that does not use Tensor Cores
                                 on NVIDIA Volta when inputs are FP16 and it is possible to do so.
                              </li>
                              <li class="li liexpand">The <samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> function may output
                                 incorrect results for
                                 <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT_TILING</samp> when the
                                 convolution mode is<samp class="ph codeph"> CUDNN_CONVOLUTION</samp> and the product
                                 <samp class="ph codeph">n*k</samp> (<samp class="ph codeph">n</samp> - batch size, <samp class="ph codeph">k</samp> -
                                 number of output feature maps) is large, that is, several thousand or more. It
                                 appears that the <samp class="ph codeph">CUDNN_CROSS_CORRELATION</samp> mode is not affected
                                 by this bug.
                              </li>
                              <li class="li liexpand">There is a small memory leak issue when calling
                                 <samp class="ph codeph">cudnnRNNBackwardData</samp> with
                                 <samp class="ph codeph">CUDNN_RNN_ALGO_STANDARD</samp>. This issue also affects previous
                                 cuDNN v7 releases.
                              </li>
                              <li class="li liexpand">RNN with half precision will not work on NVIDIA Kepler GPUs and will return
                                 <samp class="ph codeph">CUDNN_EXECUTION_FAILED</samp>. This will be fixed in future
                                 releases to return <samp class="ph codeph">CUDNN_STATUS_UNSUPPORTED</samp>.
                              </li>
                           </ul>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been fixed in this release:</p>
                           <ul class="ul">
                              <li class="li liexpand"><samp class="ph codeph">cudnnRNNbackwardData</samp> for LSTM with recurrent projection in
                                 half-precision may fail in rare cases with misaligned memory access on NVIDIA
                                 Pascal and Maxwell.
                              </li>
                              <li class="li liexpand"><samp class="ph codeph">cudnnRNNbackwardData</samp> for bidirectional LSTM with recurrent
                                 projection may produce inaccurate results or
                                 <samp class="ph codeph">CUDNN_STATUS_UNSUPPORTED</samp>.
                              </li>
                              <li class="li liexpand">Algo 1 for forward convolution and dgrad may produce erroneous results when the
                                 filter size is greater than the input size. This issue is fixed in 7.1.3.
                              </li>
                              <li class="li liexpand">For very large RNN networks, the function
                                 <samp class="ph codeph">cudnnGetRNNWorkspaceSize</samp> and
                                 <samp class="ph codeph">cudnnGetRNNTrainingReserveSize</samp> may internally overflow and
                                 give incorrect results.
                              </li>
                              <li class="li liexpand">The small performance regression on multi-layer RNNs using the STANDARD
                                 algorithm and Tensor Core math in 7.1.2, as compared to 7.0.5, is fixed in this
                                 release.
                              </li>
                              <li class="li liexpand">Fixed an issue with persistent LSTM backward pass with a hidden state size in
                                 the range 257 to 512 on GPUs with number of SMs between 22 and 31 might hang.
                                 This issue also exists in 7.1.1. This is fixed in 7.1.3.
                              </li>
                              <li class="li liexpand">Fixed an issue persistent GRU backward pass with a hidden state size in the
                                 range 513-&gt;720 on GPUs with exactly 30 SMs would hang. This issue also exists in
                                 7.1.1. This is fixed in 7.1.3.
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_712"><a name="rel_712" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_712" name="rel_712" shape="rect">2.16.&nbsp;cuDNN Release 7.1.2</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">This is the cuDNN 7.1.2 release notes. This release includes fixes from the previous
                           cuDNN v7.x.x releases as well as the following additional changes.  
                        </p>
                        <div class="section">
                           <h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <p class="p">The following enhancements have been added to this release:</p>
                           <ul class="ul">
                              <li class="li liexpand">RNN search API extended to support all RNN algorithms.</li>
                              <li class="li liexpand">Newly added projection layer supported for inference bidirectional RNN cells and
                                 for backward data and gradient.
                              </li>
                              <li class="li liexpand">Support <samp class="ph codeph">IDENTITY</samp> activation for all
                                 <samp class="ph codeph">cudnnConvolutionBiasActivationForward</samp> data types for
                                 <samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM</samp>.
                              </li>
                              <li class="li liexpand">Added documentation to clarify RNN/LSTM weight formats.</li>
                           </ul>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Known Issues</h3>
                           <p class="p">Following are known issues in this release:</p>
                           <ul class="ul">
                              <li class="li liexpand"><samp class="ph codeph">cudnnGet</samp> picks a slow algorithm that does not use Tensor Cores
                                 on NVIDIA Volta when inputs are FP16 and it is possible to do so.
                              </li>
                              <li class="li liexpand">There may be a small performance regression on multi-layer RNNs using the
                                 STANDARD algorithm with Tensor Core math in this release compared to
                                 v7.0.5.
                              </li>
                              <li class="li liexpand">LSTM projection dgrad half precision may fail in rare cases with misaligned
                                 memory access on NVIDIA Pascal and Maxwell.
                              </li>
                              <li class="li liexpand">Dgrad for bidirectional LSTM with projection should not be used, may produce
                                 inaccurate results, or <samp class="ph codeph">CUDNN_STATUS_UNSUPPORTED</samp>.
                              </li>
                              <li class="li liexpand">The <samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp> function may output
                                 incorrect results for
                                 <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT_TILING</samp> when the
                                 convolution mode is <samp class="ph codeph">CUDNN_CONVOLUTION</samp> and the product
                                 <samp class="ph codeph">n*k</samp> (<samp class="ph codeph">n</samp> - batch size, <samp class="ph codeph">k</samp> -
                                 number of output feature maps) is large, that is, several thousand or more. It
                                 appears that the <samp class="ph codeph">CUDNN_CROSS_CORRELATION</samp> mode is not affected
                                 by this.
                              </li>
                              <li class="li liexpand">Persistent LSTM backward passes with a hidden state size in the range 257 to 512
                                 on GPUs with number of SMs between 22 and 31 might hang. This issue also exists
                                 in 7.1.1 and will be fixed in 7.1.3.
                              </li>
                              <li class="li liexpand">Persistent GRU backward passes with a hidden state size in the range 513 to 720
                                 on GPUs with exactly 30 SMs would hang. This issue also exists in 7.1.1 and will
                                 be fixed in 7.1.3.
                              </li>
                              <li class="li liexpand">Algo 1 for forward convolution and dgrad may produce erroneous results when the
                                 filter size is greater than the input size.
                              </li>
                           </ul>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been fixed in this release:</p>
                           <ul class="ul">
                              <li class="li liexpand">The <samp class="ph codeph">uint8</samp> input for convolution is restricted to NVIDIA Volta
                                 and later. We added support for older architectures, for algo:
                                 <samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM</samp>.
                              </li>
                              <li class="li liexpand">In some cases when algorithm <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO1</samp>
                                 was selected, the routine <samp class="ph codeph">cudnnConvolutionBackwardFilter</samp> could
                                 fail at runtime and return <samp class="ph codeph">CUDNN_STATUS_EXECUTION_FAILED</samp>. It
                                 now returns <samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp>.
                              </li>
                              <li class="li liexpand"><samp class="ph codeph">cudnnSetRNNDescriptor</samp> no longer needs valid Dropout Descriptor
                                 in inference mode, user can pass NULL for Dropout Descriptor in inference
                                 mode.
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_711"><a name="rel_711" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_711" name="rel_711" shape="rect">2.17.&nbsp;cuDNN Release 7.1.1</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"> This is the cuDNN 7.1.1 release notes. This release includes fixes from the previous
                           cuDNN v7.x.x releases as well as the following additional changes. 
                        </p>
                        <div class="section">
                           <h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <p class="p">The following enhancements have been added to this release:</p>
                           <ul class="ul">
                              <li class="li liexpand">Added new API <samp class="ph codeph">cudnnSetRNNProjectionLayers</samp> and
                                 <samp class="ph codeph">cudnnGetRNNProjectionLayers</samp> to support Projection Layer for
                                 the RNN LSTM cell. In this release, only the inference use case will be
                                 supported. The bi-directional and the training forward and backward for training
                                 is not supported in 7.1.1 but will be supported in the upcoming 7.1.2 release
                                 without API changes. For all the unsupported cases in this release,
                                 <samp class="ph codeph">CUDNN_NOT_SUPPORTED</samp> is returned when projection layer is
                                 set and the RNN is called.
                              </li>
                              <li class="li liexpand">The <samp class="ph codeph">cudnnGetRNNLinLayerMatrixParams()</samp> function was enhanced and
                                 a bug was fixed without modifying its prototype. Specifically:<a name="rel_711__ul_s4w_hmj_ycb" shape="rect">
                                    <!-- --></a><ul class="ul" id="rel_711__ul_s4w_hmj_ycb">
                                    <li class="li liexpand">The <samp class="ph codeph">cudnnGetRNNLinLayerMatrixParams()</samp> function was
                                       updated to support the RNN projection feature. An extra
                                       <samp class="ph codeph">linLayerID</samp> value of eight can be used to retrieve
                                       the address and the size of the “recurrent” projection weight matrix
                                       when "mode" in <samp class="ph codeph">cudnnSetRNNDescriptor()</samp> is configured to
                                       <samp class="ph codeph">CUDNN_LSTM</samp> and the recurrent projection is enabled
                                       using <samp class="ph codeph">cudnnSetRNNProjectionLayers()</samp>. 
                                    </li>
                                    <li class="li liexpand">Instead of reporting the total number of elements in each weight matrix
                                       in the <samp class="ph codeph">linLayerMatDesc</samp> filter descriptor, the
                                       <samp class="ph codeph">cudnnGetRNNLinLayerMatrixParams()</samp> function returns
                                       the matrix size as two dimensions: rows and columns. This allows the
                                       user to easily print and initialize RNN weight matrices. Elements in
                                       each weight matrix are arranged in the row-major order. Due to
                                       historical reasons, the minimum number of dimensions in the filter
                                       descriptor is three. In previous versions of the cuDNN library,
                                       <samp class="ph codeph">cudnnGetRNNLinLayerMatrixParams()</samp> returned the
                                       total number of weights as follows: <samp class="ph codeph">filterDimA[0]=total_size,
                                          filterDimA[1]=1, filterDimA[2]=1</samp>. In v7.1.1, the format was
                                       changed to: <samp class="ph codeph">filterDimA[0]=1, filterDimA[1]=rows,
                                          filterDimA[2]=columns</samp>. In both cases, the "format" field of
                                       the filter descriptor should be ignored when retrieved by
                                       <samp class="ph codeph">cudnnGetFilterNdDescriptor()</samp>. 
                                    </li>
                                    <li class="li liexpand">A bug in <samp class="ph codeph">cudnnGetRNNLinLayerMatrixParams()</samp> was fixed to
                                       return a zeroed filter descriptor when the corresponding weight matrix
                                       does not exist. This occurs, for example, for
                                       <samp class="ph codeph">linLayerID</samp> values of 0-3 when the first RNN layer
                                       is configured to exclude matrix multiplications applied to RNN input
                                       data (<samp class="ph codeph">inputMode=CUDNN_SKIP_INPUT</samp> in
                                       <samp class="ph codeph">cudnnSetRNNDescriptor()</samp> specifies implicit, fixed
                                       identity weight matrices for RNN input). Such cases in previous versions
                                       of the cuDNN library caused
                                       <samp class="ph codeph">cudnnGetRNNLinLayerMatrixParams()</samp> to return
                                       corrupted filter descriptors with some entries from the previous call. A
                                       workaround was to create a new filter descriptor for every invocation of
                                       <samp class="ph codeph">cudnnGetRNNLinLayerMatrixParams()</samp>. 
                                    </li>
                                 </ul>
                              </li>
                              <li class="li liexpand">The <samp class="ph codeph">cudnnGetRNNLinLayerBiasParams()</samp> function was updated to
                                 report the bias column vectors in <samp class="ph codeph">linLayerBiasDesc</samp> in the same
                                 format as <samp class="ph codeph">cudnnGetRNNLinLayerMatrixParams()</samp>. In previous
                                 versions of the cuDNN library, <samp class="ph codeph">cudnnGetRNNLinLayerBiasParams()</samp>
                                 returned the total number of adjustable bias parameters as follows:
                                 <samp class="ph codeph">filterDimA[0]=total_size, filterDimA[1]=1, filterDimA[2]=1</samp>.
                                 In v7.1.1, the format was changed to: <samp class="ph codeph">filterDimA[0]=1,
                                    filterDimA[1]=rows, filterDimA[2]=1</samp> (number of columns). In both
                                 cases, the <samp class="ph codeph">format</samp> field of the filter descriptor should be
                                 ignored when retrieved by <samp class="ph codeph">cudnnGetFilterNdDescriptor()</samp>. The
                                 recurrent projection GEMM does not have a bias so the range of valid inputs for
                                 the <samp class="ph codeph">linLayerID</samp> argument remains the same. 
                              </li>
                              <li class="li liexpand">Added support for use of Tensor Core for the
                                 <samp class="ph codeph">CUDNN_RNN_ALGO_PERSIST_STATIC</samp>. This required cuDNN v7.1
                                 built with CUDA 9.1 and 387 or higher driver. It will not work with CUDA 9.0 and
                                 384 driver.
                              </li>
                              <li class="li liexpand">Added RNN search API that allows the application to provide an RNN descriptor
                                 and get a list of possible algorithm choices with performance and memory usage,
                                 to allow applications to choose between different implementations. For more
                                 information, refer to the documentation of:
                                 <samp class="ph codeph">cudnnFindRNNForwardInferenceAlgorithmEx</samp>,
                                 <samp class="ph codeph">cudnnFindRNNForwardTrainingAlgorithmEx</samp>,
                                 <samp class="ph codeph">cudnnFindRNNBackwardDataAlgorithmEx</samp>, and
                                 <samp class="ph codeph">cudnnFindRNNBackwardWeightsAlgorithmEx</samp>. In this release,
                                 the search will operate on <samp class="ph codeph">STANDARD</samp> algorithm and will not
                                 support <samp class="ph codeph">PERSISTENT</samp> algorithms of RNN.
                              </li>
                              <li class="li liexpand">Added <samp class="ph codeph">uint8</samp> for support for the input data for
                                 <samp class="ph codeph">cudnnConvolutionBiasActivationForward</samp> and
                                 <samp class="ph codeph">cudnnConvolutionForward</samp>. Currently, the support is on
                                 NVIDIA Volta (sm 70 ) and later architectures. Support for older architectures
                                 will be gradually added in the upcoming releases.
                              </li>
                              <li class="li liexpand">Support for <samp class="ph codeph">CUDNN_ACTIVATION_IDENTITY</samp> is added to
                                 <samp class="ph codeph">cudnnConvolutionBiasActivationForward</samp>. This allows users to
                                 perform convolution and bias without activation.
                              </li>
                              <li class="li liexpand">All API functions now support logging. User can trigger logging by setting
                                 environment variable <samp class="ph codeph">CUDNN_LOGINFO_DBG=1</samp> and
                                 <samp class="ph codeph">CUDNN_LOGDEST_DBG= &lt;option&gt;</samp> where
                                 <samp class="ph codeph">&lt;option&gt;</samp> (that is, the output destination of the log)
                                 can be chosen from <samp class="ph codeph">stdout</samp>, <samp class="ph codeph">stderr</samp>, or a file
                                 path. User may also use the new
                                 <samp class="ph codeph">Set</samp>/<samp class="ph codeph">GetCallBack</samp> functions to install their
                                 customized callback function. Log files can be added to the reported bugs or
                                 shared with us for analysis and future optimizations through
                                 partners.nvidia.com.
                              </li>
                              <li class="li liexpand">Improved performance of 3D convolution on NVIDIA Volta architecture.</li>
                              <li class="li liexpand">The following algo-related functions have been added for this release:
                                 <samp class="ph codeph">cudnnGetAlgorithmSpaceSize</samp>,
                                 <samp class="ph codeph">cudnnSaveAlgorithm</samp>, <samp class="ph codeph">cudnnRestoreAlgorithm</samp>,
                                 <samp class="ph codeph">cudnnCreateAlgorithmDescriptor</samp>,
                                 <samp class="ph codeph">cudnnSetAlgorithmDescriptor</samp>,
                                 <samp class="ph codeph">cudnnGetAlgorithmDescriptor</samp>,
                                 <samp class="ph codeph">cudnnDestroyAlgorithmDescriptor</samp>,
                                 <samp class="ph codeph">cudnnCreateAlgorithmPerformance</samp>,
                                 <samp class="ph codeph">cudnnSetAlgorithmPerformance</samp>,
                                 <samp class="ph codeph">cudnnGetAlgorithmPerformance</samp>,
                                 <samp class="ph codeph">cudnnDestroyAlgorithmPerformance</samp>.
                              </li>
                              <li class="li liexpand">All algorithms for convolutions now support groupCount &gt; 1. This includes
                                 <samp class="ph codeph">cudnConvolutionForward()</samp>,
                                 <samp class="ph codeph">cudnnConvolutionBackwardData()</samp>, and
                                 <samp class="ph codeph">cudnnConvolutionBackwardFilter()</samp>.
                              </li>
                           </ul>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Known Issues</h3>
                           <p class="p">Following are known issues in this release:</p>
                           <ul class="ul">
                              <li class="li liexpand">RNN search Algorithm is restricted to <samp class="ph codeph">STANDARD</samp> algorithm.
                              </li>
                              <li class="li liexpand">Newly added projection Layer supported for inference and one directional RNN
                                 cell.
                              </li>
                              <li class="li liexpand">uint8 input for convolution is restricted to NVIDIA Volta and later.</li>
                              <li class="li liexpand"><samp class="ph codeph">cudnnGet</samp> picks a slow algorithm that does not use Tensor Cores
                                 on NVIDIA Volta when inputs are FP16 and it is possible to do so.
                              </li>
                              <li class="li liexpand">There may be a small performance regression on multi-layer RNNs using the
                                 <samp class="ph codeph">STANDARD</samp> algorithm with Tensor Core math in this release
                                 compared to 7.0.5.
                              </li>
                           </ul>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been fixed in this release:</p>
                           <ul class="ul">
                              <li class="li liexpand">3D convolution performance improvements for NVIDIA Volta.</li>
                              <li class="li liexpand">Added support for Algorithm 0 data gradients to cover cases previously not
                                 supported.
                              </li>
                              <li class="li liexpand">Removed the requirement for dropout descriptor in RNN inference. Before
                                 application had to set a non-point for the dropout Descriptor that was not
                                 used.
                              </li>
                              <li class="li liexpand">Use of <samp class="ph codeph">CUDNN_TENSOR_NCHW_VECT_C</samp> with non-zero padding resulted
                                 in a return status of <samp class="ph codeph">CUDNN_STATUS_INTERNAL_ERROR</samp>. This issue
                                 is now fixed.
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_705"><a name="rel_705" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_705" name="rel_705" shape="rect">2.18.&nbsp;cuDNN Release 7.0.5</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"> This is the cuDNN 7.0.5 release notes. This release includes fixes from the previous
                           cuDNN v7.x.x releases as well as the following additional changes. 
                        </p>
                        <div class="section">
                           <h3 class="title sectiontitle">Known Issues</h3>
                           <p class="p">Following are known issues in this release:</p>
                           <ul class="ul">
                              <li class="li liexpand">cuDNN library may trigger a CPU floating point exception when FP exceptions are
                                 enabled by user. This issue exists for all 7.0.x releases.
                              </li>
                              <li class="li liexpand">There are heavy use cases of RNN layers that might hit a memory allocation issue
                                 in the CUDA driver when using cuDNN v7 with CUDA 8.0 and R375 driver on
                                 pre-Pascal architectures (NVIDIA Kepler and Maxwell). In these cases, subsequent
                                 CUDA kernels may fail to launch with an Error Code 30. To resolve the issue, it
                                 is recommended to use the latest R384 driver (from NVIDIA driver downloads) or
                                 to ensure that the persistence daemon is started. This behavior is observed on
                                 all 7.0.x releases. 
                              </li>
                              <li class="li liexpand">When using <samp class="ph codeph">TENSOR_OP_MATH</samp> mode with
                                 <samp class="ph codeph">cudnnConvolutionBiasActivationForward</samp>, the pointer to the
                                 bias must be aligned to 16 bytes and the size of allocated memory must be
                                 multiples of 256 elements. This behavior exists for all 7.0.x releases.
                              </li>
                           </ul>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been fixed in this release:</p>
                           <ul class="ul">
                              <li class="li liexpand">Corrected the algorithm fallback behavior in RNN when user set to use
                                 <samp class="ph codeph">CUDNN_TENSOR_OP_MATH</samp> when using compute card without Tensor
                                 Cores. Instead of returning <samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp>, the RNN
                                 algorithm will now continue to run using <samp class="ph codeph">CUDNN_DEFAULT_MATH</samp>.
                                 The correct behavior is to fall back to using default math when Tensor Core is
                                 not supported. Fixed to the expected behavior.
                              </li>
                              <li class="li liexpand">On NVIDIA Volta hardware, <samp class="ph codeph">BWD_FILTER_ALGO_1</samp> and
                                 <samp class="ph codeph">BWD_DATA_ALGO_1</samp> convolutions using a number of filter
                                 elements greater than 512 were causing
                                 <samp class="ph codeph">CUDA_ERROR_ILLEGAL_ADDRESS</samp> and
                                 <samp class="ph codeph">CUDNN_STATUS_INTERNAL_ERROR</samp> errors. Logic was added to fall
                                 back to a generic kernel for these filter sizes.
                              </li>
                              <li class="li liexpand">cuDNN v7 with CUDA 8.0 produced erroneous results on NVIDIA Volta for some
                                 common cases of Algo 1. Logic was added to fall back to a generic kernel when
                                 cudnn v7 with CUDA 8.0 is used on NVIDIA Volta.
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_704"><a name="rel_704" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_704" name="rel_704" shape="rect">2.19.&nbsp;cuDNN Release 7.0.4</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"> This is the cuDNN 7.0.4 release notes. This release includes fixes from the previous
                           cuDNN v7.x.x releases as well as the following additional changes. 
                        </p>
                        <div class="section">
                           <h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p"><a name="rel_704__ul_zdb_1d1_lwb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel_704__ul_zdb_1d1_lwb">
                                 <li class="li">Performance improvements for grouped convolutions when input channels and
                                    output channels per group are one, two, or four for the following
                                    algorithms:<a name="rel_704__ul_msl_1d1_lwb" shape="rect">
                                       <!-- --></a><ul class="ul" id="rel_704__ul_msl_1d1_lwb">
                                       <li class="li"><samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM</samp></li>
                                       <li class="li"><samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO0</samp></li>
                                       <li class="li"><samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp></li>
                                       <li class="li"><samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0</samp></li>
                                       <li class="li"><samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1</samp></li>
                                    </ul>
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Known Issues</h3>
                           <p class="p">Following are known issues in this release:</p>
                           <ul class="ul">
                              <li class="li liexpand">The CUDA 8.0 build of cuDNN may produce incorrect computations when run on
                                 NVIDIA Volta.
                              </li>
                              <li class="li liexpand">cuDNN library triggers CPU floating point exception when FP exceptions are
                                 enabled by user. This issue exists for all 7.0.x releases.
                              </li>
                              <li class="li liexpand">There are heavy use cases of RNN layers that might hit a memory allocation issue
                                 in the CUDA driver when using cuDNN v7 with CUDA 8.0 and R375 driver on
                                 pre-Pascal architectures (NVIDIA Kepler and Maxwell). In these cases, subsequent
                                 CUDA kernels may fail to launch with an Error Code 30. To resolve the issue, it
                                 is recommended to use the latest R384 driver (from NVIDIA driver downloads) or
                                 to ensure that the persistence daemon is started. This behavior is observed on
                                 all 7.0.x releases.
                              </li>
                              <li class="li liexpand">When using <samp class="ph codeph">TENSOR_OP_MATH</samp> mode with
                                 <samp class="ph codeph">cudnnConvolutionBiasActivationForward</samp>, the pointer to the
                                 bias must be aligned to 16 bytes and the size of allocated memory must be
                                 multiples of 256 elements. This behavior exists for all 7.0.x releases.
                              </li>
                           </ul>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been fixed in this release:</p>
                           <ul class="ul">
                              <li class="li liexpand">Fixed out-of-band global memory accesses in the 256-point 1D FFT kernel. The
                                 problem-affected convolutions with 1x1 filters and tall but narrow images, for
                                 example, 1x500 (WxH). In those cases, the workspace size for the
                                 <samp class="ph codeph">FFT_TILING</samp> algo was computed incorrectly. There was no
                                 error in the FFT kernel.
                              </li>
                              <li class="li liexpand">Eliminated a source of floating point exceptions in the
                                 <samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED</samp> algorithm. The
                                 host code to generate a negative infinity-floating point value was substituted
                                 with a different logic. By default, FP exceptions are disabled. However, a user
                                 program enabled them by invoking <samp class="ph codeph">feenableexcept()</samp>. There are at
                                 least two other sources of FP exceptions in the cuDNN library, affecting for
                                 example <samp class="ph codeph">BATCHNORM_SPATIAL_PERSISTENT</samp>. Those sources of FP
                                 exceptions will be eliminated in future releases of the cuDNN library.
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_703"><a name="rel_703" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_703" name="rel_703" shape="rect">2.20.&nbsp;cuDNN Release 7.0.3</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc">This is the cuDNN 7.0.3 release notes. This release includes fixes from the previous
                           cuDNN v7.x.x releases as well as the following additional changes.  
                        </p>
                        <div class="section">
                           <h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p">Performance improvements for various cases:<a name="rel_703__ul_hqr_xkn_3bb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel_703__ul_hqr_xkn_3bb">
                                 <li class="li liexpand">Forward-grouped convolutions where input channel per groups is one, two, or
                                    four and hardware is NVIDIA Volta or NVIDIA Pascal.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnTransformTensor()</samp> where input and output tensor is
                                    packed. 
                                    <div class="note note"><span class="notetitle">Note:</span> This is an improved fallback, improvements will not be seen in
                                       all cases.
                                    </div>
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Known Issues</h3>
                           <p class="p">The following are known issues in this release:</p>
                           <ul class="ul">
                              <li class="li"><samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING</samp> may cause
                                 <samp class="ph codeph">CUDA_ERROR_ILLEGAL_ADDRESS</samp>. This issue affects input images
                                 of just one pixel in width and certain <samp class="ph codeph">n</samp>, <samp class="ph codeph">c</samp>,
                                 <samp class="ph codeph">k</samp>, <samp class="ph codeph">h</samp> combinations. 
                              </li>
                           </ul>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Fixed Issues</h3>
                           <p class="p">The following issues have been fixed in this release:</p>
                           <ul class="ul">
                              <li class="li liexpand"><samp class="ph codeph">AddTensor</samp> and <samp class="ph codeph">TensorOp</samp> produce incorrect
                                 results for half and INT8 inputs for various use cases.
                              </li>
                              <li class="li liexpand"><samp class="ph codeph">cudnnPoolingBackward()</samp> can produce incorrect values for rare
                                 cases of non-deterministic MAX pooling with <samp class="ph codeph">window_width &gt; 256</samp>.
                                 These rare cases are when the maximum element in a window is duplicated
                                 horizontally (along width) by a stride of <samp class="ph codeph">256*k</samp> for some
                                 <samp class="ph codeph">k</samp>. The behavior is now fixed to accumulate derivatives for
                                 the duplicate that is left most. 
                              </li>
                              <li class="li liexpand"><samp class="ph codeph">cudnnGetConvolutionForwardWorkspaceSize()</samp> produces incorrect
                                 workspace size for algorithm <samp class="ph codeph">FFT_TILING</samp> for 1d convolutions.
                                 This only occurs for large sized convolutions where intermediate calculations
                                 produce values greater than 2^31 (2 to the power of 31). 
                              </li>
                              <li class="li liexpand"><samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp> returned by
                                 <samp class="ph codeph">cudnnPooling*()</samp> functions for small <samp class="ph codeph">x</samp>
                                 image (<samp class="ph codeph">channels * height * width &lt; 4</samp>).
                              </li>
                           </ul>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="rel_7"><a name="rel_7" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#rel_7" name="rel_7" shape="rect">2.22.&nbsp;cuDNN Release 7.0.2</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"> This is the cuDNN 7.0.2 release notes. This release includes fixes from the previous
                           cuDNN v7.x.x releases as well as the following additional changes. 
                        </p>
                        <div class="section">
                           <h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <div class="p"><a name="rel_7__ul_qkc_2d1_lwb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel_7__ul_qkc_2d1_lwb">
                                 <li class="li">This is a patch release of cuDNN 7.0 and includes bug fixes and performance
                                    improvements mainly on NVIDIA Volta.
                                    <dl class="dl">
                                       <dt class="dt dlterm">Algo 1 Convolutions Performance Improvements</dt>
                                       <dd class="dd">Performance improvements were made to
                                          <samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp>,
                                          <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1</samp>, and
                                          <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_DATA_ALGO_1</samp>. These
                                          improvements consist of new SASS kernels and improved
                                          heuristics. The new kernels implement convolutions over various
                                          data sizes and tile sizes. The improved heuristics take
                                          advantage of these new kernels.
                                       </dd>
                                    </dl>
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Known Issues</h3>
                           <p class="p">The following are known issues in this release:</p>
                           <ul class="ul">
                              <li class="li liexpand"><samp class="ph codeph">cudnnGetConvolutionForwardWorkspaceSize()</samp> returns overflowed
                                 <samp class="ph codeph">size_t</samp> value for certain input shape for
                                 <samp class="ph codeph">CUDNN_CONVOLUTION_*_ALGO_FFT_TILING</samp>.
                              </li>
                              <li class="li liexpand"><samp class="ph codeph">cudnnPoolingBackward()</samp> fails for pooling window size &gt;
                                 256.
                              </li>
                           </ul>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Fixed Issues</h3>
                           <div class="p">The following issues have been fixed in this release:<a name="rel_7__ul_ujl_fd1_lwb" shape="rect">
                                 <!-- --></a><ul class="ul" id="rel_7__ul_ujl_fd1_lwb">
                                 <li class="li liexpand">Batch Norm <samp class="ph codeph">CUDNN_BATCHNORM_SPATIAL_PERSISTENT</samp> might get
                                    into race conditions in certain scenarios.
                                 </li>
                                 <li class="li liexpand">cuDNN convolution layers using <samp class="ph codeph">TENSOR_OP_MATH</samp> with FP16
                                    inputs and outputs and FP32 compute will use “round to nearest” mode instead
                                    of “round to zero” mode as in 7.0.1. This rounding mode has proven to
                                    achieve better results in training.
                                 </li>
                                 <li class="li liexpand">Fixed synchronization logic in the
                                    <samp class="ph codeph">CUDNN_CTC_LOSS_ALGO_DETERMINISTIC</samp> algo for CTC. The
                                    original code would hang in rare cases.
                                 </li>
                                 <li class="li liexpand">Convolution algorithms using <samp class="ph codeph">TENSOR_OP_MATH</samp> returned a
                                    workspace size from <samp class="ph codeph">*GetWorkspaceSize()</samp> smaller than
                                    actually necessary.
                                 </li>
                                 <li class="li liexpand">The results of INT8 are inaccurate in certain cases when calling
                                    <samp class="ph codeph">cudnnConvolutionForward()</samp> in convolution layer.
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnConvolutionForward()</samp> called with <samp class="ph codeph">xDesc’s
                                       channel = yDesc’s channel = groupCount</samp> could compute incorrect
                                    values when vertical padding &gt; 0.
                                 </li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="unique_536635605"><a name="unique_536635605" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#unique_536635605" name="unique_536635605" shape="rect">cuDNN Release 7.0.1</a></h3>
                     <div class="body refbody">
                        <p class="shortdesc"> This is the cuDNN 7.0.1 release notes. This release includes the following changes. </p>
                        <div class="section">
                           <p class="p">cuDNN v7.0.1 is the first release to support the NVIDIA Volta GPU architecture. In
                              addition, cuDNN v7.0.1 brings new layers, grouped convolutions, and improved
                              convolution find as error query mechanism.
                           </p>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Key Features and Enhancements</h3>
                           <p class="p">This cuDNN release includes the following key features and enhancements. </p>
                           <div class="p">
                              <dl class="dl">
                                 <dt class="dt dltermexpand">Tensor Cores</dt>
                                 <dd class="dd">Version 7.0.1 of cuDNN is the first to support the Tensor Core
                                    operations in its implementation. Tensor Cores provide highly optimized
                                    matrix multiplication building blocks that do not have an equivalent
                                    numerical behavior in the traditional instructions, therefore, its
                                    numerical behavior is slightly different. 
                                    <p class="p"></p>
                                 </dd>
                                 <dt class="dt dltermexpand"><samp class="ph codeph">cudnnSetConvolutionMathType</samp>,
                                    <samp class="ph codeph">cudnnSetRNNMatrixMathType</samp>, and
                                    <samp class="ph codeph">cudnnMathType_t</samp></dt>
                                 <dd class="dd">The <samp class="ph codeph">cudnnSetConvolutionMathType</samp> and
                                    <samp class="ph codeph">cudnnSetRNNMatrixMathType</samp> functions enable you to
                                    choose whether or not to use Tensor Core operations in the convolution
                                    and RNN layers respectively by setting the math mode to either
                                    <samp class="ph codeph">CUDNN_TENSOR_OP_MATH</samp> or
                                    <samp class="ph codeph">CUDNN_DEFAULT_MATH</samp>. 
                                    <p class="p">Tensor Core operations
                                       perform parallel floating point accumulation of multiple floating
                                       point products. 
                                    </p>
                                    <p class="p">Setting the math mode to
                                       <samp class="ph codeph">CUDNN_TENSOR_OP_MATH</samp> indicates that the library
                                       will use Tensor Core operations.
                                    </p>
                                    <div class="p">The default is
                                       <samp class="ph codeph">CUDNN_DEFAULT_MATH</samp>. This default indicates that
                                       the Tensor Core operations will be avoided by the library. The
                                       default mode is a serialized operation whereas, the Tensor Core is a
                                       parallelized operation, therefore, the two might result in slightly
                                       different numerical results due to the different sequencing of
                                       operations. 
                                       <div class="note note"><span class="notetitle">Note:</span> The library falls back to the default math mode
                                          when Tensor Core operations are not supported or not permitted.
                                          
                                       </div>
                                    </div>
                                 </dd>
                                 <dt class="dt dltermexpand"><samp class="ph codeph">cudnnSetConvolutionGroupCount</samp></dt>
                                 <dd class="dd">A new interface that allows applications to perform convolution groups
                                    in the convolution layers in a single API call. 
                                 </dd>
                                 <dt class="dt dltermexpand"><samp class="ph codeph">cudnnCTCLoss</samp></dt>
                                 <dd class="dd"><samp class="ph codeph">cudnnCTCLoss</samp> provides a GPU implementation of the
                                    Connectionist Temporal Classification (CTC) loss function for RNNs. The
                                    CTC loss function is used for phoneme recognition in speech and
                                    handwriting recognition.
                                 </dd>
                                 <dt class="dt dltermexpand"><samp class="ph codeph">CUDNN_BATCHNORM_SPATIAL_PERSISTENT</samp></dt>
                                 <dd class="dd">The <samp class="ph codeph">CUDNN_BATCHNORM_SPATIAL_PERSISTENT</samp> function is a
                                    new batch normalization mode for
                                    <samp class="ph codeph">cudnnBatchNormalizationForwardTraining</samp> and
                                    <samp class="ph codeph">cudnnBatchNormalizationBackward</samp>. This mode is
                                    similar to <samp class="ph codeph">CUDNN_BATCHNORM_SPATIAL</samp>, however, it can be
                                    faster for some tasks. 
                                 </dd>
                                 <dt class="dt dltermexpand"><samp class="ph codeph">cudnnQueryRuntimeError</samp></dt>
                                 <dd class="dd">The <samp class="ph codeph">cudnnQueryRuntimeError</samp> function reports error codes
                                    written by GPU kernels when executing
                                    <samp class="ph codeph">cudnnBatchNormalizationForwardTraining</samp> and
                                    <samp class="ph codeph">cudnnBatchNormalizationBackward</samp> with the
                                    <samp class="ph codeph">CUDNN_BATCHNORM_SPATIAL_PERSISTENT</samp> mode.
                                 </dd>
                                 <dt class="dt dltermexpand"><samp class="ph codeph">cudnnGetConvolutionForwardAlgorithm_v7</samp></dt>
                                 <dd class="dd">This new API returns all algorithms sorted by expected performance
                                    (using internal heuristics). These algorithms are output similarly to
                                    <samp class="ph codeph">cudnnFindConvolutionForwardAlgorithm</samp>.
                                 </dd>
                                 <dt class="dt dltermexpand"><samp class="ph codeph">cudnnGetConvolutionBackwardDataAlgorithm_v7</samp></dt>
                                 <dd class="dd">This new API returns all algorithms sorted by expected performance
                                    (using internal heuristics). These algorithms are output similarly to
                                    <samp class="ph codeph">cudnnFindConvolutionBackwardAlgorithm</samp>.
                                 </dd>
                                 <dt class="dt dltermexpand"><samp class="ph codeph">cudnnGetConvolutionBackwardFilterAlgorithm_v7</samp></dt>
                                 <dd class="dd">This new API returns all algorithms sorted by expected performance
                                    (using internal heuristics). These algorithms are output similarly to
                                    <samp class="ph codeph">cudnnFindConvolutionBackwardFilterAlgorithm</samp>.
                                 </dd>
                                 <dt class="dt dltermexpand"><samp class="ph codeph">CUDNN_REDUCE_TENSOR_MUL_NO_ZEROS</samp></dt>
                                 <dd class="dd">The <samp class="ph codeph">MUL_NO_ZEROS</samp> function is a multiplication reduction
                                    that ignores zeros in the data. 
                                 </dd>
                                 <dt class="dt dltermexpand"><samp class="ph codeph">CUDNN_OP_TENSOR_NOT</samp></dt>
                                 <dd class="dd">The <samp class="ph codeph">OP_TENSOR_NOT</samp> function is a unary operation that
                                    takes the negative of (alpha*A).
                                 </dd>
                                 <dt class="dt dltermexpand"><samp class="ph codeph">cudnnGetDropoutDescriptor</samp></dt>
                                 <dd class="dd">The <samp class="ph codeph">cudnnGetDropoutDescriptor</samp> function allows
                                    applications to get dropout values.
                                 </dd>
                              </dl>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Using cuDNN v7.0.1</h3>
                           <div class="p">Ensure you are familiar with the following notes when using this release.<a name="unique_536635605__ul_n5h_htf_21b" shape="rect">
                                 <!-- --></a><ul class="ul" id="unique_536635605__ul_n5h_htf_21b">
                                 <li class="li liexpand">Multi-threading behavior has been modified. Multi-threading is allowed only
                                    when using different cuDNN handles in different threads.
                                 </li>
                                 <li class="li liexpand">In <samp class="ph codeph">cudnnConvolutionBackwardFilter</samp>, dilated convolution did
                                    not support cases where the product of all filter dimensions was odd for
                                    half precision-floating point. These are now supported by
                                    <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO1</samp>. 
                                 </li>
                                 <li class="li liexpand">Fixed bug that produced a silent computation error for when a batch size was
                                    larger than 65536 for
                                    <samp class="ph codeph">CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM</samp>.
                                 </li>
                                 <li class="li liexpand">In <samp class="ph codeph">getConvolutionForwardAlgorithm</samp>, an error was not
                                    correctly reported in v5 when the output size was larger than expected. In
                                    v6 the <samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp>, error message displayed.
                                    In v7, this error is modified to <samp class="ph codeph">CUDNN_STATUS_BAD_PARAM</samp>. 
                                 </li>
                                 <li class="li liexpand">In <samp class="ph codeph">cudnnConvolutionBackwardFilter</samp>, cuDNN now runs some
                                    exceptional cases correctly where it previously erroneously returned
                                    <samp class="ph codeph">CUDNN_STATUS_NOT_SUPPORTED</samp>. This impacted the
                                    algorithms <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO0</samp> and
                                    <samp class="ph codeph">CUDNN_CONVOLUTION_BWD_FILTER_ALGO3</samp>. 
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Deprecated Features</h3>
                           <div class="p">The following routines have been removed:<a name="unique_536635605__ul_w5x_ftf_21b" shape="rect">
                                 <!-- --></a><ul class="ul" id="unique_536635605__ul_w5x_ftf_21b">
                                 <li class="li"><samp class="ph codeph">cudnnSetConvolution2dDescriptor_v4</samp></li>
                                 <li class="li"><samp class="ph codeph">cudnnSetConvolution2dDescriptor_v5</samp></li>
                                 <li class="li"><samp class="ph codeph">cudnnGetConvolution2dDescriptor_v4</samp></li>
                                 <li class="li"><samp class="ph codeph">cudnnGetConvolution2dDescriptor_v5</samp></li>
                              </ul>
                              <div class="note note"><span class="notetitle">Note:</span> Only the non-suffixed versions of these routines remain.
                              </div>
                           </div>
                           <div class="p">The following routines have been created and have the same API prototype as their
                              non-suffixed equivalent from cuDNN v6:<a name="unique_536635605__ul_xqc_gqk_r1b" shape="rect">
                                 <!-- --></a><ul class="ul" id="unique_536635605__ul_xqc_gqk_r1b">
                                 <li class="li liexpand"><samp class="ph codeph">cudnnSetRNNDescriptor_v5</samp> - The non-suffixed version of the
                                    routines in cuDNN v7.0.1 are now mapped to their <samp class="ph codeph">_v6</samp>
                                    equivalent. 
                                    <div class="note attention"><span class="attentiontitle">Attention:</span>  It is strongly advised using the
                                       non-suffixed version as the <samp class="ph codeph">_v5</samp> and
                                       <samp class="ph codeph">_v6</samp> routines will be removed in the next cuDNN
                                       release.
                                    </div>
                                 </li>
                                 <li class="li liexpand"><samp class="ph codeph">cudnnGetConvolutionForwardAlgorithm</samp>,
                                    <samp class="ph codeph">cudnnGetConvolutionBackwardDataAlgorithm</samp>, and
                                    <samp class="ph codeph">cudnnGetConvolutionBackwardFilterAlgorithm</samp> - A
                                    <samp class="ph codeph">_v7</samp> version of this routine has been created. For more
                                    information, see the <em class="ph i">Backward compatibility and deprecation policy</em>
                                    chapter of the cuDNN documentation for details.
                                 </li>
                              </ul>
                           </div>
                        </div>
                        <div class="section">
                           <h3 class="title sectiontitle">Known Issues</h3>
                           <div class="p"><a name="unique_536635605__ul_i5b_1ng_21b" shape="rect">
                                 <!-- --></a><ul class="ul" id="unique_536635605__ul_i5b_1ng_21b">
                                 <li class="li">cuDNN pooling backwards fails for pooling window size &gt; 256.</li>
                              </ul>
                           </div>
                        </div>
                     </div>
                  </div>
               </div>
               <div class="topic concept nested0" id="notices-header"><a name="notices-header" shape="rect">
                     <!-- --></a><h2 class="title topictitle1"><a href="#notices-header" name="notices-header" shape="rect">Notices</a></h2>
                  <div class="topic reference nested1" id="notice"><a name="notice" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#notice" name="notice" shape="rect"></a></h3>
                     <div class="body refbody">
                        <div class="section notices" id="notice__section_kbg_pmm_flb"><a name="notice__section_kbg_pmm_flb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle notices">Notice</h3>
                           <p class="p" id="notice__notice-para-1"><a name="notice__notice-para-1" shape="rect">
                                 <!-- --></a>This document is provided for information purposes
                              only and shall not be regarded as a warranty of a certain
                              functionality, condition, or quality of a product. NVIDIA
                              Corporation (“NVIDIA”) makes no representations or warranties,
                              expressed or implied, as to the accuracy or completeness of the
                              information contained in this document and assumes no responsibility
                              for any errors contained herein. NVIDIA shall have no liability for
                              the consequences or use of such information or for any infringement
                              of patents or other rights of third parties that may result from its
                              use. This document is not a commitment to develop, release, or
                              deliver any Material (defined below), code, or functionality.
                           </p>
                           <p class="p" id="notice__notice-para-2"><a name="notice__notice-para-2" shape="rect">
                                 <!-- --></a>NVIDIA reserves the right to make corrections,
                              modifications, enhancements, improvements, and any other changes to
                              this document, at any time without notice.
                           </p>
                           <p class="p" id="notice__notice-para-3"><a name="notice__notice-para-3" shape="rect">
                                 <!-- --></a>Customer should obtain the latest relevant information
                              before placing orders and should verify that such information is
                              current and complete.
                           </p>
                           <p class="p" id="notice__notice-para-4"><a name="notice__notice-para-4" shape="rect">
                                 <!-- --></a>NVIDIA products are sold subject to the NVIDIA
                              standard terms and conditions of sale supplied at the time of order
                              acknowledgement, unless otherwise agreed in an individual sales
                              agreement signed by authorized representatives of NVIDIA and
                              customer (“Terms of Sale”). NVIDIA hereby expressly objects to
                              applying any customer general terms and conditions with regards to
                              the purchase of the NVIDIA product referenced in this document. No
                              contractual obligations are formed either directly or indirectly by
                              this document.
                           </p>
                           <p class="p" id="notice__notice-para-5"><a name="notice__notice-para-5" shape="rect">
                                 <!-- --></a>NVIDIA products are not designed, authorized, or
                              warranted to be suitable for use in medical, military, aircraft,
                              space, or life support equipment, nor in applications where failure
                              or malfunction of the NVIDIA product can reasonably be expected to
                              result in personal injury, death, or property or environmental
                              damage. NVIDIA accepts no liability for inclusion and/or use of
                              NVIDIA products in such equipment or applications and therefore such
                              inclusion and/or use is at customer’s own risk.
                           </p>
                           <p class="p" id="notice__notice-para-6"><a name="notice__notice-para-6" shape="rect">
                                 <!-- --></a>NVIDIA makes no representation or warranty that
                              products based on this document will be suitable for any specified
                              use. Testing of all parameters of each product is not necessarily
                              performed by NVIDIA. It is customer’s sole responsibility to
                              evaluate and determine the applicability of any information
                              contained in this document, ensure the product is suitable and fit
                              for the application planned by customer, and perform the necessary
                              testing for the application in order to avoid a default of the
                              application or the product. Weaknesses in customer’s product designs
                              may affect the quality and reliability of the NVIDIA product and may
                              result in additional or different conditions and/or requirements
                              beyond those contained in this document. NVIDIA accepts no liability
                              related to any default, damage, costs, or problem which may be based
                              on or attributable to: (i) the use of the NVIDIA product in any
                              manner that is contrary to this document or (ii) customer product
                              designs.
                           </p>
                           <p class="p" id="notice__notice-para-7"><a name="notice__notice-para-7" shape="rect">
                                 <!-- --></a>No license, either expressed or implied, is granted
                              under any NVIDIA patent right, copyright, or other NVIDIA
                              intellectual property right under this document. Information
                              published by NVIDIA regarding third-party products or services does
                              not constitute a license from NVIDIA to use such products or
                              services or a warranty or endorsement thereof. Use of such
                              information may require a license from a third party under the
                              patents or other intellectual property rights of the third party, or
                              a license from NVIDIA under the patents or other intellectual
                              property rights of NVIDIA.
                           </p>
                           <p class="p" id="notice__notice-para-8"><a name="notice__notice-para-8" shape="rect">
                                 <!-- --></a>Reproduction of information in this document is
                              permissible only if approved in advance by NVIDIA in writing,
                              reproduced without alteration and in full compliance with all
                              applicable export laws and regulations, and accompanied by all
                              associated conditions, limitations, and notices.
                           </p>
                           <p class="p" id="notice__notice-para-9"><a name="notice__notice-para-9" shape="rect">
                                 <!-- --></a>THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS,
                              REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER
                              DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED
                              “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY,
                              OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS
                              ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND
                              FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY
                              LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING
                              WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL,
                              PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF
                              THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT,
                              EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
                              Notwithstanding any damages that customer might incur for any reason
                              whatsoever, NVIDIA’s aggregate and cumulative liability towards
                              customer for the products described herein shall be limited in
                              accordance with the Terms of Sale for the product.
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="arm"><a name="arm" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#arm" name="arm" shape="rect"></a></h3>
                     <div class="body refbody">
                        <div class="section notices" id="arm__section_n3n_kf2_qtb"><a name="arm__section_n3n_kf2_qtb" shape="rect">
                              <!-- --></a><h3 class="title sectiontitle notices">Arm</h3>
                           <p class="p">Arm, AMBA and Arm Powered are registered trademarks of Arm Limited. Cortex, MPCore
                              and Mali are trademarks of Arm Limited. "Arm" is used to represent Arm Holdings plc;
                              its operating company Arm Limited; and the regional subsidiaries Arm Inc.; Arm KK;
                              Arm Korea Limited.; Arm Taiwan Limited; Arm France SAS; Arm Consulting (Shanghai)
                              Co. Ltd.; Arm Germany GmbH; Arm Embedded Technologies Pvt. Ltd.; Arm Norway, AS and
                              Arm Sweden AB.
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="hdmi"><a name="hdmi" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#hdmi" name="hdmi" shape="rect"></a></h3>
                     <div class="body refbody">
                        <div class="section notices">
                           <h3 class="title sectiontitle notices">HDMI</h3>
                           <p class="p">HDMI, the HDMI logo, and High-Definition Multimedia Interface are trademarks or
                              registered trademarks of HDMI Licensing LLC.
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="blackberry"><a name="blackberry" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#blackberry" name="blackberry" shape="rect"></a></h3>
                     <div class="body refbody">
                        <div class="section notices">
                           <h3 class="title sectiontitle notices">Blackberry/QNX</h3>
                           <p class="p">Copyright © 2020 BlackBerry Limited. All rights reserved.</p>
                           <p class="p">Trademarks, including but not limited to BLACKBERRY, EMBLEM Design, QNX, AVIAGE,
                              MOMENTICS, NEUTRINO and QNX CAR are the trademarks or registered trademarks of
                              BlackBerry Limited, used under license, and the exclusive rights to such trademarks
                              are expressly reserved. 
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="google"><a name="google" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#google" name="google" shape="rect"></a></h3>
                     <div class="body refbody">
                        <div class="section notices">
                           <h3 class="title sectiontitle notices">Google</h3>
                           <p class="p">Android, Android TV, Google Play and the Google Play logo are trademarks of Google,
                              Inc.
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="trademarks"><a name="trademarks" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#trademarks" name="trademarks" shape="rect"></a></h3>
                     <div class="body refbody">
                        <div class="section notices">
                           <h3 class="title sectiontitle notices">Trademarks</h3>
                           <p class="p">NVIDIA, the NVIDIA logo, and BlueField, CUDA, DALI, DRIVE, Hopper, JetPack, Jetson
                              AGX Xavier, Jetson Nano, Maxwell, NGC, Nsight, Orin, Pascal, Quadro, Tegra,
                              TensorRT, Triton, Turing and Volta are trademarks and/or registered trademarks of
                              NVIDIA Corporation in the United States and other countries. Other company and
                              product names may be trademarks of the respective companies with which they are
                              associated.
                           </p>
                        </div>
                     </div>
                  </div>
                  <div class="topic reference nested1" id="copyright-past-to-present"><a name="copyright-past-to-present" shape="rect">
                        <!-- --></a><h3 class="title topictitle2"><a href="#copyright-past-to-present" name="copyright-past-to-present" shape="rect"></a></h3>
                     <div class="body refbody">
                        <div class="section notices">
                           <h3 class="title sectiontitle notices">Copyright</h3>
                           <p class="p">© <span class="ph">2014</span>-<span class="ph">2024</span> NVIDIA Corporation &amp;
                              affiliates. All rights reserved.
                           </p>
                        </div>
                     </div>
                  </div>
               </div>
               
               
            </article>
            <footer id="footer"><img src="../common/formatting/NVIDIA-LogoBlack.svg"></img><div><a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank">Privacy Policy</a> |
                  <a href="https://www.nvidia.com/en-us/privacy-center/" target="_blank">Manage My Privacy</a> |
                  <a href="https://www.nvidia.com/en-us/preferences/email-preferences/" target="_blank">Do Not Sell or Share My Data</a> |
                  <a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank">Terms of Service</a> |
                  <a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank">Accessibility</a> |
                  <a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank">Corporate Policies</a> |
                  <a href="https://www.nvidia.com/en-us/product-security/" target="_blank">Product Security</a> |
                  <a href="https://www.nvidia.com/en-us/contact/" target="_blank">Contact</a></div>
               <div class="copyright">Copyright © 2024 NVIDIA Corporation</div>
            </footer>
         </div>
      </div>
      <script language="JavaScript" type="text/javascript" charset="utf-8" src="../common/formatting/common.min.js"></script>
      <script language="JavaScript" type="text/javascript" charset="utf-8" src="../common/scripts/google-analytics/google-analytics-write.js"></script>
      <script language="JavaScript" type="text/javascript" charset="utf-8" src="../common/scripts/google-analytics/google-analytics-tracker.js"></script>
      <script type="text/javascript">var switchTo5x=true;</script><script type="text/javascript">stLight.options({publisher: "998dc202-a267-4d8e-bce9-14debadb8d92", doNotHash: false, doNotCopy: false, hashAddressBar: false});</script>
      <script type="text/javascript">_satellite.pageBottom();</script></body>

<!-- Mirrored from docs.nvidia.com/deeplearning/cudnn/archives/cudnn-897/release-notes/index.html by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 06 Aug 2024 07:08:57 GMT -->
</html>